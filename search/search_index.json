{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-apflow","title":"Welcome to apflow","text":"<p>apflow is a unified framework for orchestrating and executing tasks across multiple execution methods. It manages when tasks run, how they depend on each other, and ensures everything executes in the right order\u2014whether you're calling HTTP APIs, executing SSH commands, running Docker containers, or coordinating AI agents.</p>"},{"location":"#problems-we-solve","title":"Problems We Solve","text":"<p>Are you struggling with these common challenges?</p> <ul> <li> <p>Complex Task Dependencies</p> <p>Manually tracking dependencies, ensuring proper execution order, and handling failures across complex workflows becomes a nightmare. You end up writing custom coordination code and dealing with race conditions.</p> </li> <li> <p>Multiple Execution Methods</p> <p>You need to call HTTP APIs, execute SSH commands, run Docker containers, communicate via gRPC, and coordinate AI agents\u2014but each requires different libraries and integration patterns.</p> </li> <li> <p>Traditional Tasks + AI Agents</p> <p>You want to add AI capabilities to existing workflows, but most solutions force you to choose: either traditional task execution OR AI agents. You're stuck with all-or-nothing decisions.</p> </li> <li> <p>State Persistence &amp; Recovery</p> <p>When workflows fail or get interrupted, you lose progress. Implementing retry logic, checkpointing, and state recovery requires significant custom development.</p> </li> <li> <p>Real-time Monitoring</p> <p>You need to show progress to users, but building real-time monitoring with polling, WebSocket connections, or custom streaming solutions takes weeks.</p> </li> </ul>"},{"location":"#why-apflow","title":"Why apflow?","text":"<ul> <li> <p>Unified Interface</p> <p>One framework handles traditional tasks, HTTP/REST APIs, SSH commands, Docker containers, gRPC services, WebSocket communication, MCP tools, and AI agents\u2014all through the same ExecutableTask interface.</p> </li> <li> <p>Start Simple, Scale Up</p> <p>Begin with a lightweight, dependency-free core. Add AI capabilities, A2A server, CLI tools, or PostgreSQL storage only when you need them. No forced upfront installations.</p> </li> <li> <p>Language-Agnostic Protocol</p> <p>Built on the AI Partner Up Flow Protocol, ensuring interoperability across Python, Go, Rust, JavaScript, and more. Different language implementations work together seamlessly.</p> </li> <li> <p>Production-Ready</p> <p>Built-in storage (DuckDB or PostgreSQL), real-time streaming, automatic retries, state persistence, and comprehensive monitoring\u2014all included. No need to build these from scratch.</p> </li> <li> <p>Extensive Executor Ecosystem</p> <p>Choose from HTTP/REST APIs (with authentication), SSH remote execution, Docker containers, gRPC services, WebSocket communication, MCP integration, and LLM-based task tree generation.</p> </li> </ul>"},{"location":"#what-happens-when-you-use-apflow","title":"What Happens When You Use apflow?","text":"Before After Weeks of custom coordination code Days to define task trees with dependencies Multiple orchestration systems for different execution methods One unified interface for all execution methods All-or-nothing decisions requiring complete rewrites Gradual addition of AI agents incrementally Weeks building custom polling or streaming solutions Built-in real-time streaming via A2A Protocol Manual recovery logic and lost progress Automatic retries with exponential backoff and state persistence Worrying about resource usage at scale Production-ready from day one, handle hundreds of concurrent workflows"},{"location":"#quick-start","title":"Quick Start","text":"<p>New to apflow? Get up and running in minutes!</p> <p>Installation:</p> <pre><code>pip install apflow\n</code></pre> <p>Quick Start Guide</p> <p>Core Concepts</p> <p>Examples</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":"<ul> <li> <p>Getting Started</p> <p>Learn the fundamentals and get started quickly</p> <p>Getting Started \u2192</p> </li> <li> <p>User Guides</p> <p>Complete guides for using apflow</p> <p>Guides \u2192</p> </li> <li> <p>API Reference</p> <p>Complete API documentation for Python and HTTP</p> <p>API Reference \u2192</p> </li> <li> <p>Architecture</p> <p>System architecture and design principles</p> <p>Architecture \u2192</p> </li> <li> <p>Protocol</p> <p>Language-agnostic protocol specification</p> <p>Protocol \u2192</p> </li> <li> <p>Development</p> <p>Contributing and extending the framework</p> <p>Development \u2192</p> </li> <li> <p>Examples</p> <p>Code examples and common patterns</p> <p>Examples \u2192</p> </li> </ul>"},{"location":"#learning-paths","title":"Learning Paths","text":""},{"location":"#quick-start-15-minutes","title":"Quick Start (15 minutes)","text":"<p>Perfect for getting started quickly:</p> <ol> <li>Quick Start - Get running in 10 minutes</li> <li>Basic Examples - Try examples</li> <li>Core Concepts - Understand basics</li> </ol>"},{"location":"#complete-beginner-1-2-hours","title":"Complete Beginner (1-2 hours)","text":"<p>Step-by-step learning path:</p> <ol> <li>Getting Started Index - Overview and learning paths</li> <li>First Steps Tutorial - Complete beginner tutorial</li> <li>Task Trees Tutorial - Build task trees</li> <li>Dependencies Tutorial - Master dependencies</li> <li>Core Concepts - Deep dive</li> <li>Basic Examples - Practice</li> </ol>"},{"location":"#professional-developer-2-4-hours","title":"Professional Developer (2-4 hours)","text":"<p>For experienced developers:</p> <ol> <li>Quick Start - Quick refresher</li> <li>Task Orchestration - Master orchestration</li> <li>Custom Tasks - Create executors</li> <li>Best Practices - Learn patterns</li> <li>API Reference - Complete reference</li> </ol>"},{"location":"#contributor-4-hours","title":"Contributor (4+ hours)","text":"<p>For framework contributors:</p> <ol> <li>Development Setup - Set up environment</li> <li>Architecture Overview - Understand design</li> <li>Contributing - Learn process</li> </ol>"},{"location":"#popular-guides","title":"Popular Guides","text":""},{"location":"#for-users","title":"For Users","text":"<ul> <li>Task Orchestration - Complete guide to task orchestration, dependencies, and priorities</li> <li>Custom Tasks - Guide to creating custom tasks with ExecutableTask interface</li> <li>CLI - Complete CLI usage guide</li> <li>API Server - API server setup and usage guide</li> <li>Best Practices - Best practices and recommendations</li> <li>FAQ - Common questions and troubleshooting</li> </ul>"},{"location":"#for-developers","title":"For Developers","text":"<ul> <li>Python API - Core Python library API reference (TaskManager, ExecutableTask, TaskTreeNode, etc.)</li> <li>HTTP API - A2A Protocol Server HTTP API reference</li> <li>Quick Reference - Cheat sheet with common snippets</li> <li>Contributing - Contribution guidelines and process</li> </ul>"},{"location":"#architecture-design","title":"Architecture &amp; Design","text":"<ul> <li>Architecture Overview - System architecture and design principles</li> <li>Directory Structure - Directory structure and naming conventions</li> <li>Extension Registry Design - Extension registry design (Protocol-based architecture)</li> <li>Configuration - Database table configuration</li> </ul>"},{"location":"#protocol","title":"Protocol","text":"<ul> <li>Protocol Overview - AI Partner Up Flow Protocol specification</li> <li>Core Concepts - Fundamental protocol concepts</li> <li>Data Model - Task schema and data structures</li> <li>Execution Lifecycle - State machine and execution flow</li> </ul>"},{"location":"#examples-tutorials","title":"Examples &amp; Tutorials","text":"<ul> <li>Basic Task - Basic task examples and common patterns</li> <li>Task Tree - Task tree examples with dependencies and priorities</li> <li>Real World Examples - Real-world use cases and examples</li> <li>First Steps Tutorial - Complete beginner tutorial</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#by-task","title":"By Task","text":"<p>I want to...</p> <ul> <li>Get started quickly \u2192 Quick Start</li> <li>Understand concepts \u2192 Core Concepts</li> <li>Create a custom executor \u2192 Custom Tasks Guide</li> <li>Build complex workflows \u2192 Task Orchestration Guide</li> <li>See examples \u2192 Examples</li> <li>Find API reference \u2192 Python API or Quick Reference</li> <li>Troubleshoot issues \u2192 FAQ</li> <li>Learn best practices \u2192 Best Practices</li> <li>Set up development \u2192 Development Setup</li> <li>Understand architecture \u2192 Architecture Overview</li> <li>Read the protocol \u2192 Protocol Specification</li> </ul>"},{"location":"#by-role","title":"By Role","text":"<p>I am a...</p> <ul> <li>New User \u2192 Start with Getting Started</li> <li>Developer \u2192 Check Guides and API Reference</li> <li>Contributor \u2192 See Development section</li> <li>Architect \u2192 Review Architecture documentation</li> </ul>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>GitHub Repository - Source code and issues</li> <li>PyPI Package - Install from PyPI</li> <li>Protocol Documentation - AI Partner Up Flow Protocol specification</li> <li>GitHub Issues - Report bugs and request features</li> <li>GitHub Discussions - Ask questions and share ideas</li> </ul>"},{"location":"#need-help","title":"Need Help?","text":"<p>Check out our FAQ for common questions and answers, or start a discussion on GitHub.</p> <p>Ready to start? \u2192 Getting Started \u2192 or Quick Start \u2192</p>"},{"location":"api/graphql/","title":"GraphQL API Reference","text":"<p>This document provides a complete reference for the apflow GraphQL API, built on Strawberry GraphQL.</p>"},{"location":"api/graphql/#overview","title":"Overview","text":"<p>The GraphQL API provides: - Typed schema with full introspection support - Queries for reading tasks and task trees - Mutations for creating, updating, cancelling, and deleting tasks - Subscriptions for real-time task status updates via WebSocket - GraphiQL interactive playground for exploring the schema</p>"},{"location":"api/graphql/#installation","title":"Installation","text":"<p>Install apflow with the GraphQL extra:</p> <pre><code>pip install apflow[graphql]\n</code></pre>"},{"location":"api/graphql/#configuration","title":"Configuration","text":"<p>Set the API protocol to <code>graphql</code>:</p> <pre><code>export APFLOW_API_PROTOCOL=graphql\n</code></pre> <p>Start the server:</p> <pre><code>apflow serve\n# Or: python -m apflow.api.main\n</code></pre> <p>The GraphQL endpoint is available at <code>http://localhost:8000/graphql</code>.</p>"},{"location":"api/graphql/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>APFLOW_API_PROTOCOL</code> string <code>a2a</code> Set to <code>graphql</code> to enable the GraphQL adapter <code>APFLOW_ENABLE_DOCS</code> boolean <code>true</code> Enable GraphiQL interactive playground <code>APFLOW_ENABLE_SYSTEM_ROUTES</code> boolean <code>true</code> Enable system info endpoint at <code>/system</code> <code>APFLOW_API_HOST</code> string <code>0.0.0.0</code> Server host <code>APFLOW_API_PORT</code> integer <code>8000</code> Server port"},{"location":"api/graphql/#schema-reference","title":"Schema Reference","text":""},{"location":"api/graphql/#types","title":"Types","text":""},{"location":"api/graphql/#tasktype","title":"<code>TaskType</code>","text":"<p>The primary task representation:</p> <pre><code>type TaskType {\n  id: String!\n  name: String!\n  status: TaskStatusEnum!\n  priority: Int!\n  progress: Float!\n  result: String\n  error: String\n  createdAt: DateTime\n  updatedAt: DateTime\n  parentId: String\n  description: String\n  children: [TaskType!]\n}\n</code></pre>"},{"location":"api/graphql/#taskstatusenum","title":"<code>TaskStatusEnum</code>","text":"<pre><code>enum TaskStatusEnum {\n  PENDING\n  IN_PROGRESS\n  COMPLETED\n  FAILED\n  CANCELLED\n}\n</code></pre>"},{"location":"api/graphql/#input-types","title":"Input Types","text":""},{"location":"api/graphql/#createtaskinput","title":"<code>CreateTaskInput</code>","text":"<pre><code>input CreateTaskInput {\n  name: String!\n  description: String\n  executor: String\n  priority: Int\n  inputs: JSON\n  parentId: String\n}\n</code></pre>"},{"location":"api/graphql/#updatetaskinput","title":"<code>UpdateTaskInput</code>","text":"<pre><code>input UpdateTaskInput {\n  name: String\n  description: String\n  status: TaskStatusEnum\n  inputs: JSON\n}\n</code></pre>"},{"location":"api/graphql/#queries","title":"Queries","text":""},{"location":"api/graphql/#task","title":"<code>task</code>","text":"<p>Fetch a single task by ID.</p> <pre><code>query {\n  task(taskId: \"task-123\") {\n    id\n    name\n    status\n    progress\n    result\n    error\n    createdAt\n  }\n}\n</code></pre>"},{"location":"api/graphql/#tasks","title":"<code>tasks</code>","text":"<p>Fetch a list of tasks with optional filters.</p> <pre><code>query {\n  tasks(status: PENDING, limit: 10, offset: 0) {\n    id\n    name\n    status\n    priority\n    createdAt\n  }\n}\n</code></pre> <p>Arguments:</p> Argument Type Description <code>status</code> <code>TaskStatusEnum</code> Filter by task status <code>limit</code> <code>Int</code> Maximum number of tasks to return <code>offset</code> <code>Int</code> Number of tasks to skip"},{"location":"api/graphql/#taskchildren","title":"<code>taskChildren</code>","text":"<p>Fetch all child tasks of a parent task.</p> <pre><code>query {\n  taskChildren(parentId: \"parent-task-123\") {\n    id\n    name\n    status\n    priority\n  }\n}\n</code></pre>"},{"location":"api/graphql/#mutations","title":"Mutations","text":""},{"location":"api/graphql/#createtask","title":"<code>createTask</code>","text":"<p>Create a new task.</p> <pre><code>mutation {\n  createTask(taskInput: {\n    name: \"my_task\"\n    description: \"A sample task\"\n    executor: \"rest_executor\"\n    priority: 5\n    inputs: { url: \"https://api.example.com/data\" }\n  }) {\n    id\n    name\n    status\n  }\n}\n</code></pre>"},{"location":"api/graphql/#updatetask","title":"<code>updateTask</code>","text":"<p>Update an existing task.</p> <pre><code>mutation {\n  updateTask(\n    taskId: \"task-123\"\n    taskInput: {\n      name: \"updated_name\"\n      status: CANCELLED\n    }\n  ) {\n    id\n    name\n    status\n    updatedAt\n  }\n}\n</code></pre>"},{"location":"api/graphql/#canceltask","title":"<code>cancelTask</code>","text":"<p>Cancel a running task. Returns <code>true</code> on success.</p> <pre><code>mutation {\n  cancelTask(taskId: \"task-123\")\n}\n</code></pre>"},{"location":"api/graphql/#deletetask","title":"<code>deleteTask</code>","text":"<p>Delete a task. Returns <code>true</code> on success.</p> <pre><code>mutation {\n  deleteTask(taskId: \"task-123\")\n}\n</code></pre>"},{"location":"api/graphql/#subscriptions","title":"Subscriptions","text":""},{"location":"api/graphql/#taskstatuschanged","title":"<code>taskStatusChanged</code>","text":"<p>Subscribe to real-time status updates for a specific task. Emits a <code>TaskType</code> each time the task's status changes. The subscription ends automatically when the task reaches a terminal status (<code>COMPLETED</code>, <code>FAILED</code>, or <code>CANCELLED</code>).</p> <pre><code>subscription {\n  taskStatusChanged(taskId: \"task-123\") {\n    id\n    name\n    status\n    progress\n    result\n    error\n  }\n}\n</code></pre> <p>The subscription uses the <code>graphql-transport-ws</code> protocol over WebSocket.</p>"},{"location":"api/graphql/#client-examples","title":"Client Examples","text":""},{"location":"api/graphql/#python-httpx","title":"Python (httpx)","text":"<pre><code>import httpx\n\n# Query a task\nquery = \"\"\"\nquery GetTask($taskId: String!) {\n  task(taskId: $taskId) {\n    id\n    name\n    status\n    progress\n    result\n  }\n}\n\"\"\"\n\nresponse = httpx.post(\n    \"http://localhost:8000/graphql\",\n    json={\"query\": query, \"variables\": {\"taskId\": \"task-123\"}},\n)\nprint(response.json())\n</code></pre>"},{"location":"api/graphql/#python-create-task","title":"Python (create task)","text":"<pre><code>import httpx\n\nmutation = \"\"\"\nmutation CreateTask($input: CreateTaskInput!) {\n  createTask(taskInput: $input) {\n    id\n    name\n    status\n  }\n}\n\"\"\"\n\nvariables = {\n    \"input\": {\n        \"name\": \"my_executor\",\n        \"description\": \"Run a data pipeline\",\n        \"priority\": 5,\n        \"inputs\": {\"source\": \"s3://bucket/data.csv\"},\n    }\n}\n\nresponse = httpx.post(\n    \"http://localhost:8000/graphql\",\n    json={\"query\": mutation, \"variables\": variables},\n)\nprint(response.json())\n</code></pre>"},{"location":"api/graphql/#curl","title":"cURL","text":"<pre><code># Query tasks\ncurl -X POST http://localhost:8000/graphql \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"{ tasks(status: PENDING, limit: 5) { id name status priority } }\"\n  }'\n\n# Create a task\ncurl -X POST http://localhost:8000/graphql \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query\": \"mutation { createTask(taskInput: { name: \\\"my_task\\\" }) { id name status } }\"\n  }'\n</code></pre>"},{"location":"api/graphql/#authentication","title":"Authentication","text":"<p>The GraphQL adapter supports the same JWT authentication as the A2A and MCP adapters. Include the token in the <code>Authorization</code> header:</p> <pre><code>curl -X POST http://localhost:8000/graphql \\\n  -H \"Authorization: Bearer &lt;your-jwt-token&gt;\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query\": \"{ tasks { id name status } }\"}'\n</code></pre> <p>See the API Server Guide for token generation details.</p>"},{"location":"api/graphql/#graphiql-playground","title":"GraphiQL Playground","text":"<p>When <code>APFLOW_ENABLE_DOCS=true</code> (the default), opening <code>http://localhost:8000/graphql</code> in a browser loads the GraphiQL interactive playground. Use it to:</p> <ul> <li>Explore the schema with auto-complete</li> <li>Run queries and mutations interactively</li> <li>View type documentation inline</li> <li>Test subscriptions</li> </ul> <p>To disable GraphiQL in production:</p> <pre><code>export APFLOW_ENABLE_DOCS=false\n</code></pre>"},{"location":"api/graphql/#protocol-comparison","title":"Protocol Comparison","text":"Feature A2A (HTTP) MCP GraphQL Protocol JSON-RPC over HTTP JSON-RPC (HTTP/stdio) GraphQL over HTTP/WS Schema Agent Card discovery Tool/Resource listing Full introspection Streaming SSE / WebSocket N/A Subscriptions (WS) Best for Agent-to-agent communication LLM tool integration Frontend apps, typed APIs Install extra <code>apflow[a2a]</code> <code>apflow[a2a]</code> <code>apflow[graphql]</code>"},{"location":"api/graphql/#see-also","title":"See Also","text":"<ul> <li>API Server Guide - Server setup and protocol selection</li> <li>HTTP API Reference - A2A Protocol HTTP API</li> <li>Python API Reference - Python library API</li> <li>Environment Variables - All configuration options</li> </ul>"},{"location":"api/http/","title":"API Reference","text":"<p>This document provides a complete reference for the apflow API, which implements the A2A (Agent-to-Agent) Protocol standard.</p>"},{"location":"api/http/#overview","title":"Overview","text":"<p>The apflow API server provides: - A2A Protocol Server: Standard agent-to-agent communication protocol - Task Management: Create, read, update, and delete tasks - Task Execution: Execute task trees with dependency management - Real-time Streaming: Progress updates via Server-Sent Events (SSE) and WebSocket - JWT Authentication: Optional token-based authentication</p>"},{"location":"api/http/#base-url","title":"Base URL","text":"<pre><code>http://localhost:8000  # Default development server\n</code></pre>"},{"location":"api/http/#endpoints","title":"Endpoints","text":""},{"location":"api/http/#a2a-protocol-endpoints","title":"A2A Protocol Endpoints","text":""},{"location":"api/http/#get-well-knownagent-card","title":"<code>GET /.well-known/agent-card</code>","text":"<p>Description: Retrieves the agent card that describes the service capabilities, available skills, and protocol support. This endpoint follows the A2A Protocol standard for agent discovery.</p> <p>Authentication: Not required (public endpoint)</p> <p>Request Parameters: None</p> <p>Response Format: Returns a JSON object containing agent metadata:</p> <pre><code>{\n  \"name\": \"apflow\",\n  \"description\": \"Agent workflow orchestration and execution platform\",\n  \"url\": \"http://localhost:8000\",\n  \"version\": \"0.2.0\",\n  \"capabilities\": {\n    \"streaming\": true,\n    \"push_notifications\": true\n  },\n  \"skills\": [\n    {\n      \"id\": \"tasks.execute\",\n      \"name\": \"Execute Task Tree\",\n      \"description\": \"Execute a complete task tree with multiple tasks\",\n      \"tags\": [\"task\", \"orchestration\", \"workflow\", \"execution\"]\n    }\n  ]\n}\n</code></pre> <p>Response Fields: - <code>name</code> (string): Service name - <code>description</code> (string): Service description - <code>url</code> (string): Base URL of the service - <code>version</code> (string): Service version - <code>capabilities</code> (object): Supported capabilities   - <code>streaming</code> (boolean): Whether streaming mode is supported   - <code>push_notifications</code> (boolean): Whether push notifications are supported - <code>skills</code> (array): List of available skills/operations</p> <p>Example Request: <pre><code>curl http://localhost:8000/.well-known/agent-card\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"name\": \"apflow\",\n  \"description\": \"Agent workflow orchestration and execution platform\",\n  \"url\": \"http://localhost:8000\",\n  \"version\": \"0.2.0\",\n  \"capabilities\": {\n    \"streaming\": true,\n    \"push_notifications\": true\n  },\n  \"skills\": [\n    {\n      \"id\": \"tasks.execute\",\n      \"name\": \"Execute Task Tree\",\n      \"description\": \"Execute a complete task tree with multiple tasks\",\n      \"tags\": [\"task\", \"orchestration\", \"workflow\", \"execution\"]\n    }\n  ]\n}\n</code></pre></p> <p>Notes: - This endpoint is always public and does not require authentication - The agent card is used by A2A Protocol clients to discover service capabilities - The URL in the response should match the actual service URL</p>"},{"location":"api/http/#post","title":"<code>POST /</code>","text":"<p>Description: Main A2A Protocol RPC endpoint that handles all A2A protocol requests. This endpoint implements the standard A2A Protocol JSON-RPC 2.0 interface for agent-to-agent communication. It supports task tree execution with streaming, push notifications, and real-time progress updates.</p> <p>Authentication: Optional (JWT token in <code>Authorization</code> header if JWT is enabled)</p> <p>Request Format: JSON-RPC 2.0 format:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"execute_task_tree\",\n  \"params\": {\n    \"tasks\": [\n      {\n        \"id\": \"task1\",\n        \"name\": \"Task 1\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\n          \"method\": \"system_info_executor\"\n        },\n        \"inputs\": {\n          \"resource\": \"cpu\"\n        }\n      }\n    ]\n  },\n  \"id\": \"request-123\",\n  \"configuration\": {\n    \"push_notification_config\": {\n      \"url\": \"https://your-server.com/callback\",\n      \"headers\": {\n        \"Authorization\": \"Bearer token\"\n      }\n    }\n  },\n  \"metadata\": {\n    \"stream\": true\n  }\n}\n</code></pre> <p>Request Parameters: - <code>jsonrpc</code> (string, required): JSON-RPC version, must be \"2.0\" - <code>method</code> (string, required): Method name. The A2A <code>POST /</code> endpoint supports agent-level actions only:   - <code>tasks.execute</code> (recommended) or <code>execute_task_tree</code> (backward compatible): Execute a task tree   - <code>tasks.generate</code>: Generate task tree from natural language using LLM   - <code>tasks.cancel</code>: Cancel running task(s)</p> <p>Note: CRUD and query operations (<code>tasks.create</code>, <code>tasks.get</code>, <code>tasks.update</code>, <code>tasks.delete</code>, <code>tasks.list</code>, etc.) should use <code>POST /tasks</code> instead. See Native API below. - <code>params</code> (object, required): Method parameters (varies by method)   - For <code>tasks.execute</code> or <code>execute_task_tree</code>: <code>tasks</code> (array, required): Array of task objects to execute - <code>id</code> (string/number, required): Request identifier for matching responses - <code>configuration</code> (object, optional): Configuration for push notifications   - <code>push_notification_config</code> (object, optional): Push notification settings     - <code>url</code> (string, required): Callback URL for status updates     - <code>headers</code> (object, optional): HTTP headers for callback requests     - <code>method</code> (string, optional): HTTP method for callbacks (default: \"POST\") - <code>metadata</code> (object, optional): Additional metadata   - <code>stream</code> (boolean, optional): Enable streaming mode for real-time updates   - <code>task_id</code> (string, optional): Existing task ID to execute.</p> <p>Response Format: JSON-RPC 2.0 response with A2A Protocol Task object:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-123\",\n  \"result\": {\n    \"id\": \"task-execution-id\",\n    \"context_id\": \"task-abc-123\",\n    \"kind\": \"task\",\n    \"status\": {\n      \"state\": \"completed\",\n      \"message\": {\n        \"kind\": \"message\",\n        \"parts\": [\n          {\n            \"kind\": \"data\",\n            \"data\": {\n              \"protocol\": \"a2a\",\n              \"status\": \"completed\",\n              \"progress\": 1.0,\n              \"root_task_id\": \"task-abc-123\",\n              \"task_count\": 2\n            }\n          }\n        ]\n      }\n    },\n    \"artifacts\": [...],\n    \"metadata\": {\n      \"protocol\": \"a2a\",\n      \"root_task_id\": \"task-abc-123\",\n      \"user_id\": \"user123\"\n    }\n  }\n}\n</code></pre> <p>Response Fields: - <code>jsonrpc</code> (string): JSON-RPC version (\"2.0\") - <code>id</code> (string/number): Request identifier (matches request) - <code>result</code> (object): A2A Protocol Task object   - <code>id</code> (string): Task execution instance ID   - <code>context_id</code> (string): Task definition ID (root task ID)   - <code>kind</code> (string): Always <code>\"task\"</code> for A2A protocol   - <code>status</code> (object): Task status object     - <code>state</code> (string): Task state (\"completed\", \"working\", \"failed\", etc.)     - <code>message</code> (object): Status message with parts       - <code>parts[].data.protocol</code> (string): Protocol identifier, always <code>\"a2a\"</code> for A2A protocol responses       - <code>parts[].data.status</code> (string): Task status (\"completed\", \"in_progress\", \"failed\", \"pending\")       - <code>parts[].data.progress</code> (float): Overall progress (0.0 to 1.0)       - <code>parts[].data.root_task_id</code> (string): ID of the root task       - <code>parts[].data.task_count</code> (integer): Number of tasks in the tree   - <code>artifacts</code> (array): Execution artifacts   - <code>metadata</code> (object): Task metadata     - <code>protocol</code> (string): Protocol identifier, always <code>\"a2a\"</code> for A2A protocol responses     - <code>root_task_id</code> (string): ID of the root task     - <code>user_id</code> (string, optional): User ID associated with the task</p> <p>Error Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-123\",\n  \"error\": {\n    \"code\": -32603,\n    \"message\": \"Internal error\",\n    \"data\": \"Error details\"\n  }\n}\n</code></pre></p> <p>Example Request: <pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [\n        {\n          \"id\": \"task1\",\n          \"name\": \"my_task\",\n          \"user_id\": \"user123\",\n          \"schemas\": {\"method\": \"system_info_executor\"},\n          \"inputs\": {}\n        }\n      ]\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre></p> <p>Example with Streaming: <pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [...]\n    },\n    \"metadata\": {\n      \"stream\": true\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre></p> <p>Example with Push Notifications: <pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [...]\n    },\n    \"configuration\": {\n      \"push_notification_config\": {\n        \"url\": \"https://your-server.com/callback\",\n        \"headers\": {\n          \"Authorization\": \"Bearer your-token\"\n        }\n      }\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre></p> <p>Notes: - This endpoint implements the A2A Protocol standard - All task management operations (CRUD, query, execution, cancellation, copy) are now fully supported through the A2A Protocol <code>/</code> route - Method naming: Use <code>tasks.execute</code> (recommended) or <code>execute_task_tree</code> (backward compatible) for task execution - When <code>push_notification_config</code> is provided, the server executes tasks asynchronously and sends updates to the callback URL - When <code>metadata.stream</code> is true, progress updates are sent via EventQueue (SSE/WebSocket). For JSON-RPC <code>tasks.execute</code>, use <code>use_streaming=true</code> instead. - All task management methods return A2A Protocol Task objects with real-time status updates via <code>TaskStatusUpdateEvent</code> - Task execution follows dependency order and priority scheduling - All tasks in a tree must have the same <code>user_id</code> (or be accessible by the authenticated user) - All responses include <code>protocol: \"a2a\"</code> in Task metadata and event data to identify this as an A2A Protocol response - This differs from JSON-RPC protocol responses (which use <code>protocol: \"jsonrpc\"</code> in the response object)</p>"},{"location":"api/http/#cancel-method","title":"<code>cancel</code> Method","text":"<p>Description: Cancels a running task execution. This method is part of the A2A Protocol <code>AgentExecutor</code> interface and allows clients to cancel tasks that are currently executing or pending.</p> <p>Method: <code>cancel</code></p> <p>Request Format: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"cancel\",\n  \"params\": {},\n  \"id\": \"cancel-request-123\",\n  \"task_id\": \"task-abc-123\",\n  \"context_id\": \"context-xyz-456\",\n  \"metadata\": {\n    \"error_message\": \"Cancelled by user\",\n    \"force\": false\n  }\n}\n</code></pre></p> <p>Request Parameters: - <code>jsonrpc</code> (string, required): JSON-RPC version, must be \"2.0\" - <code>method</code> (string, required): Method name, must be \"cancel\" - <code>params</code> (object, optional): Method parameters (currently unused, can be empty) - <code>id</code> (string/number, required): Request identifier for matching responses - <code>task_id</code> (string, optional): Task ID to cancel. Priority: <code>task_id</code> &gt; <code>context_id</code> &gt; <code>metadata.task_id</code> &gt; <code>metadata.context_id</code> - <code>context_id</code> (string, optional): Context ID (used as task ID if <code>task_id</code> is not provided) - <code>metadata</code> (object, optional): Additional metadata   - <code>error_message</code> (string, optional): Custom error message for cancellation   - <code>force</code> (boolean, optional): Force flag (logged but not used by current implementation)   - <code>task_id</code> (string, optional): Task ID (used as fallback if not in top-level <code>task_id</code>)   - <code>context_id</code> (string, optional): Context ID (used as fallback if not in top-level <code>context_id</code>)</p> <p>Response Format: The response is sent via <code>TaskStatusUpdateEvent</code> through the EventQueue (SSE/WebSocket streaming):</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"cancel-request-123\",\n  \"result\": {\n    \"task_id\": \"task-abc-123\",\n    \"context_id\": \"context-xyz-456\",\n    \"status\": {\n      \"state\": \"canceled\",\n      \"message\": {\n        \"kind\": \"message\",\n        \"parts\": [\n          {\n            \"kind\": \"data\",\n            \"data\": {\n              \"protocol\": \"a2a\",\n              \"status\": \"cancelled\",\n              \"message\": \"Task cancelled successfully\",\n              \"token_usage\": {\n                \"total_tokens\": 1000,\n                \"prompt_tokens\": 500,\n                \"completion_tokens\": 500\n              },\n              \"result\": {\n                \"partial_data\": \"some result\"\n              },\n              \"timestamp\": \"2024-01-15T10:30:00Z\"\n            }\n          }\n        ]\n      }\n    },\n    \"final\": true\n  }\n}\n</code></pre> <p>Response Fields: - <code>task_id</code> (string): Task ID that was cancelled - <code>context_id</code> (string): Context ID - <code>status</code> (object): Task status object   - <code>state</code> (string): Task state (\"canceled\" or \"failed\")   - <code>message</code> (object): Status message with parts     - <code>parts[].data.protocol</code> (string): Protocol identifier, always <code>\"a2a\"</code>     - <code>parts[].data.status</code> (string): Cancellation status (\"cancelled\" or \"failed\")     - <code>parts[].data.message</code> (string): Cancellation message     - <code>parts[].data.token_usage</code> (object, optional): Token usage information if available     - <code>parts[].data.result</code> (any, optional): Partial result if available     - <code>parts[].data.error</code> (string, optional): Error message if cancellation failed     - <code>parts[].data.timestamp</code> (string): ISO 8601 timestamp - <code>final</code> (boolean): Always <code>true</code> for cancel operations</p> <p>Error Cases: - Task ID not found: Returns <code>TaskStatusUpdateEvent</code> with <code>state: \"failed\"</code> and error message \"Task ID not found in context\" - Task not found: Returns <code>state: \"failed\"</code> with message \"Task {task_id} not found\" - Task already completed: Returns <code>state: \"failed\"</code> with message \"Task {task_id} is already {status}, cannot cancel\" - Task already cancelled: Returns <code>state: \"failed\"</code> with message \"Task {task_id} is already cancelled, cannot cancel\" - Exception during cancellation: Returns <code>state: \"failed\"</code> with error details</p> <p>Notes: - The <code>cancel()</code> method attempts to gracefully cancel tasks by calling the executor's <code>cancel()</code> method if supported - Token usage is preserved if available from the executor - Partial results may be returned if the task was partially completed before cancellation - The method sends a <code>TaskStatusUpdateEvent</code> through the EventQueue, which is compatible with SSE/WebSocket streaming - Task ID extraction follows priority: <code>task_id</code> &gt; <code>context_id</code> &gt; <code>metadata.task_id</code> &gt; <code>metadata.context_id</code> - If no task ID is found, an error event is sent with <code>state: \"failed\"</code></p> <p>Example: Cancel a Running Task</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"cancel\",\n  \"params\": {},\n  \"id\": \"cancel-1\",\n  \"task_id\": \"my-running-task\",\n  \"metadata\": {\n    \"error_message\": \"User requested cancellation\"\n  }\n}\n</code></pre> <p>Example Response (via EventQueue): <pre><code>{\n  \"task_id\": \"my-running-task\",\n  \"context_id\": \"my-running-task\",\n  \"status\": {\n    \"state\": \"canceled\",\n    \"message\": {\n      \"parts\": [{\n        \"data\": {\n          \"protocol\": \"a2a\",\n          \"status\": \"cancelled\",\n          \"message\": \"Task cancelled successfully\",\n          \"timestamp\": \"2024-01-15T10:30:00Z\"\n        }\n      }]\n    }\n  },\n  \"final\": true\n}\n</code></pre></p>"},{"location":"api/http/#task-management-endpoints","title":"Task Management Endpoints","text":""},{"location":"api/http/#post-tasks","title":"<code>POST /tasks</code>","text":"<p>Description: Unified task management endpoint that supports multiple task operations via JSON-RPC 2.0 format. This endpoint handles all task-related operations including creation, retrieval, updates, deletion, querying, and execution. All operations are performed through different <code>method</code> values in the JSON-RPC request.</p> <p>Authentication: Optional (JWT token in <code>Authorization</code> header if JWT is enabled)</p> <p>Request Format: JSON-RPC 2.0 format with method-specific parameters:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.create\",\n  \"params\": {\n    \"tasks\": [\n      {\n        \"id\": \"task1\",\n        \"name\": \"Task 1\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\n          \"method\": \"system_info_executor\"\n        },\n        \"inputs\": {\n          \"resource\": \"cpu\"\n        }\n      }\n    ]\n  },\n  \"id\": \"request-123\"\n}\n</code></pre> <p>Supported Methods: - <code>tasks.create</code> - Create and execute task trees - <code>tasks.get</code> - Get task by ID - <code>tasks.update</code> - Update task properties - <code>tasks.delete</code> - Delete a task - <code>tasks.detail</code> - Get detailed task information - <code>tasks.tree</code> - Get task tree structure - <code>tasks.children</code> - Get child tasks of a parent task - <code>tasks.list</code> - List tasks with filters - <code>tasks.running.list</code> - List currently running tasks - <code>tasks.running.status</code> - Get status of running tasks - <code>tasks.running.count</code> - Get count of running tasks - <code>tasks.cancel</code> / <code>tasks.running.cancel</code> - Cancel running tasks - <code>tasks.clone</code> - Clone task tree for re-execution - <code>tasks.generate</code> - Generate task tree from natural language requirement - <code>tasks.execute</code> - Execute a task by ID</p> <p>Request Headers: - <code>Content-Type</code>: <code>application/json</code> (required) - <code>Authorization</code>: <code>Bearer &lt;token&gt;</code> (optional, if JWT is enabled, priority over cookie) - <code>X-LLM-API-KEY</code>: <code>&lt;api-key&gt;</code> or <code>&lt;provider&gt;:&lt;api-key&gt;</code> (optional, for LLM tasks)</p> <p>Request Cookies (Alternative to Header): - <code>Authorization</code>: <code>&lt;token&gt;</code> (optional, if JWT is enabled, fallback if header not present)</p> <p>Response Format: JSON-RPC 2.0 response format. The <code>result</code> field varies by method.</p> <p>Error Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-123\",\n  \"error\": {\n    \"code\": -32601,\n    \"message\": \"Method not found\",\n    \"data\": \"Unknown task method: tasks.invalid\"\n  }\n}\n</code></pre></p> <p>Notes: - All task operations require proper permissions if JWT is enabled - Tasks with different <code>user_id</code> values cannot be in the same tree - This is the primary endpoint for CRUD and query operations. A2A <code>POST /</code> only handles agent-level actions (execute, generate, cancel).</p>"},{"location":"api/http/#get-tasksmethods","title":"<code>GET /tasks/methods</code>","text":"<p>Description: Method discovery endpoint that returns all available task methods grouped by category, with input schemas and descriptions. Useful for programmatic discovery and client code generation.</p> <p>Authentication: Optional (JWT token in <code>Authorization</code> header if JWT is enabled)</p> <p>Response Format:</p> <pre><code>{\n  \"total\": 15,\n  \"categories\": [\"agent_action\", \"crud\", \"query\", \"monitoring\"],\n  \"methods\": {\n    \"agent_action\": [\n      {\n        \"method\": \"tasks.execute\",\n        \"name\": \"Execute Task Tree\",\n        \"description\": \"Execute a complete task tree with multiple tasks\",\n        \"category\": \"agent_action\",\n        \"tags\": [\"task\", \"orchestration\", \"workflow\", \"execution\"],\n        \"input_schema\": { \"type\": \"object\", \"properties\": { ... } },\n        \"supports_streaming\": true,\n        \"examples\": [\"Execute a 3-task workflow\"]\n      }\n    ],\n    \"crud\": [ ... ],\n    \"query\": [ ... ],\n    \"monitoring\": [ ... ]\n  }\n}\n</code></pre> <p>Example Request: <pre><code>curl http://localhost:8000/tasks/methods\n</code></pre> - The <code>X-LLM-API-KEY</code> header can be used to provide LLM API keys for tasks that require them - Task operations are atomic and support transaction rollback on errors</p>"},{"location":"api/http/#task-management-methods","title":"Task Management Methods","text":"<p>All task management methods use the <code>/tasks</code> endpoint with JSON-RPC 2.0 format.</p>"},{"location":"api/http/#taskscreate","title":"<code>tasks.create</code>","text":"<p>Description: Creates one or more tasks and automatically executes them as a task tree. This method validates task dependencies, ensures all tasks form a single tree structure, and handles task execution with proper dependency ordering. Tasks can be provided as a single object or an array of objects.</p> <p>Method: <code>tasks.create</code></p> <p>Parameters: - <code>tasks</code> (array or object, required): Array of task objects, or single task object (will be converted to array). All tasks must have the same <code>user_id</code> after resolution.</p> <p>Task Object Fields: - <code>id</code> (string, optional): Task ID. If not provided, auto-generated UUID will be used - <code>name</code> (string, required): Task name - <code>user_id</code> (string, optional): User ID for multi-user scenarios - <code>parent_id</code> (string, optional): Parent task ID for task tree structure. Note: Parent-child relationships are for organizational purposes only and do NOT affect execution order. Use <code>dependencies</code> to control execution order. - <code>priority</code> (integer, optional): Priority level (0=urgent, 1=high, 2=normal, 3=low). Default: 1 - <code>dependencies</code> (array, optional): Dependency list. Format: <code>[{\"id\": \"task-id\", \"required\": true}]</code>. This determines execution order - a task executes only when all its required dependencies are satisfied. - <code>inputs</code> (object, optional): Execution-time input parameters - <code>schemas</code> (object, optional): Task schemas configuration. If provided, the <code>method</code> field is REQUIRED.   - <code>method</code> (string, required when <code>schemas</code> is provided): Executor ID that must exactly match the executor's <code>id</code> from the extensions registry (registered via <code>@executor_register()</code>). This is the primary way to specify which executor should execute the task. Examples: <code>\"system_info_executor\"</code>, <code>\"command_executor\"</code>, <code>\"rest_executor\"</code>, <code>\"crewai_executor\"</code>, etc.   - <code>type</code> (string, optional): Executor type for fallback lookup (e.g., <code>\"stdio\"</code>, <code>\"http\"</code>). Only used if <code>method</code> is not a registered executor ID.   - <code>input_schema</code> (object, optional): Input validation schema (JSON Schema format)   - <code>model</code> (string, optional): Model name for AI executors (e.g., <code>\"gpt-4\"</code>, <code>\"claude-3-opus\"</code>) - <code>params</code> (object, optional): Executor initialization parameters. Can include <code>executor_id</code> as an alternative way to specify the executor (takes precedence over <code>schemas.method</code> if provided). - Custom fields: Any additional fields supported by your TaskModel</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.create\",\n  \"params\": [\n    {\n      \"id\": \"root\",\n      \"name\": \"Root Task\",\n      \"user_id\": \"user123\",\n      \"schemas\": {\n        \"method\": \"system_info_executor\"\n      },\n      \"inputs\": {\n        \"resource\": \"cpu\"\n      }\n    },\n    {\n      \"id\": \"child\",\n      \"name\": \"Child Task\",\n      \"user_id\": \"user123\",\n      \"parent_id\": \"root\",\n      \"schemas\": {\n        \"method\": \"command_executor\"\n      },\n      \"inputs\": {\n        \"command\": \"echo 'Processing system info'\"\n      }\n    }\n  ],\n  \"id\": \"create-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"create-request-1\",\n  \"result\": {\n    \"id\": \"root\",\n    \"name\": \"Root Task\",\n    \"status\": \"completed\",\n    \"progress\": 1.0,\n    \"user_id\": \"user123\",\n    \"children\": [\n      {\n        \"id\": \"child\",\n        \"name\": \"Child Task\",\n        \"status\": \"completed\",\n        \"progress\": 1.0,\n        \"parent_id\": \"root\",\n        \"children\": []\n      }\n    ]\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>id</code> (string): Root task ID - <code>name</code> (string): Root task name - <code>status</code> (string): Task status - <code>progress</code> (float): Overall progress (0.0 to 1.0) - <code>user_id</code> (string): User ID - <code>children</code> (array): Array of child task objects (nested structure)</p> <p>Validation Rules: - All tasks must form a single task tree (exactly one root task) - No circular dependencies allowed - All dependent tasks must be included in the input array - All tasks must be reachable from the root task via <code>parent_id</code> chain - All tasks must have the same <code>user_id</code> (or be accessible by authenticated user)</p> <p>Error Cases: - Circular dependency detected: Returns error with code -32602 - Multiple root tasks: Returns error with code -32602 - Missing required fields: Returns error with code -32602 - Permission denied: Returns error with code -32001</p> <p>Notes: - Tasks are automatically executed after creation - Task execution follows dependency order and priority - not parent-child relationships - Parent-child relationships (<code>parent_id</code>) are for organizing the task tree structure only - Dependencies (<code>dependencies</code>) determine when tasks execute - a task runs when its dependencies are satisfied - If a task fails, dependent tasks are not executed - Task IDs can be auto-generated if not provided</p>"},{"location":"api/http/#tasksget","title":"<code>tasks.get</code>","text":"<p>Description: Retrieves a task by its ID. Returns the complete task object including all fields such as status, progress, inputs, results, and metadata. This is a simple lookup operation that does not include child tasks.</p> <p>Method: <code>tasks.get</code></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID to retrieve. Can also use <code>id</code> as an alias.</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.get\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\"\n  },\n  \"id\": \"get-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"get-request-1\",\n  \"result\": {\n    \"id\": \"task-abc-123\",\n    \"name\": \"Task 1\",\n    \"status\": \"completed\",\n    \"progress\": 1.0,\n    \"user_id\": \"user123\",\n    \"parent_id\": null,\n    \"priority\": 1,\n    \"dependencies\": [],\n    \"inputs\": {\"resource\": \"cpu\"},\n    \"result\": {\"system\": \"Darwin\", \"brand\": \"Apple M1 Pro\", \"cores\": 10},\n    \"error\": null,\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"created_at\": \"2024-01-01T00:00:00Z\",\n    \"updated_at\": \"2024-01-01T00:05:00Z\",\n    \"started_at\": \"2024-01-01T00:01:00Z\",\n    \"completed_at\": \"2024-01-01T00:05:00Z\"\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>id</code> (string): Task ID - <code>name</code> (string): Task name - <code>status</code> (string): Task status (\"pending\", \"in_progress\", \"completed\", \"failed\", \"cancelled\") - <code>progress</code> (float): Progress (0.0 to 1.0) - <code>user_id</code> (string): User ID - <code>parent_id</code> (string, nullable): Parent task ID - <code>priority</code> (integer): Priority level - <code>dependencies</code> (array): Task dependencies - <code>inputs</code> (object): Input parameters - <code>result</code> (object, nullable): Execution result - <code>error</code> (string, nullable): Error message if failed - <code>schemas</code> (object): Task schemas - <code>created_at</code> (string): Creation timestamp (ISO 8601) - <code>updated_at</code> (string): Last update timestamp (ISO 8601) - <code>started_at</code> (string, nullable): Start timestamp (ISO 8601) - <code>completed_at</code> (string, nullable): Completion timestamp (ISO 8601)</p> <p>Error Cases: - Task not found: Returns <code>null</code> in result field - Permission denied: Returns error with code -32001</p> <p>Notes: - This method returns only the specified task, not its children - Use <code>tasks.tree</code> to get the full task tree structure - Task must be accessible by the authenticated user (if JWT is enabled)</p>"},{"location":"api/http/#tasksupdate","title":"<code>tasks.update</code>","text":"<p>Description: Updates task properties with critical field validation. This method allows partial updates to task fields. Critical fields (<code>parent_id</code>, <code>user_id</code>, <code>dependencies</code>) are strictly validated to prevent fatal errors, while other fields can be updated freely without status restrictions.</p> <p>Method: <code>tasks.update</code></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID to update - <code>status</code> (string, optional): New status (\"pending\", \"in_progress\", \"completed\", \"failed\", \"cancelled\") - <code>inputs</code> (object, optional): Updated input parameters (replaces entire inputs object) - <code>name</code> (string, optional): Updated task name - <code>priority</code> (integer, optional): Updated priority level - <code>params</code> (object, optional): Updated executor parameters - <code>schemas</code> (object, optional): Updated validation schemas - <code>dependencies</code> (array, optional): Updated dependencies list (see validation rules below) - <code>result</code> (object, optional): Updated result (replaces entire result object) - <code>error</code> (string, optional): Error message (typically set when status is \"failed\") - <code>progress</code> (float, optional): Progress value (0.0 to 1.0) - <code>started_at</code> (string, optional): Start timestamp (ISO 8601 format) - <code>completed_at</code> (string, optional): Completion timestamp (ISO 8601 format)</p> <p>Critical Field Validation Rules:</p> <ol> <li><code>parent_id</code> - Always rejected</li> <li>Cannot be modified after task creation</li> <li> <p>Error: \"Cannot update 'parent_id': field cannot be modified (task hierarchy is fixed)\"</p> </li> <li> <p><code>user_id</code> - Always rejected</p> </li> <li>Cannot be modified after task creation</li> <li> <p>Error: \"Cannot update 'user_id': field cannot be modified (task ownership is fixed)\"</p> </li> <li> <p><code>dependencies</code> - Conditional validation (only when updating)</p> </li> <li>Status check: Task must be in <code>pending</code> status<ul> <li>Error: \"Cannot update 'dependencies': task status is '{status}' (must be 'pending')\"</li> </ul> </li> <li>Reference validation: All dependency IDs must exist in the same task tree<ul> <li>Error: \"Dependency reference '{dep_id}' not found in task tree\"</li> </ul> </li> <li>Circular dependency detection: Prevents circular dependencies using DFS algorithm<ul> <li>Error: \"Circular dependency detected: Task A -&gt; Task B -&gt; Task A\"</li> </ul> </li> <li>Execution check: Prevents updates if dependent tasks are executing<ul> <li>Error: \"Cannot update dependencies: task '{task_id}' has dependent tasks that are executing: ['task-456']\"</li> </ul> </li> </ol> <p>Other Fields: - All other fields (<code>inputs</code>, <code>name</code>, <code>priority</code>, <code>params</code>, <code>schemas</code>, <code>status</code>, <code>result</code>, <code>error</code>, <code>progress</code>, timestamps) can be updated freely without status restrictions.</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.update\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"status\": \"in_progress\",\n    \"progress\": 0.5\n  },\n  \"id\": \"update-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"update-request-1\",\n  \"result\": {\n    \"id\": \"task-abc-123\",\n    \"name\": \"Task 1\",\n    \"status\": \"in_progress\",\n    \"progress\": 0.5,\n    \"user_id\": \"user123\",\n    \"inputs\": {\"key\": \"value\"},\n    \"updated_at\": \"2024-01-01T00:03:00Z\"\n  }\n}\n</code></pre></p> <p>Response Fields: Returns the complete updated task object with all fields.</p> <p>Example Request (Update Multiple Fields): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.update\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"name\": \"Updated Task Name\",\n    \"priority\": 3,\n    \"inputs\": {\"new_key\": \"new_value\"},\n    \"status\": \"in_progress\",\n    \"progress\": 0.5\n  },\n  \"id\": \"update-request-2\"\n}\n</code></pre></p> <p>Example Request (Update Dependencies - Valid): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.update\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"dependencies\": [\n      {\"id\": \"task-dep-1\", \"required\": true},\n      {\"id\": \"task-dep-2\", \"required\": false}\n    ]\n  },\n  \"id\": \"update-request-3\"\n}\n</code></pre></p> <p>Example Response (Error - Critical Field Validation): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"update-request-4\",\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Invalid params\",\n    \"data\": \"Update failed:\\n- Cannot update 'parent_id': field cannot be modified (task hierarchy is fixed)\\n- Cannot update 'dependencies': task status is 'completed' (must be 'pending')\"\n  }\n}\n</code></pre></p> <p>Error Cases: - Task not found: Returns error with code -32602 - Permission denied: Returns error with code -32001 - Critical field violation (<code>parent_id</code>, <code>user_id</code>): Returns error with detailed message - Dependencies validation failure: Returns error with specific validation failure reason:   - Task not in <code>pending</code> status   - Invalid dependency reference   - Circular dependency detected   - Dependent tasks are executing - Invalid field values: Returns error with code -32602</p> <p>Notes: - Updates are atomic and immediately persisted to the database - Critical fields (<code>parent_id</code>, <code>user_id</code>, <code>dependencies</code>) are validated strictly to prevent fatal errors - Other fields can be updated from any task status without restrictions - Dependencies can only be updated for <code>pending</code> tasks to ensure data integrity - All validation errors are collected and returned together in a single error message - Status changes may trigger dependent task execution - Timestamps should be in ISO 8601 format - Only authenticated users can update their own tasks (or admins can update any task)</p>"},{"location":"api/http/#tasksdelete","title":"<code>tasks.delete</code>","text":"<p>Description: Physically deletes a task from the database with comprehensive validation. The task and all its children are permanently removed if deletion conditions are met. Deletion is only allowed when: 1. All tasks (the task itself + all children recursively) are in <code>pending</code> status 2. No other tasks depend on the task being deleted</p> <p>If all conditions are met, the task and all its children are physically deleted. Otherwise, a detailed error message is returned explaining why deletion is not allowed.</p> <p>Method: <code>tasks.delete</code></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID to delete</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.delete\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\"\n  },\n  \"id\": \"delete-request-1\"\n}\n</code></pre></p> <p>Example Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"delete-request-1\",\n  \"result\": {\n    \"success\": true,\n    \"task_id\": \"task-abc-123\",\n    \"deleted_count\": 4,\n    \"children_deleted\": 3\n  }\n}\n</code></pre></p> <p>Response Fields (Success): - <code>success</code> (boolean): Whether deletion was successful - <code>task_id</code> (string): ID of the deleted task - <code>deleted_count</code> (integer): Total number of tasks deleted (including the main task and all children) - <code>children_deleted</code> (integer): Number of child tasks deleted</p> <p>Example Response (Error - Non-pending Children): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"delete-request-1\",\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Invalid params\",\n    \"data\": \"Cannot delete task: task has 2 non-pending children: [child-1: in_progress, child-2: completed]\"\n  }\n}\n</code></pre></p> <p>Example Response (Error - Dependent Tasks): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"delete-request-1\",\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Invalid params\",\n    \"data\": \"Cannot delete task: 2 tasks depend on this task: [dependent-1, dependent-2]\"\n  }\n}\n</code></pre></p> <p>Example Response (Error - Mixed Conditions): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"delete-request-1\",\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Invalid params\",\n    \"data\": \"Cannot delete task: task has 1 non-pending children: [child-1: completed]; 1 tasks depend on this task: [dependent-1]\"\n  }\n}\n</code></pre></p> <p>Error Cases: - Task not found: Returns error with code -32602 - Permission denied: Returns error with code -32001 - Task has non-pending children: Returns error with code -32602, includes list of non-pending children with their statuses - Task itself is not pending: Returns error with code -32602, indicates current status - Other tasks depend on this task: Returns error with code -32602, includes list of dependent task IDs - Mixed conditions: Returns error with code -32602, includes all blocking conditions</p> <p>Deletion Conditions: - All tasks must be pending: The task itself and all its children (recursively) must be in <code>pending</code> status - No dependencies: No other tasks can depend on the task being deleted - Recursive deletion: When deletion is allowed, all child tasks (including grandchildren) are automatically deleted</p> <p>Notes: - Tasks are physically deleted from the database (not soft-deleted) - Deletion is atomic: Either all tasks (task + children) are deleted, or none are deleted - Child tasks are automatically deleted when the parent task is deleted (if all are pending) - Deletion requires proper permissions (own task or admin role) - Error messages provide detailed information about why deletion failed - To delete a task with non-pending children, first cancel or complete those children - To delete a task with dependencies, first remove or update the dependent tasks</p>"},{"location":"api/http/#tasksclone","title":"<code>tasks.clone</code>","text":"<p>Description: Creates a new task or task tree by copying, linking, archiveting, or mixing origin types from an existing task. This endpoint supports advanced cloning options:</p> <ul> <li>Copy (default): Deeply copies the original task and its subtree. All fields are duplicated, and new IDs are generated. You can choose to copy only the selected task or the entire subtree.</li> <li>Link: Creates a new task that references the original, preserving its status and result. Only allowed if the source task tree is fully completed.</li> <li>Snapshot: Creates a read-only, immutable archive of the original task or subtree.</li> <li>Mixed: Allows selective linking and copying within the same operation, using <code>link_task_ids</code> to specify which tasks to link.</li> </ul> <p>Parameters: - <code>task_id</code> (string, required): ID of the task to clone. - <code>origin_type</code> (string, optional): One of <code>\"copy\"</code> (default), <code>\"link\"</code>, <code>\"archive\"</code>, or <code>\"mixed\"</code>. - <code>recursive</code> (boolean, optional): If <code>true</code> (default), clone the entire subtree; if <code>false</code>, only the specified task. - <code>link_task_ids</code> (array, optional): For <code>\"mixed\"</code> mode, list of task IDs to link instead of copy. - <code>reset_fields</code> (object, optional): Dictionary of fields to override/reset in the cloned tasks. - <code>save</code> (boolean, optional): If <code>true</code> (default), save the cloned tasks to the database and return the new tree; if <code>false</code>, return a preview array of the cloned tasks without saving.</p> <p>Behavior: - Validates permissions and task existence. - Supports advanced use cases: subtree promotion, dependency inclusion, and selective field resets. - Returns the new task tree (if <code>save=true</code>) or a preview array (if <code>save=false</code>).</p> <p>Error Cases: - Task not found - Invalid parameters (e.g., missing <code>link_task_ids</code> for <code>\"mixed\"</code>) - Permission denied</p> <p>Method: <code>tasks.clone</code></p> <p>What Gets Cloned: - The original task and all its children (recursive) - All tasks that depend on the original task (direct and transitive dependencies) - Task structure (parent-child relationships) - Task definitions (name, inputs, schemas, params, dependencies) - User and product associations (user_id, product_id) - Priority settings</p> <p>What Gets Reset (Minimal Mode): - Task IDs (new IDs generated) - Status (reset to \"pending\") - Progress (reset to 0.0) - Result (reset to null) - Error (reset to null) - Execution timestamps (started_at, completed_at)</p> <p>What Gets Reset (Full Mode): - Task IDs (new IDs generated) - Status: Tasks needing re-execution \u2192 \"pending\", unrelated tasks \u2192 \"completed\" - Progress: Re-execution tasks \u2192 0.0, unrelated tasks \u2192 1.0 - Result: Re-execution tasks \u2192 null, unrelated tasks \u2192 preserved with token_usage - Error: Re-execution tasks \u2192 null, unrelated tasks \u2192 preserved - Execution timestamps: Re-execution tasks \u2192 reset, unrelated tasks \u2192 preserved</p> <p>What Gets Preserved: - Task definitions (name, code, inputs, schemas, params) - User and product associations (user_id, product_id) - Priority settings - Dependencies structure</p> <p>Metadata: - <code>original_task_id</code>: Links cloned task to original task's root ID - <code>origin_type</code>: Task origin type (own, clone, reference, link, archive) - <code>task_tree_id</code>: Task tree identifier for grouping and querying across trees - <code>has_references</code>: Set to <code>true</code> on all original tasks that were cloned</p> <p>Parameters: - <code>task_id</code> (string, required): ID of the task to clone. Can be root task or any task in the tree. The method will clone the minimal subtree containing the task and all its dependencies. - <code>_recursive</code> (boolean, optional): If <code>true</code>, also clone each direct child task of the original task with its dependencies. When cloning children, tasks that depend on multiple cloned tasks are only cloned once (deduplication by task ID). Default: <code>false</code>. - <code>reset_fields</code> (array of strings, optional): List of field names to reset during clone. Common fields: <code>[\"status\", \"progress\", \"result\", \"error\"]</code>. Default: <code>null</code> (uses mode-specific defaults). - <code>save</code> (boolean, optional): If <code>true</code> (default), saves cloned tasks to database and returns TaskTreeNode. If <code>false</code>, returns task array without saving to database (suitable for preview or direct use with <code>tasks.create</code>).</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.clone\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"children\": false\n  },\n  \"id\": \"clone-request-1\"\n}\n</code></pre></p> <p>Example Request with save=false (returns task array): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.clone\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"save\": false\n  },\n  \"id\": \"clone-request-5\"\n}\n</code></pre></p> <p>Example Response (save=false): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"clone-request-5\",\n  \"result\": {\n    \"tasks\": [\n      {\n        \"id\": \"new-uuid-1\",\n        \"name\": \"Original Task Name\",\n        \"parent_id\": null,\n        \"dependencies\": [],\n        \"status\": \"pending\",\n        \"progress\": 0.0\n      },\n      {\n        \"id\": \"new-uuid-2\",\n        \"name\": \"Child Task\",\n        \"parent_id\": \"new-uuid-1\",\n        \"dependencies\": [{\"id\": \"new-uuid-1\", \"required\": true}],\n        \"status\": \"pending\",\n        \"progress\": 0.0\n      }\n    ],\n    \"saved\": false\n  }\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"clone-request-1\",\n  \"result\": {\n    \"id\": \"task-clone-xyz-789\",\n    \"name\": \"Original Task Name\",\n    \"original_task_id\": \"task-abc-123\",\n    \"status\": \"pending\",\n    \"progress\": 0.0,\n    \"children\": [\n      {\n        \"id\": \"child-clone-456\",\n        \"name\": \"Child Task\",\n        \"original_task_id\": \"task-abc-123\",\n        \"status\": \"pending\",\n        \"progress\": 0.0\n      }\n    ]\n  }\n}\n</code></pre></p> <p>Error Cases: - Task not found: Returns error with code -32602 - Permission denied: Returns error with code -32001</p> <p>Notes: - The cloned task tree has new task IDs (UUIDs) but preserves the original structure - All execution fields are reset (status=\"pending\", progress=0.0, result=null) unless <code>reset_fields</code> is specified - The original task's <code>has_references</code> flag is set to <code>true</code> when <code>save=true</code> - Dependencies correctly reference new task IDs within the cloned tree - When <code>save=false</code>, the returned task array is compatible with <code>tasks.create</code> API - Each cloned task's <code>original_task_id</code> points to its direct original counterpart (not root) - Failed leaf nodes are automatically handled (pending dependents are filtered out) - The cloned tree is ready for immediate execution</p>"},{"location":"api/http/#tasksgenerate","title":"<code>tasks.generate</code>","text":"<p>Description: Generates a valid task tree JSON array from a natural language requirement using LLM (Large Language Model). This method uses the <code>generate_executor</code> to create task structures that conform to the framework's requirements. The generated tasks can optionally be saved directly to the database for immediate execution.</p> <p>Method: <code>tasks.generate</code></p> <p>Parameters: - <code>requirement</code> (string, required): Natural language description of the desired task tree. Should describe the workflow, data flow, and operations needed. - <code>user_id</code> (string, optional): User ID for the generated tasks. If not provided and JWT is enabled, uses authenticated user's ID. If not provided and JWT is disabled, uses default user ID. - <code>llm_provider</code> (string, optional): LLM provider to use. Valid values: <code>\"openai\"</code>, <code>\"anthropic\"</code>. Default: <code>\"openai\"</code> or from environment variable <code>APFLOW_LLM_PROVIDER</code>. - <code>model</code> (string, optional): Specific LLM model name. Examples: <code>\"gpt-4o\"</code>, <code>\"claude-3-5-sonnet-20241022\"</code>. Default: Provider-specific default (from <code>OPENAI_MODEL</code> or <code>ANTHROPIC_MODEL</code> environment variables). - <code>temperature</code> (float, optional): LLM temperature for generation (0.0 to 2.0). Higher values make output more creative, lower values more deterministic. Default: <code>0.7</code>. - <code>max_tokens</code> (integer, optional): Maximum number of tokens in LLM response. Default: <code>4000</code>. - <code>save</code> (boolean, optional): If <code>true</code>, automatically saves the generated tasks to the database using <code>TaskCreator.create_task_tree_from_array()</code>. When <code>true</code>, the response includes <code>root_task_id</code> of the saved task tree. Default: <code>false</code>.</p> <p>LLM API Key Configuration: - Set <code>OPENAI_API_KEY</code> environment variable for OpenAI provider - Set <code>ANTHROPIC_API_KEY</code> environment variable for Anthropic provider - API keys can also be provided via <code>.env</code> file in the project root</p> <p>Example Request (Generate Only): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.generate\",\n  \"params\": {\n    \"requirement\": \"Fetch data from API, process it, and save to database\",\n    \"user_id\": \"user123\"\n  },\n  \"id\": \"generate-request-1\"\n}\n</code></pre></p> <p>Example Request (With LLM Configuration): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.generate\",\n  \"params\": {\n    \"requirement\": \"Create a workflow to fetch user data from REST API, transform it, and store in database\",\n    \"user_id\": \"user123\",\n    \"llm_provider\": \"openai\",\n    \"model\": \"gpt-4o\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 4000\n  },\n  \"id\": \"generate-request-2\"\n}\n</code></pre></p> <p>Example Request (Save to Database): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.generate\",\n  \"params\": {\n    \"requirement\": \"Fetch data from two APIs in parallel, merge results, and save to database\",\n    \"user_id\": \"user123\",\n    \"save\": true\n  },\n  \"id\": \"generate-request-3\"\n}\n</code></pre></p> <p>Example Response (Generate Only): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"generate-request-1\",\n  \"result\": {\n    \"tasks\": [\n      {\n        \"name\": \"rest_executor\",\n        \"inputs\": {\n          \"url\": \"https://api.example.com/data\",\n          \"method\": \"GET\"\n        },\n        \"priority\": 1\n      },\n      {\n        \"name\": \"command_executor\",\n        \"dependencies\": [{\"id\": \"task_1\", \"required\": true}],\n        \"inputs\": {\n          \"command\": \"python process_data.py --input /tmp/api_response.json\"\n        },\n        \"priority\": 2\n      },\n      {\n        \"name\": \"database_executor\",\n        \"dependencies\": [{\"id\": \"task_2\", \"required\": true}],\n        \"inputs\": {\n          \"table\": \"processed_data\",\n          \"data\": \"{{task_2.result}}\"\n        },\n        \"priority\": 2\n      }\n    ],\n    \"count\": 3,\n    \"message\": \"Successfully generated 3 task(s)\"\n  }\n}\n</code></pre></p> <p>Example Response (Save to Database): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"generate-request-3\",\n  \"result\": {\n    \"tasks\": [\n      {\n        \"name\": \"rest_executor\",\n        \"inputs\": {\n          \"url\": \"https://api1.example.com/data\",\n          \"method\": \"GET\"\n        },\n        \"priority\": 1\n      },\n      {\n        \"name\": \"rest_executor\",\n        \"inputs\": {\n          \"url\": \"https://api2.example.com/data\",\n          \"method\": \"GET\"\n        },\n        \"priority\": 1\n      },\n      {\n        \"name\": \"aggregate_results_executor\",\n        \"dependencies\": [\n          {\"id\": \"task_1\", \"required\": true},\n          {\"id\": \"task_2\", \"required\": true}\n        ],\n        \"inputs\": {\n          \"tasks\": [\"task_1\", \"task_2\"]\n        },\n        \"priority\": 2\n      }\n    ],\n    \"count\": 3,\n    \"root_task_id\": \"generated-root-task-abc-123\",\n    \"message\": \"Successfully generated 3 task(s) and saved to database (root_task_id: generated-root-task-abc-123)\"\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>tasks</code> (array): Generated task tree JSON array. Each task object follows the standard task format with <code>name</code>, <code>inputs</code>, <code>dependencies</code>, <code>priority</code>, etc. - <code>count</code> (integer): Number of tasks generated - <code>root_task_id</code> (string, optional): Present only when <code>save=true</code>. The ID of the root task in the saved task tree. - <code>message</code> (string): Status message describing the generation result</p> <p>Error Cases: - Missing <code>requirement</code>: Returns error with code -32602, message \"Requirement is a mandatory input for tasks.generate.\" - Missing LLM API key: Returns error with code -32602, message \"LLM API key not found. Please set OPENAI_API_KEY or ANTHROPIC_API_KEY environment variable or in a .env file.\" - LLM generation failed: Returns error with code -32602, includes error details from LLM API - Validation failed: Returns error with code -32602, includes validation error details - Permission denied: Returns error with code -32001 (if JWT is enabled and user cannot generate tasks for specified user_id)</p> <p>Validation: The generated tasks are automatically validated to ensure they conform to <code>TaskCreator.create_task_tree_from_array()</code> requirements: - All tasks have <code>name</code> field matching available executor IDs - Either all tasks have <code>id</code> field or none do (no mixing) - All <code>parent_id</code> references exist in the array - All <code>dependencies</code> references exist in the array - No circular dependencies - Single root task (task with no <code>parent_id</code>) - All tasks are reachable from the root task</p> <p>Notes: - The LLM uses framework documentation and available executor information to generate appropriate task structures - Generated tasks use realistic input parameters based on executor schemas - For better results, provide detailed requirements describing:   - Data flow between steps   - Parallel vs sequential operations   - Specific operations (API calls, database operations, file processing, etc.) - When <code>save=true</code>, the generated tasks are immediately saved and ready for execution - Use the returned <code>tasks</code> array with <code>TaskCreator.create_task_tree_from_array()</code> if you need custom processing before saving - The method respects permission checks if JWT is enabled - LLM API keys are never stored in the database, only used during generation - Generation may take several seconds depending on LLM provider and model</p> <p>Tips for Better Generation: - Use specific keywords: \"parallel\", \"sequential\", \"merge\", \"aggregate\" help guide the generation - Describe data flow: Explain how data moves between steps - Mention executors: Specify operations like \"API\", \"database\", \"file\", \"command\" for better executor selection - Be detailed: More context leads to more accurate task trees - Example: \"Fetch data from two different APIs in parallel, then merge the results and save to database\"</p>"},{"location":"api/http/#taskslist","title":"<code>tasks.list</code>","text":"<p>Description: Lists all tasks from the database with optional filtering by user ID, status, and pagination support. This method queries the database (not just running tasks) and returns tasks matching the specified filters. Results are sorted by creation time (newest first) and can be paginated using limit and offset.</p> <p>Method: <code>tasks.list</code></p> <p>Parameters: - <code>user_id</code> (string, optional): Filter tasks by user ID. If not provided and JWT is enabled, uses authenticated user's ID. If not provided and JWT is disabled, returns all tasks. - <code>status</code> (string, optional): Filter by task status. Valid values: \"pending\", \"in_progress\", \"completed\", \"failed\", \"cancelled\", \"deleted\". If not provided, returns tasks with any status. - <code>limit</code> (integer, optional): Maximum number of tasks to return (default: 100, maximum recommended: 1000) - <code>offset</code> (integer, optional): Number of tasks to skip for pagination (default: 0)</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.list\",\n  \"params\": {\n    \"user_id\": \"user123\",\n    \"status\": \"completed\",\n    \"limit\": 50,\n    \"offset\": 0\n  },\n  \"id\": \"list-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"list-request-1\",\n  \"result\": [\n    {\n      \"id\": \"task-1\",\n      \"name\": \"Task 1\",\n      \"status\": \"completed\",\n      \"progress\": 1.0,\n      \"user_id\": \"user123\",\n      \"created_at\": \"2024-01-01T00:00:00Z\",\n      \"completed_at\": \"2024-01-01T00:05:00Z\"\n    },\n    {\n      \"id\": \"task-2\",\n      \"name\": \"Task 2\",\n      \"status\": \"completed\",\n      \"progress\": 1.0,\n      \"user_id\": \"user123\",\n      \"created_at\": \"2024-01-01T00:01:00Z\",\n      \"completed_at\": \"2024-01-01T00:06:00Z\"\n    }\n  ]\n}\n</code></pre></p> <p>Response Fields: Returns an array of task objects, each containing: - All standard task fields (id, name, status, progress, user_id, etc.) - Tasks are sorted by <code>created_at</code> in descending order (newest first)</p> <p>Error Cases: - Permission denied: Tasks for other users are filtered out (not returned in error)</p> <p>Notes: - This method queries the database, not just running tasks - Use <code>tasks.running.list</code> to get only currently running tasks - Results are paginated using limit and offset - Deleted tasks are excluded from results - Use pagination for large result sets to avoid performance issues</p>"},{"location":"api/http/#tasksexecute","title":"<code>tasks.execute</code>","text":"<p>Description: Executes a task by its ID with automatic dependency handling. This method supports two execution modes: - Root task execution: If the specified task is a root task (no parent), the entire task tree is executed. - Child task execution: If the specified task is a child task, the method automatically collects all dependencies (including transitive dependencies) and executes the task along with all required dependencies.</p> <p>Task Status Handling: - Pending tasks: Execute normally (newly created tasks) - Failed tasks: Can be re-executed by calling this method with the failed task's ID - Completed tasks: Skipped unless they are dependencies of a task being re-executed (in which case they are also re-executed) - In-progress tasks: Skipped to avoid conflicts</p> <p>Re-execution Behavior: When re-executing a failed task, all its dependency tasks are also re-executed (even if they are completed) to ensure consistency. This ensures that the entire execution context is refreshed and results are up-to-date.</p> <p>The task must exist in the database and must not already be running. Execution follows dependency order and priority scheduling. This unified execution behavior ensures consistent task execution across both A2A Protocol and JSON-RPC endpoints.</p> <p>Method: <code>tasks.execute</code></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID to execute. Can also use <code>id</code> as an alias.    - If the task is a root task, the entire task tree will be executed.   - If the task is a child task, the task and all its dependencies (including transitive) will be executed. - <code>use_streaming</code> (boolean, optional): Whether to use streaming mode for real-time progress updates (default: false). If true, the endpoint returns a <code>StreamingResponse</code> with Server-Sent Events (SSE) instead of a JSON response. - <code>webhook_config</code> (object, optional): Webhook configuration for push notifications. If provided, task execution updates will be sent to the specified webhook URL via HTTP callbacks. This is similar to A2A Protocol's push notification feature.   - <code>url</code> (string, required): Webhook callback URL where updates will be sent   - <code>headers</code> (object, optional): HTTP headers to include in webhook requests (e.g., <code>{\"Authorization\": \"Bearer token\"}</code>)   - <code>method</code> (string, optional): HTTP method for webhook requests (default: \"POST\")   - <code>timeout</code> (float, optional): Request timeout in seconds (default: 30.0)   - <code>max_retries</code> (int, optional): Maximum retry attempts for failed webhook requests (default: 3)</p> <p>Example Request (Non-streaming): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.execute\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"use_streaming\": false\n  },\n  \"id\": \"execute-request-1\"\n}\n</code></pre></p> <p>Example Request (Streaming mode): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.execute\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"use_streaming\": true\n  },\n  \"id\": \"execute-request-1\"\n}\n</code></pre></p> <p>Example Request (Webhook mode): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.execute\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"webhook_config\": {\n      \"url\": \"https://example.com/api/task-callback\",\n      \"headers\": {\n        \"Authorization\": \"Bearer your-api-token\",\n        \"Content-Type\": \"application/json\"\n      },\n      \"method\": \"POST\",\n      \"timeout\": 30.0,\n      \"max_retries\": 3\n    }\n  },\n  \"id\": \"execute-request-1\"\n}\n</code></pre></p> <p>Example Response (Non-streaming): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"execute-request-1\",\n  \"result\": {\n    \"success\": true,\n    \"protocol\": \"jsonrpc\",\n    \"root_task_id\": \"task-abc-123\",\n    \"task_id\": \"task-abc-123\",\n    \"status\": \"started\",\n    \"message\": \"Task task-abc-123 execution started\"\n  }\n}\n</code></pre></p> <p>Example Response (Streaming mode): When <code>use_streaming=true</code>, the response is a Server-Sent Events (SSE) stream with <code>Content-Type: text/event-stream</code>.</p> <p>The first event contains the initial JSON-RPC response: <pre><code>data: {\"jsonrpc\": \"2.0\", \"id\": \"execute-request-1\", \"result\": {\"success\": true, \"protocol\": \"jsonrpc\", \"root_task_id\": \"task-abc-123\", \"task_id\": \"task-abc-123\", \"status\": \"started\", \"streaming\": true, \"message\": \"Task task-abc-123 execution started with streaming\"}}\n</code></pre></p> <p>Subsequent events contain real-time progress updates: <pre><code>data: {\"type\": \"progress\", \"task_id\": \"task-abc-123\", \"status\": \"in_progress\", \"progress\": 0.5, \"message\": \"Task tree execution started\", \"timestamp\": \"2025-11-26T08:00:00\"}\n\ndata: {\"type\": \"task_completed\", \"task_id\": \"task-abc-123\", \"status\": \"completed\", \"result\": {...}, \"timestamp\": \"2025-11-26T08:00:05\"}\n\ndata: {\"type\": \"final\", \"task_id\": \"task-abc-123\", \"status\": \"completed\", \"result\": {\"progress\": 1.0}, \"final\": true, \"timestamp\": \"2025-11-26T08:00:05\"}\n\ndata: {\"type\": \"stream_end\", \"task_id\": \"task-abc-123\"}\n</code></pre></p> <p>Note: When <code>use_streaming=true</code>, you must parse the SSE stream format (<code>data: {...}</code>) instead of expecting a JSON response.</p> <p>Example Response (Webhook mode): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"execute-request-1\",\n  \"result\": {\n    \"success\": true,\n    \"protocol\": \"jsonrpc\",\n    \"root_task_id\": \"task-abc-123\",\n    \"task_id\": \"task-abc-123\",\n    \"status\": \"started\",\n    \"streaming\": true,\n    \"message\": \"Task task-abc-123 execution started with webhook callbacks. Updates will be sent to https://example.com/callback\",\n    \"webhook_url\": \"https://example.com/callback\"\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>success</code> (boolean): Whether execution was started successfully - <code>protocol</code> (string): Protocol identifier, always <code>\"jsonrpc\"</code> for this endpoint. Used to distinguish from A2A protocol responses. - <code>root_task_id</code> (string): ID of the root task in the tree - <code>task_id</code> (string): ID of the task that was executed  - <code>status</code> (string): Execution status (\"started\", \"already_running\", \"failed\") - <code>message</code> (string): Status message - <code>streaming</code> (boolean, optional): Present when <code>use_streaming=true</code> or <code>webhook_config</code> is provided. Indicates that streaming/webhook mode is enabled. - <code>webhook_url</code> (string, optional): Present only when <code>webhook_config</code> is provided. The webhook URL where updates will be sent.</p> <p>Error Cases: - Task not found: Returns error with code -32602 - Permission denied: Returns error with code -32001 - Task already running: Returns success=false with status \"already_running\"</p> <p>Webhook Callback Format:</p> <p>When <code>webhook_config</code> is provided, the server will send HTTP POST requests to your webhook URL with the following payload format:</p> <pre><code>{\n  \"protocol\": \"jsonrpc\",\n  \"root_task_id\": \"task-abc-123\",\n  \"task_id\": \"task-abc-123\",\n  \"status\": \"completed\",\n  \"progress\": 1.0,\n  \"message\": \"Task execution completed\",\n  \"type\": \"final\",\n  \"timestamp\": \"2024-01-01T12:00:00Z\",\n  \"final\": true,\n  \"result\": {\n    \"status\": \"completed\",\n    \"progress\": 1.0,\n    \"root_task_id\": \"task-abc-123\",\n    \"task_count\": 1\n  }\n}\n</code></pre> <p>Webhook Update Types:</p> <p>The server sends different types of updates during task execution: - <code>task_start</code>: Task execution started - <code>progress</code>: Progress update (status, progress percentage) - <code>task_completed</code>: Task completed successfully - <code>task_failed</code>: Task execution failed - <code>final</code>: Final status update (always sent at the end)</p> <p>Webhook Retry Behavior:</p> <ul> <li>The server automatically retries failed webhook requests up to <code>max_retries</code> times</li> <li>Retries use exponential backoff (1s, 2s, 4s, ...)</li> <li>Client errors (4xx) are not retried</li> <li>Server errors (5xx) and network errors are retried</li> <li>Webhook failures are logged but do not affect task execution</li> </ul> <p>Notes: - Root task execution: If <code>task_id</code> refers to a root task, the entire task tree is executed. - Child task execution: If <code>task_id</code> refers to a child task, the method automatically collects all dependencies (including transitive dependencies) and executes only the required subtree containing the task and its dependencies. - Re-execution support: Failed tasks can be re-executed by calling this method with the failed task's ID. When re-executing, all dependency tasks are also re-executed to ensure consistency. - Execution order: Task execution follows dependency order and priority, not parent-child relationships. Parent-child relationships (<code>parent_id</code>) are for organizing the task tree structure only. Dependencies (<code>dependencies</code>) determine when tasks execute. - Task execution is asynchronous - the method returns immediately after starting execution - The unified execution logic ensures consistent behavior across A2A Protocol and JSON-RPC endpoints - Only <code>pending</code> and <code>failed</code> status tasks are executed; <code>completed</code> and <code>in_progress</code> tasks are skipped unless they are dependencies of a task being re-executed - Use <code>tasks.running.status</code> to check execution progress - Use <code>use_streaming=true</code> to receive real-time progress updates via Server-Sent Events (SSE) - the response will be a streaming response instead of JSON - The SSE stream includes the initial JSON-RPC response as the first event, followed by real-time progress updates - Use <code>webhook_config</code> to receive updates via HTTP callbacks (independent of response mode) - <code>webhook_config</code> and <code>use_streaming</code> can be used together - webhook callbacks will be sent regardless of response mode - Tasks are executed following dependency order and priority - All responses include <code>protocol: \"jsonrpc\"</code> field to identify this as a JSON-RPC protocol response - This differs from A2A Protocol responses (which use <code>protocol: \"a2a\"</code> in metadata and event data)</p>"},{"location":"api/http/#tasksdetail","title":"<code>tasks.detail</code>","text":"<p>Description: Retrieves detailed task information including all fields. This method is functionally equivalent to <code>tasks.get</code> and returns the complete task object with all metadata, inputs, results, and execution history.</p> <p>Method: <code>tasks.detail</code></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID to get details for</p> <p>Response Format: Same as <code>tasks.get</code> - returns complete task object.</p> <p>Notes: - This method is an alias for <code>tasks.get</code> - Use this method when you need explicit clarity that you're requesting detailed information - Returns the same response format as <code>tasks.get</code></p>"},{"location":"api/http/#taskschildren","title":"<code>tasks.children</code>","text":"<p>Description: Retrieves all child tasks of a specified parent task. This method returns a flat list of direct children (not a nested tree structure). Useful for getting immediate children without the full tree hierarchy.</p> <p>Method: <code>tasks.children</code></p> <p>Parameters: - <code>parent_id</code> (string, required): Parent task ID to get children for. Can also use <code>task_id</code> as an alias. - <code>task_id</code> (string, optional): Alternative parameter name for parent_id (same as <code>parent_id</code>)</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.children\",\n  \"params\": {\n    \"parent_id\": \"task-abc-123\"\n  },\n  \"id\": \"children-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"children-request-1\",\n  \"result\": [\n    {\n      \"id\": \"child-task-1\",\n      \"name\": \"Child Task 1\",\n      \"status\": \"completed\",\n      \"progress\": 1.0,\n      \"parent_id\": \"task-abc-123\",\n      \"user_id\": \"user123\"\n    },\n    {\n      \"id\": \"child-task-2\",\n      \"name\": \"Child Task 2\",\n      \"status\": \"in_progress\",\n      \"progress\": 0.5,\n      \"parent_id\": \"task-abc-123\",\n      \"user_id\": \"user123\"\n    }\n  ]\n}\n</code></pre></p> <p>Response Fields: Returns an array of task objects, each containing: - All standard task fields (id, name, status, progress, user_id, parent_id, etc.) - Each task object represents a direct child of the specified parent - Tasks are returned in a flat list (not nested)</p> <p>Error Cases: - Parent task not found: Returns error with code -32602 - Missing parent_id: Returns error with code -32602 - Permission denied: Returns error with code -32001 (for parent task access) - Child tasks with permission denied are filtered out (not returned, but no error)</p> <p>Notes: - Returns only direct children, not grandchildren or deeper descendants - Use <code>tasks.tree</code> to get the complete nested tree structure - Child tasks are filtered by permission - tasks you cannot access are not returned - Returns an empty array if the parent task has no children - Useful for pagination or when you only need immediate children</p>"},{"location":"api/http/#taskstree","title":"<code>tasks.tree</code>","text":"<p>Description: Retrieves the complete task tree structure starting from a specified task. If the specified task has a parent, the method automatically finds the root task and returns the entire tree. The response includes nested children in a hierarchical structure.</p> <p>Method: <code>tasks.tree</code></p> <p>Parameters: - <code>task_id</code> (string, optional): Task ID to start from. If the task has a parent, the root task will be found automatically. Either <code>task_id</code> or <code>root_id</code> is required. - <code>root_id</code> (string, optional): Alternative parameter name for root task ID. Can be used instead of <code>task_id</code>.</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.tree\",\n  \"params\": {\n    \"task_id\": \"child-task-id\"\n  },\n  \"id\": \"tree-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"tree-request-1\",\n  \"result\": {\n    \"id\": \"root-task-id\",\n    \"name\": \"Root Task\",\n    \"status\": \"completed\",\n    \"progress\": 1.0,\n    \"user_id\": \"user123\",\n    \"children\": [\n      {\n        \"id\": \"child-task-id\",\n        \"name\": \"Child Task\",\n        \"status\": \"completed\",\n        \"progress\": 1.0,\n        \"parent_id\": \"root-task-id\",\n        \"children\": []\n      }\n    ]\n  }\n}\n</code></pre></p> <p>Response Fields: - Root task object with all fields - <code>children</code> (array): Array of child task objects (recursive structure)   - Each child includes all task fields   - Each child may have its own <code>children</code> array</p> <p>Error Cases: - Task not found: Returns error with code -32602 - Permission denied: Returns error with code -32001</p> <p>Notes: - The tree structure is built recursively from the root task - All tasks in the tree are included, regardless of status - The tree structure preserves parent-child relationships - Use this method to visualize the complete task hierarchy</p>"},{"location":"api/http/#tasksrunninglist","title":"<code>tasks.running.list</code>","text":"<p>Description: Lists all currently running tasks from memory. This method queries the in-memory task tracker to find tasks that are actively executing. Tasks are sorted by creation time (newest first) and can be filtered by user ID.</p> <p>Method: <code>tasks.running.list</code></p> <p>Parameters: - <code>user_id</code> (string, optional): Filter tasks by user ID. If not provided and JWT is enabled, uses authenticated user's ID. If not provided and JWT is disabled, returns all running tasks. - <code>limit</code> (integer, optional): Maximum number of tasks to return (default: 100, maximum recommended: 1000)</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.running.list\",\n  \"params\": {\n    \"user_id\": \"user123\",\n    \"limit\": 50\n  },\n  \"id\": \"list-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"list-request-1\",\n  \"result\": [\n    {\n      \"id\": \"task-1\",\n      \"name\": \"Task 1\",\n      \"status\": \"in_progress\",\n      \"progress\": 0.3,\n      \"user_id\": \"user123\",\n      \"created_at\": \"2024-01-01T00:00:00Z\",\n      \"started_at\": \"2024-01-01T00:01:00Z\"\n    },\n    {\n      \"id\": \"task-2\",\n      \"name\": \"Task 2\",\n      \"status\": \"in_progress\",\n      \"progress\": 0.7,\n      \"user_id\": \"user123\",\n      \"created_at\": \"2024-01-01T00:00:00Z\",\n      \"started_at\": \"2024-01-01T00:01:00Z\"\n    }\n  ]\n}\n</code></pre></p> <p>Response Fields: Returns an array of task objects, each containing: - All standard task fields (id, name, status, progress, user_id, etc.) - Tasks are sorted by <code>created_at</code> in descending order (newest first)</p> <p>Error Cases: - Permission denied: Tasks for other users are filtered out (not returned in error)</p> <p>Notes: - This method queries in-memory task tracker, not the database - Only tasks that are actively running are returned - Completed or failed tasks are not included - Use <code>tasks.list</code> to query all tasks from the database - Results are limited to prevent performance issues</p>"},{"location":"api/http/#tasksrunningstatus","title":"<code>tasks.running.status</code>","text":"<p>Description: Gets the status of one or more tasks. This method checks both the in-memory task tracker (for active execution status) and the database (for persistent status). Returns detailed status information including progress, error messages, and execution timestamps.</p> <p>Method: <code>tasks.running.status</code></p> <p>Parameters: - <code>task_ids</code> (array, required): Array of task IDs to check status for. Can also use <code>context_ids</code> as an alias (for A2A Protocol compatibility). - <code>context_ids</code> (array, optional): Alternative parameter name for task IDs (same as <code>task_ids</code>)</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.running.status\",\n  \"params\": {\n    \"task_ids\": [\"task-1\", \"task-2\"]\n  },\n  \"id\": \"status-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"status-request-1\",\n  \"result\": [\n    {\n      \"task_id\": \"task-1\",\n      \"context_id\": \"task-1\",\n      \"status\": \"in_progress\",\n      \"progress\": 0.5,\n      \"error\": null,\n      \"is_running\": true,\n      \"started_at\": \"2024-01-01T00:00:00Z\",\n      \"updated_at\": \"2024-01-01T00:01:00Z\"\n    },\n    {\n      \"task_id\": \"task-2\",\n      \"context_id\": \"task-2\",\n      \"status\": \"completed\",\n      \"progress\": 1.0,\n      \"error\": null,\n      \"is_running\": false,\n      \"started_at\": \"2024-01-01T00:00:00Z\",\n      \"updated_at\": \"2024-01-01T00:02:00Z\"\n    }\n  ]\n}\n</code></pre></p> <p>Response Fields: Returns an array of task status objects, each containing: - <code>task_id</code> (string): Task ID - <code>context_id</code> (string): Context ID (same as task_id, for A2A Protocol compatibility) - <code>status</code> (string): Task status (\"pending\", \"in_progress\", \"completed\", \"failed\", \"cancelled\", \"not_found\", \"permission_denied\") - <code>progress</code> (float): Progress value (0.0 to 1.0) - <code>error</code> (string, nullable): Error message if task failed - <code>is_running</code> (boolean): Whether task is currently running in memory - <code>started_at</code> (string, nullable): ISO 8601 timestamp when task started - <code>updated_at</code> (string, nullable): ISO 8601 timestamp when task was last updated</p> <p>Error Cases: - Task not found: Returns status \"not_found\" for that task - Permission denied: Returns status \"permission_denied\" for that task</p> <p>Notes: - This method checks both in-memory task tracker and database - Returns status for all requested tasks, even if some are not found - Tasks that are not found but are running in memory will have status \"in_progress\" - Use this method to get real-time status of tasks</p>"},{"location":"api/http/#tasksrunningcount","title":"<code>tasks.running.count</code>","text":"<p>Description: Gets the count of currently running tasks. This method queries the in-memory task tracker to count tasks that are actively executing. Can be filtered by user ID to get user-specific counts.</p> <p>Method: <code>tasks.running.count</code></p> <p>Parameters: - <code>user_id</code> (string, optional): Filter by user ID. If not provided and JWT is enabled, uses authenticated user's ID. If not provided and JWT is disabled, returns total count of all running tasks.</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.running.count\",\n  \"params\": {\n    \"user_id\": \"user123\"\n  },\n  \"id\": \"count-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"count-request-1\",\n  \"result\": {\n    \"count\": 5,\n    \"user_id\": \"user123\"\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>count</code> (integer): Number of running tasks - <code>user_id</code> (string, optional): User ID filter applied (only present if user_id was specified)</p> <p>Error Cases: - Permission denied: Returns error with code -32001 (if trying to count another user's tasks without admin role)</p> <p>Notes: - Count is based on in-memory task tracker, not database - Only actively running tasks are counted - Returns 0 if no running tasks match the filter - Useful for monitoring system load and user activity</p>"},{"location":"api/http/#taskscancel-tasksrunningcancel","title":"<code>tasks.cancel</code> / <code>tasks.running.cancel</code>","text":"<p>Description: Cancels one or more running tasks. This method attempts to gracefully cancel tasks by calling the executor's <code>cancel()</code> method if supported. If force is enabled, tasks are immediately marked as cancelled. Returns detailed cancellation results including token usage and partial results if available.</p> <p>Method: <code>tasks.cancel</code> or <code>tasks.running.cancel</code> (both are equivalent)</p> <p>Parameters: - <code>task_ids</code> (array, required): Array of task IDs to cancel. Can also use <code>context_ids</code> as an alias (for A2A Protocol compatibility). - <code>context_ids</code> (array, optional): Alternative parameter name for task IDs - <code>force</code> (boolean, optional): Force immediate cancellation without waiting for graceful shutdown (default: false) - <code>error_message</code> (string, optional): Custom error message for cancellation. If not provided, defaults to \"Cancelled by user\" or \"Force cancelled by user\" based on force flag.</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.cancel\",\n  \"params\": {\n    \"task_ids\": [\"task-1\", \"task-2\"],\n    \"force\": false\n  },\n  \"id\": \"cancel-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"cancel-request-1\",\n  \"result\": [\n    {\n      \"task_id\": \"task-1\",\n      \"status\": \"cancelled\",\n      \"message\": \"Task cancelled successfully\",\n      \"force\": false,\n      \"token_usage\": {\n        \"total_tokens\": 1000,\n        \"prompt_tokens\": 500,\n        \"completion_tokens\": 500\n      },\n      \"result\": null\n    },\n    {\n      \"task_id\": \"task-2\",\n      \"status\": \"cancelled\",\n      \"message\": \"Task cancelled successfully\",\n      \"force\": false,\n      \"token_usage\": null,\n      \"result\": null\n    }\n  ]\n}\n</code></pre></p> <p>Response Fields: Returns an array of cancellation result objects, each containing: - <code>task_id</code> (string): Task ID that was cancelled - <code>status</code> (string): Final status (\"cancelled\" or \"failed\") - <code>message</code> (string): Cancellation message - <code>force</code> (boolean): Whether force cancellation was used - <code>token_usage</code> (object, nullable): Token usage information if available - <code>result</code> (any, nullable): Partial result if available</p> <p>Error Cases: - Task not found: Returns status \"error\" with error message - Permission denied: Returns status \"failed\" with \"permission_denied\" error - Task already completed: May return status \"failed\" if task cannot be cancelled</p> <p>Notes: - Cancellation attempts to call executor's <code>cancel()</code> method if supported - Force cancellation immediately marks task as cancelled without waiting - Token usage is recorded if available from the executor - Partial results may be returned if task was partially completed - Child tasks are not automatically cancelled (cancel parent task to cancel children)</p>"},{"location":"api/http/#system-endpoints","title":"System Endpoints","text":""},{"location":"api/http/#post-system","title":"<code>POST /system</code>","text":"<p>Description: Unified system operations endpoint that handles system-level operations via JSON-RPC 2.0 format. This endpoint provides health checks, configuration management, and example data initialization.</p> <p>Authentication: Optional (JWT token in <code>Authorization</code> header if JWT is enabled)</p> <p>Request Format: JSON-RPC 2.0 format with method-specific parameters:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"system.health\",\n  \"params\": {},\n  \"id\": \"request-123\"\n}\n</code></pre> <p>Supported Methods: - <code>system.health</code> - Check system health status - <code>system.executors</code> - Get available executors based on configuration - <code>config.llm_key.set</code> - Set LLM API key for user - <code>config.llm_key.get</code> - Get LLM API key status - <code>config.llm_key.delete</code> - Delete LLM API key</p> <p>Response Format: JSON-RPC 2.0 response format. The <code>result</code> field varies by method.</p> <p>Notes: - System operations may require authentication depending on configuration - Some operations (like LLM key management) require proper permissions</p>"},{"location":"api/http/#systemhealth","title":"<code>system.health</code>","text":"<p>Description: Checks the system health status and returns basic system information including version, uptime, and running tasks count. This endpoint is useful for monitoring and health checks.</p> <p>Method: <code>system.health</code></p> <p>Parameters: None</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"system.health\",\n  \"params\": {},\n  \"id\": \"health-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"health-request-1\",\n  \"result\": {\n    \"status\": \"healthy\",\n    \"message\": \"apflow is healthy\",\n    \"version\": \"0.2.0\",\n    \"timestamp\": \"2024-01-01T00:00:00Z\",\n    \"running_tasks_count\": 0\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>status</code> (string): Health status (\"healthy\" or \"unhealthy\") - <code>message</code> (string): Health status message - <code>version</code> (string): Service version - <code>timestamp</code> (string): Current timestamp (ISO 8601) - <code>running_tasks_count</code> (integer): Number of currently running tasks</p> <p>Notes: - This endpoint does not require authentication - Useful for load balancer health checks - Returns basic system information for monitoring</p>"},{"location":"api/http/#systemexecutors","title":"<code>system.executors</code>","text":"<p>Description: Gets the list of available executors based on the APFLOW_EXTENSIONS configuration. This method returns metadata for all executors that are currently accessible. If APFLOW_EXTENSIONS is set, only executors from enabled extensions are returned (security restriction).</p> <p>Method: <code>system.executors</code></p> <p>Parameters: None</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"system.executors\",\n  \"params\": {},\n  \"id\": \"executors-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"executors-request-1\",\n  \"result\": {\n    \"executors\": [\n      {\n        \"id\": \"system_info_executor\",\n        \"name\": \"System Info Executor\",\n        \"extension\": \"stdio\",\n        \"description\": \"Retrieve system information like CPU, memory, disk usage\"\n      },\n      {\n        \"id\": \"command_executor\",\n        \"name\": \"Command Executor\",\n        \"extension\": \"stdio\",\n        \"description\": \"Execute shell commands on the local system\"\n      },\n      {\n        \"id\": \"rest_executor\",\n        \"name\": \"REST Executor\",\n        \"extension\": \"http\",\n        \"description\": \"Make HTTP REST API calls\"\n      }\n    ],\n    \"count\": 3,\n    \"restricted\": false\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>executors</code> (array): List of available executors, each with:   - <code>id</code> (string): Unique executor identifier (e.g., \"command_executor\")   - <code>name</code> (string): Human-readable executor name   - <code>extension</code> (string): Extension this executor belongs to   - <code>description</code> (string): Detailed description of what the executor does - <code>count</code> (integer): Total number of available executors - <code>restricted</code> (boolean): Whether executor access is restricted by APFLOW_EXTENSIONS - <code>allowed_ids</code> (array, optional): List of allowed executor IDs (only present if <code>restricted</code> is true)</p> <p>Restricted Response Example: When APFLOW_EXTENSIONS environment variable is set to \"stdio,http\": <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"executors-request-1\",\n  \"result\": {\n    \"executors\": [\n      {\n        \"id\": \"system_info_executor\",\n        \"name\": \"System Info Executor\",\n        \"extension\": \"stdio\",\n        \"description\": \"Retrieve system information like CPU, memory, disk usage\"\n      },\n      {\n        \"id\": \"command_executor\",\n        \"name\": \"Command Executor\",\n        \"extension\": \"stdio\",\n        \"description\": \"Execute shell commands on the local system\"\n      },\n      {\n        \"id\": \"rest_executor\",\n        \"name\": \"REST Executor\",\n        \"extension\": \"http\",\n        \"description\": \"Make HTTP REST API calls\"\n      }\n    ],\n    \"count\": 3,\n    \"restricted\": true,\n    \"allowed_ids\": [\"system_info_executor\", \"command_executor\", \"rest_executor\"]\n  }\n}\n</code></pre></p> <p>Notes: - This endpoint does not require authentication - Returns all available executors if APFLOW_EXTENSIONS is not set - Returns only restricted executors if APFLOW_EXTENSIONS is set - Useful for discovery of available executor types before executing tasks - The executor metadata can be used to validate task schemas against available executors</p>"},{"location":"api/http/#configllm_keyset","title":"<code>config.llm_key.set</code>","text":"<p>Description: Sets an LLM API key for a user. This method stores the API key securely for use in LLM-based tasks (e.g., CrewAI tasks). The key is associated with a user ID and optional provider name. Keys are stored securely and never returned in responses.</p> <p>Method: <code>config.llm_key.set</code></p> <p>Parameters: - <code>api_key</code> (string, required): LLM API key to store - <code>user_id</code> (string, optional): User ID to associate the key with. If not provided and JWT is enabled, uses authenticated user's ID. If not provided and JWT is disabled, raises an error. - <code>provider</code> (string, optional): Provider name (e.g., \"openai\", \"anthropic\", \"google\", \"gemini\", \"mistral\", \"groq\", \"cohere\", \"together\"). If not provided, the provider will be auto-detected from the API key format or use \"default\".</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"config.llm_key.set\",\n  \"params\": {\n    \"api_key\": \"sk-your-api-key\",\n    \"user_id\": \"user123\",\n    \"provider\": \"openai\"\n  },\n  \"id\": \"set-key-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"set-key-request-1\",\n  \"result\": {\n    \"success\": true,\n    \"user_id\": \"user123\",\n    \"provider\": \"openai\"\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>success</code> (boolean): Whether the key was set successfully - <code>user_id</code> (string): User ID the key is associated with - <code>provider</code> (string): Provider name (or \"default\" if not specified)</p> <p>Error Cases: - Missing api_key: Returns error with code -32602 - Missing user_id (and not authenticated): Returns error with code -32602 - Permission denied: Returns error with code -32001 - Extension not available: Returns error if llm-key-config extension is not installed</p> <p>Notes: - Requires the <code>llm-key-config</code> extension to be installed - Keys are stored securely and never returned in API responses - Keys are used during task execution and cleared after completion - Multiple keys can be stored per user (one per provider) - Keys take precedence over environment variables during task execution</p>"},{"location":"api/http/#configllm_keyget","title":"<code>config.llm_key.get</code>","text":"<p>Description: Gets the status of LLM API keys for a user. This method checks if keys exist without returning the actual key values (for security). Returns information about which providers have keys configured.</p> <p>Method: <code>config.llm_key.get</code></p> <p>Parameters: - <code>user_id</code> (string, optional): User ID to check keys for. If not provided and JWT is enabled, uses authenticated user's ID. If not provided and JWT is disabled, uses \"default\". - <code>provider</code> (string, optional): Provider name to check. If not provided, returns status for all providers.</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"config.llm_key.get\",\n  \"params\": {\n    \"user_id\": \"user123\",\n    \"provider\": \"openai\"\n  },\n  \"id\": \"get-key-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"get-key-request-1\",\n  \"result\": {\n    \"has_key\": true,\n    \"user_id\": \"user123\",\n    \"provider\": \"openai\",\n    \"providers\": {\n      \"openai\": true,\n      \"anthropic\": false,\n      \"google\": true\n    }\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>has_key</code> (boolean): Whether a key exists for the specified provider (or any provider if provider not specified) - <code>user_id</code> (string): User ID checked - <code>provider</code> (string, nullable): Provider name checked (or null if checking all) - <code>providers</code> (object): Dictionary of provider names to boolean values indicating key existence</p> <p>Error Cases: - Permission denied: Returns empty status (graceful degradation) - Extension not available: Returns empty status (graceful degradation)</p> <p>Notes: - This method never returns the actual API key values (for security) - Returns graceful responses even if the extension is not available - Useful for checking key configuration status before task execution - Returns false/empty status if extension is not installed (graceful degradation)</p>"},{"location":"api/http/#configllm_keydelete","title":"<code>config.llm_key.delete</code>","text":"<p>Description: Deletes an LLM API key for a user. This method removes the stored API key for a specific provider, or all keys for the user if no provider is specified.</p> <p>Method: <code>config.llm_key.delete</code></p> <p>Parameters: - <code>user_id</code> (string, optional): User ID to delete keys for. If not provided and JWT is enabled, uses authenticated user's ID. If not provided and JWT is disabled, raises an error. - <code>provider</code> (string, optional): Provider name to delete key for. If not provided, deletes all keys for the user.</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"config.llm_key.delete\",\n  \"params\": {\n    \"user_id\": \"user123\",\n    \"provider\": \"openai\"\n  },\n  \"id\": \"delete-key-request-1\"\n}\n</code></pre></p> <p>Example Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"delete-key-request-1\",\n  \"result\": {\n    \"success\": true,\n    \"user_id\": \"user123\",\n    \"deleted\": true,\n    \"provider\": \"openai\"\n  }\n}\n</code></pre></p> <p>Response Fields: - <code>success</code> (boolean): Whether the operation was successful - <code>user_id</code> (string): User ID the key was deleted for - <code>deleted</code> (boolean): Whether a key was actually deleted (false if key didn't exist) - <code>provider</code> (string): Provider name (or \"all\" if no provider specified)</p> <p>Error Cases: - Missing user_id (and not authenticated): Returns error with code -32602 - Permission denied: Returns error with code -32001 - Extension not available: Returns error if llm-key-config extension is not installed</p> <p>Notes: - Requires the <code>llm-key-config</code> extension to be installed - If provider is not specified, all keys for the user are deleted - Returns <code>deleted: false</code> if the key didn't exist (not an error) - Keys are permanently deleted and cannot be recovered</p>"},{"location":"api/http/#authentication","title":"Authentication","text":""},{"location":"api/http/#jwt-authentication","title":"JWT Authentication","text":"<p>The API supports optional JWT authentication. To enable:</p> <ol> <li>Set environment variable <code>APFLOW_JWT_SECRET</code></li> <li>Include JWT token in request headers or cookies</li> </ol> <p>Token Sources (Priority Order): 1. Authorization Header (highest priority): <code>Authorization: Bearer &lt;token&gt;</code> 2. Cookie (fallback): <code>Authorization</code> cookie containing the token</p> <p>Token Format: <pre><code># Header format\nAuthorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\n\n# Cookie format (alternative)\nCookie: Authorization=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\n</code></pre></p> <p>Token Payload: <pre><code>{\n  \"user_id\": \"user123\",\n  \"sub\": \"user123\",  // Standard JWT claim (used if user_id not present)\n  \"roles\": [\"admin\"],\n  \"exp\": 1234567890\n}\n</code></pre></p> <p>Token Generation: You can generate JWT tokens using the <code>generate_token()</code> function:</p> <pre><code>from apflow.api.a2a.server import generate_token\n\n# Generate token with user_id\npayload = {\"user_id\": \"user123\", \"roles\": [\"admin\"]}\nsecret_key = \"your-secret-key\"\ntoken = generate_token(payload, secret_key, expires_in_days=30)\n\n# Use token in requests\nheaders = {\"Authorization\": f\"Bearer {token}\"}\n</code></pre> <p>Permission Checking: - If <code>roles</code> contains <code>\"admin\"</code>, user can access any task - Otherwise, user can only access tasks with matching <code>user_id</code> - If no <code>user_id</code> in token, the <code>sub</code> field is used as fallback - If no <code>user_id</code> or <code>sub</code> in token, permission checking is skipped</p>"},{"location":"api/http/#llm-api-key-headers","title":"LLM API Key Headers","text":"<p>For tasks that require LLM API keys (e.g., CrewAI tasks), you can provide keys via request headers.</p>"},{"location":"api/http/#x-llm-api-key-header","title":"X-LLM-API-KEY Header","text":"<p>Format: - Simple: <code>X-LLM-API-KEY: &lt;api-key&gt;</code> (provider auto-detected from model name) - Provider-specific: <code>X-LLM-API-KEY: &lt;provider&gt;:&lt;api-key&gt;</code></p> <p>Examples: <pre><code># OpenAI (auto-detected from model name)\ncurl -X POST http://localhost:8000/tasks \\\n  -H \"X-LLM-API-KEY: sk-your-openai-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{...}'\n\n# OpenAI (explicit provider)\ncurl -X POST http://localhost:8000/tasks \\\n  -H \"X-LLM-API-KEY: openai:sk-your-openai-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{...}'\n\n# Anthropic\ncurl -X POST http://localhost:8000/tasks \\\n  -H \"X-LLM-API-KEY: anthropic:sk-ant-your-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{...}'\n</code></pre></p> <p>Supported Providers: - <code>openai</code> - OpenAI (GPT models) - <code>anthropic</code> - Anthropic (Claude models) - <code>google</code> / <code>gemini</code> - Google (Gemini models) - <code>mistral</code> - Mistral AI - <code>groq</code> - Groq - <code>cohere</code> - Cohere - <code>together</code> - Together AI - And more (see provider list in LLM Key Injector)</p> <p>Priority Order: 1. Request header (<code>X-LLM-API-KEY</code>) - highest priority 2. User config (if <code>llm-key-config</code> extension is installed) 3. Environment variables (automatically read by CrewAI/LiteLLM)</p> <p>Note: LLM keys are never stored in the database. They are only used during task execution and cleared after completion.</p>"},{"location":"api/http/#error-responses","title":"Error Responses","text":"<p>All endpoints return JSON-RPC 2.0 error format on failure:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-id\",\n  \"error\": {\n    \"code\": -32603,\n    \"message\": \"Internal error\",\n    \"data\": \"Error details\"\n  }\n}\n</code></pre> <p>Error Codes: - <code>-32600</code>: Invalid Request - <code>-32601</code>: Method not found - <code>-32602</code>: Invalid params - <code>-32603</code>: Internal error - <code>-32000</code>: Server error (custom)</p>"},{"location":"api/http/#streaming-support","title":"Streaming Support","text":"<p>The API supports real-time progress updates via Server-Sent Events (SSE) and WebSocket. This allows clients to receive live updates about task execution progress without polling.</p>"},{"location":"api/http/#server-sent-events-sse-via-tasksexecute","title":"Server-Sent Events (SSE) via <code>tasks.execute</code>","text":"<p>Description: When using <code>tasks.execute</code> with <code>use_streaming=true</code>, the endpoint returns a <code>StreamingResponse</code> with Server-Sent Events (SSE). The response stream includes the initial JSON-RPC response followed by real-time progress updates.</p> <p>Usage: Set <code>use_streaming=true</code> in the <code>tasks.execute</code> request parameters. The endpoint will return a streaming response with <code>Content-Type: text/event-stream</code>.</p> <p>Example Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.execute\",\n  \"params\": {\n    \"task_id\": \"task-abc-123\",\n    \"use_streaming\": true\n  },\n  \"id\": \"execute-request-1\"\n}\n</code></pre></p> <p>Response Format: The response is an SSE stream. The first event contains the initial JSON-RPC response: <pre><code>data: {\"jsonrpc\": \"2.0\", \"id\": \"execute-request-1\", \"result\": {\"success\": true, \"protocol\": \"jsonrpc\", \"root_task_id\": \"task-abc-123\", \"task_id\": \"task-abc-123\", \"status\": \"started\", \"streaming\": true, \"message\": \"Task task-abc-123 execution started with streaming\"}}\n</code></pre></p> <p>Subsequent events contain real-time progress updates: <pre><code>data: {\"type\": \"progress\", \"task_id\": \"task-abc-123\", \"status\": \"in_progress\", \"progress\": 0.5, \"message\": \"Task tree execution started\", \"timestamp\": \"2025-11-26T08:00:00\"}\n\ndata: {\"type\": \"task_completed\", \"task_id\": \"task-abc-123\", \"status\": \"completed\", \"result\": {...}, \"timestamp\": \"2025-11-26T08:00:05\"}\n\ndata: {\"type\": \"final\", \"task_id\": \"task-abc-123\", \"status\": \"completed\", \"result\": {\"progress\": 1.0}, \"final\": true, \"timestamp\": \"2025-11-26T08:00:05\"}\n\ndata: {\"type\": \"stream_end\", \"task_id\": \"task-abc-123\"}\n</code></pre></p> <p>Event Types: - <code>progress</code>: Task progress update - <code>task_start</code>: Task execution started - <code>task_completed</code>: Task completed successfully - <code>task_failed</code>: Task execution failed - <code>final</code>: Final status update (always sent at the end) - <code>stream_end</code>: Stream connection closed</p> <p>Notes: - Connection remains open until task completes or client disconnects - Events are sent in real-time as tasks execute - Must parse SSE format (<code>data: {...}</code>) instead of expecting JSON response - Suitable for web applications and simple client implementations - Can be combined with <code>webhook_config</code> for dual update delivery</p>"},{"location":"api/http/#websocket-ws","title":"<code>WebSocket /ws</code>","text":"<p>Description: WebSocket endpoint for bidirectional real-time communication. This endpoint supports both receiving task execution updates and sending commands. WebSocket provides lower latency and better performance than SSE for high-frequency updates.</p> <p>Authentication: Optional (JWT token in WebSocket handshake headers if JWT is enabled)</p> <p>Connection: - Protocol: WebSocket (ws:// or wss://) - URL: <code>ws://localhost:8000/ws</code> or <code>wss://localhost:8000/ws</code> (secure) - Headers: Standard WebSocket headers, plus optional <code>Authorization: Bearer &lt;token&gt;</code></p> <p>Message Format: Messages are sent and received as JSON:</p> <p>Incoming Messages (from server): <pre><code>{\n  \"type\": \"task_progress\",\n  \"task_id\": \"task-abc-123\",\n  \"context_id\": \"task-abc-123\",\n  \"data\": {\n    \"progress\": 0.5,\n    \"status\": \"in_progress\"\n  }\n}\n</code></pre></p> <p>Outgoing Messages (to server): <pre><code>{\n  \"action\": \"subscribe\",\n  \"task_id\": \"task-abc-123\"\n}\n</code></pre></p> <p>Supported Actions: - <code>subscribe</code>: Subscribe to task updates   - <code>task_id</code> (string, required): Task ID to subscribe to - <code>unsubscribe</code>: Unsubscribe from task updates   - <code>task_id</code> (string, required): Task ID to unsubscribe from - <code>ping</code>: Keep-alive ping (server responds with <code>pong</code>)</p> <p>Example JavaScript Client: <pre><code>const ws = new WebSocket('ws://localhost:8000/ws');\n\nws.onopen = () =&gt; {\n  // Subscribe to task updates\n  ws.send(JSON.stringify({\n    action: 'subscribe',\n    task_id: 'task-abc-123'\n  }));\n};\n\nws.onmessage = (event) =&gt; {\n  const message = JSON.parse(event.data);\n  console.log('Update:', message);\n\n  if (message.type === 'task_progress') {\n    console.log(`Task ${message.task_id}: ${message.data.progress * 100}%`);\n  }\n};\n\nws.onerror = (error) =&gt; {\n  console.error('WebSocket error:', error);\n};\n\nws.onclose = () =&gt; {\n  console.log('WebSocket closed');\n};\n</code></pre></p> <p>Event Types: Similar to SSE streaming in <code>tasks.execute</code> (when <code>use_streaming=true</code>): - <code>task_progress</code>: Task progress update - <code>task_status</code>: Task status change - <code>task_completed</code>: Task completion notification - <code>task_failed</code>: Task failure notification - <code>pong</code>: Response to ping (keep-alive)</p> <p>Notes: - WebSocket provides bidirectional communication - Lower latency than SSE for high-frequency updates - Supports multiple subscriptions per connection - Automatic reconnection recommended for production use - Use secure WebSocket (wss://) in production environments - Suitable for real-time dashboards and interactive applications</p> <p>Comparison: SSE (via tasks.execute) vs WebSocket: - SSE (tasks.execute with use_streaming=true): Integrated with task execution, returns StreamingResponse directly, simpler for JSON-RPC clients, unidirectional (server to client) - WebSocket (/ws): Lower latency, bidirectional communication, better for high-frequency updates, requires custom reconnection logic</p>"},{"location":"api/http/#examples","title":"Examples","text":""},{"location":"api/http/#complete-example-create-and-execute-task-tree","title":"Complete Example: Create and Execute Task Tree","text":"<pre><code>curl -X POST http://localhost:8000/tasks \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.create\",\n    \"params\": [\n      {\n        \"id\": \"root\",\n        \"name\": \"Root Task\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\"method\": \"system_info_executor\"},\n        \"inputs\": {\"resource\": \"cpu\"}\n      },\n      {\n        \"id\": \"child\",\n        \"name\": \"Child Task\",\n        \"user_id\": \"user123\",\n        \"parent_id\": \"root\",\n        \"schemas\": {\"method\": \"command_executor\"},\n        \"inputs\": {\"command\": \"echo 'Processing system info'\"}\n      }\n    ],\n    \"id\": \"1\"\n  }'\n</code></pre>"},{"location":"api/http/#example-with-authentication","title":"Example with Authentication","text":"<pre><code>curl -X POST http://localhost:8000/tasks \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer YOUR_JWT_TOKEN\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.get\",\n    \"params\": {\n      \"task_id\": \"task-abc-123\"\n    },\n    \"id\": \"2\"\n  }'\n</code></pre>"},{"location":"api/http/#protocol-identification","title":"Protocol Identification","text":"<p>The API supports two execution protocols, each with distinct response formats. To help clients identify which protocol a response belongs to, all responses include a <code>protocol</code> field.</p>"},{"location":"api/http/#json-rpc-protocol-tasks-endpoint","title":"JSON-RPC Protocol (<code>/tasks</code> endpoint)","text":"<p>When using the <code>/tasks</code> endpoint with <code>tasks.execute</code>, responses include:</p> <pre><code>{\n  \"success\": true,\n  \"protocol\": \"jsonrpc\",  // Protocol identifier\n  \"root_task_id\": \"...\",\n  \"task_id\": \"...\",\n  \"status\": \"started\",\n  ...\n}\n</code></pre> <p>Identification: - <code>protocol: \"jsonrpc\"</code> field in the response - <code>success</code> field present - No <code>kind</code> field</p>"},{"location":"api/http/#a2a-protocol-endpoint","title":"A2A Protocol (<code>/</code> endpoint)","text":"<p>When using the A2A Protocol endpoint (<code>/</code>), responses include:</p> <pre><code>{\n  \"id\": \"...\",\n  \"kind\": \"task\",\n  \"metadata\": {\n    \"protocol\": \"a2a\",  // Protocol identifier in metadata\n    ...\n  },\n  \"status\": {\n    \"message\": {\n      \"parts\": [{\n        \"data\": {\n          \"protocol\": \"a2a\",  // Protocol identifier in event data\n          ...\n        }\n      }]\n    }\n  }\n}\n</code></pre> <p>Identification: - <code>protocol: \"a2a\"</code> in <code>metadata</code> field - <code>protocol: \"a2a\"</code> in status message parts data - <code>kind: \"task\"</code> field present - No <code>success</code> field</p>"},{"location":"api/http/#how-to-distinguish-protocols","title":"How to Distinguish Protocols","text":"<p>Method 1: Check <code>protocol</code> field (Recommended) <pre><code># JSON-RPC response\nif response.get(\"protocol\") == \"jsonrpc\":\n    # Handle JSON-RPC response\n    pass\n\n# A2A Protocol response\nif response.get(\"metadata\", {}).get(\"protocol\") == \"a2a\":\n    # Handle A2A Protocol response\n    pass\n\n# Or check in event data\nif event_data.get(\"protocol\") == \"a2a\":\n    # Handle A2A Protocol event\n    pass\n</code></pre></p> <p>Method 2: Check existing fields (Backward compatible) <pre><code># JSON-RPC response\nif \"success\" in response:\n    # Handle JSON-RPC response\n    pass\n\n# A2A Protocol response\nif \"kind\" in response and response[\"kind\"] == \"task\":\n    # Handle A2A Protocol response\n    pass\n</code></pre></p>"},{"location":"api/http/#customa2astarletteapplication-custom-routes","title":"CustomA2AStarletteApplication Custom Routes","text":"<p>The <code>CustomA2AStarletteApplication</code> extends the standard A2A Starlette Application with additional custom routes for task management and system operations.</p>"},{"location":"api/http/#custom-routes-overview","title":"Custom Routes Overview","text":"<p>The custom application adds the following routes:</p> <ol> <li><code>POST /tasks</code> - Task management endpoint (see Task Management Endpoints)</li> <li><code>POST /system</code> - System operations endpoint (see System Endpoints)</li> </ol> <p>These routes are enabled by default when using <code>CustomA2AStarletteApplication</code>. To disable them, set <code>enable_system_routes=False</code> when creating the application.</p>"},{"location":"api/http/#route-handler-architecture","title":"Route Handler Architecture","text":"<p>The route handlers are implemented in the <code>api/routes/</code> directory as protocol-agnostic modules that can be reused across different protocol implementations (A2A, REST, GraphQL, etc.):</p> <ul> <li><code>api/routes/base.py</code>: Provides <code>BaseRouteHandler</code> class with shared functionality for permission checking, user information extraction, and common utilities</li> <li><code>api/routes/tasks.py</code>: Contains <code>TaskRoutes</code> class with handlers for task CRUD operations, execution, and monitoring</li> <li><code>api/routes/system.py</code>: Contains <code>SystemRoutes</code> class with handlers for system operations like health checks, LLM key configuration, and examples management</li> </ul> <p>This architecture allows the same route handlers to be used by different protocol implementations, promoting code reuse and maintainability.</p>"},{"location":"api/http/#initialization","title":"Initialization","text":"<pre><code>from apflow.api.a2a.server import create_a2a_server\n\n# Create A2A server with custom routes enabled (default)\napp = create_a2a_server(\n    verify_token_secret_key=\"your-secret-key\",  # Optional: JWT authentication\n    base_url=\"http://localhost:8000\",\n    enable_system_routes=True,  # Enable custom routes (default: True)\n    auto_initialize_extensions=False,  # Optional: auto-initialize extensions (default: False)\n    task_routes_class=None,  # Optional: custom TaskRoutes class\n)\n</code></pre> <p>New Parameters: - <code>auto_initialize_extensions</code>: If <code>True</code>, automatically calls <code>initialize_extensions()</code> before creating the server. Matches behavior of <code>create_app_by_protocol()</code>. Default: <code>False</code> (backward compatible). - <code>task_routes_class</code>: Optional custom <code>TaskRoutes</code> class to use instead of default. Allows extending TaskRoutes functionality without monkey patching. Default: <code>None</code> (uses default <code>TaskRoutes</code>).</p>"},{"location":"api/http/#jwt-authentication_1","title":"JWT Authentication","text":"<p>The custom application supports optional JWT authentication via middleware:</p> <pre><code>from apflow.api.a2a.server import create_a2a_server, generate_token, verify_token\n\n# Generate JWT token\npayload = {\"user_id\": \"user123\", \"roles\": [\"admin\"]}\nsecret_key = \"your-secret-key\"\ntoken = generate_token(payload, secret_key, expires_in_days=30)\n\n# Verify JWT token (built-in function)\ndef verify_token_func(token: str) -&gt; Optional[dict]:\n    \"\"\"JWT token verification using built-in function\"\"\"\n    return verify_token(token, secret_key)\n\n# Or use custom verification\ndef custom_verify_token(token: str) -&gt; Optional[dict]:\n    \"\"\"Custom JWT token verification function\"\"\"\n    # Your token verification logic\n    return payload if valid else None\n\napp = create_a2a_server(\n    verify_token_func=verify_token_func,  # or custom_verify_token\n    verify_token_secret_key=secret_key,  # Optional: for built-in verification\n    enable_system_routes=True\n)\n</code></pre> <p>Token Sources: - JWT tokens can be provided via <code>Authorization: Bearer &lt;token&gt;</code> header (priority) - Or via <code>Authorization</code> cookie (fallback) - Both methods are supported for flexible authentication scenarios</p>"},{"location":"api/http/#a2a-protocol-features","title":"A2A Protocol Features","text":""},{"location":"api/http/#push-notification-configuration-callback-mode","title":"Push Notification Configuration (Callback Mode)","text":"<p>The A2A protocol supports push notifications via <code>configuration.push_notification_config</code>. This allows the server to send task execution updates to a callback URL instead of waiting for polling.</p>"},{"location":"api/http/#how-it-works","title":"How It Works","text":"<p>When <code>configuration.push_notification_config</code> is provided in the A2A request, the server will:</p> <ol> <li>Execute tasks in callback mode (asynchronous)</li> <li>Send task status updates to the configured callback URL</li> <li>Return immediately with an initial response</li> </ol>"},{"location":"api/http/#configuration-format","title":"Configuration Format","text":"<p>The <code>push_notification_config</code> should be included in the request's <code>configuration</code> field:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"execute_task_tree\",\n  \"params\": {\n    \"tasks\": [...]\n  },\n  \"configuration\": {\n    \"push_notification_config\": {\n      \"url\": \"https://your-server.com/callback\",\n      \"headers\": {\n        \"Authorization\": \"Bearer your-token\"\n      }\n    }\n  },\n  \"id\": \"request-123\"\n}\n</code></pre>"},{"location":"api/http/#push-notification-config-fields","title":"Push Notification Config Fields","text":"<ul> <li><code>url</code> (string, required): Callback URL where status updates will be sent</li> <li><code>headers</code> (object, optional): HTTP headers to include in callback requests</li> <li><code>method</code> (string, optional): HTTP method for callback (default: \"POST\")</li> </ul>"},{"location":"api/http/#callback-payload-format","title":"Callback Payload Format","text":"<p>The server will send POST requests to your callback URL with the following payload:</p> <pre><code>{\n  \"task_id\": \"task-abc-123\",\n  \"context_id\": \"context-xyz-456\",\n  \"status\": {\n    \"state\": \"completed\",\n    \"message\": {\n      \"role\": \"agent\",\n      \"parts\": [\n        {\n          \"kind\": \"data\",\n          \"data\": {\n            \"status\": \"completed\",\n            \"progress\": 1.0,\n            \"root_task_id\": \"task-abc-123\",\n            \"task_count\": 2\n          }\n        }\n      ]\n    }\n  },\n  \"final\": true\n}\n</code></pre>"},{"location":"api/http/#example-using-push-notifications","title":"Example: Using Push Notifications","text":"<p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"execute_task_tree\",\n  \"params\": {\n    \"tasks\": [\n      {\n        \"id\": \"task-1\",\n        \"name\": \"Task 1\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\n          \"method\": \"system_info_executor\"\n        },\n        \"inputs\": {}\n      }\n    ]\n  },\n  \"configuration\": {\n    \"push_notification_config\": {\n      \"url\": \"https://my-app.com/api/task-callback\",\n      \"headers\": {\n        \"Authorization\": \"Bearer my-api-token\",\n        \"Content-Type\": \"application/json\"\n      }\n    }\n  },\n  \"id\": \"request-123\"\n}\n</code></pre></p> <p>Immediate Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-123\",\n  \"result\": {\n    \"status\": \"in_progress\",\n    \"root_task_id\": \"task-1\",\n    \"task_count\": 1\n  }\n}\n</code></pre></p> <p>Callback Request (sent to your URL): <pre><code>{\n  \"task_id\": \"task-1\",\n  \"context_id\": \"context-123\",\n  \"status\": {\n    \"state\": \"completed\",\n    \"message\": {\n      \"role\": \"agent\",\n      \"parts\": [\n        {\n          \"kind\": \"data\",\n          \"data\": {\n            \"status\": \"completed\",\n            \"progress\": 1.0,\n            \"root_task_id\": \"task-1\"\n          }\n        }\n      ]\n    }\n  },\n  \"final\": true\n}\n</code></pre></p>"},{"location":"api/http/#streaming-mode","title":"Streaming Mode","text":"<p>The A2A protocol also supports streaming mode via <code>metadata.stream</code>. When enabled, the server will send multiple status update events through the EventQueue (SSE/WebSocket).</p> <p>Note: For JSON-RPC <code>tasks.execute</code> endpoint, use <code>use_streaming=true</code> parameter instead of <code>metadata.stream</code>. The endpoint will return a StreamingResponse with Server-Sent Events directly.</p> <p>Request with Streaming: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"execute_task_tree\",\n  \"params\": {\n    \"tasks\": [...]\n  },\n  \"metadata\": {\n    \"stream\": true\n  },\n  \"id\": \"request-123\"\n}\n</code></pre></p> <ul> <li>Preserve Execution History: When you want to re-execute a task without losing the original task's results and status</li> <li>A/B Testing: Compare different execution strategies on the same task definition</li> <li>Retry Failed Tasks: Create a fresh copy of a failed task for retry without modifying the original</li> <li>Task Templates: Use completed tasks as templates for new executions</li> </ul>"},{"location":"api/http/#a2a-client-sdk-usage","title":"A2A Client SDK Usage","text":"<p>The A2A protocol provides an official client SDK for easy integration. This section demonstrates how to use the A2A client SDK to interact with apflow.</p>"},{"location":"api/http/#installation","title":"Installation","text":"<pre><code>pip install a2a\n</code></pre>"},{"location":"api/http/#basic-usage","title":"Basic Usage","text":"<pre><code>from a2a.client import ClientFactory, ClientConfig\nfrom a2a.types import Message, DataPart, Role\nimport httpx\nimport uuid\n\n# Create HTTP client\nhttpx_client = httpx.AsyncClient(base_url=\"http://localhost:8000\")\n\n# Create A2A client config\nconfig = ClientConfig(\n    streaming=True,  # Enable streaming mode\n    polling=False,\n    httpx_client=httpx_client\n)\n\n# Create client factory\nfactory = ClientFactory(config=config)\n\n# Fetch agent card\nfrom a2a.utils.constants import AGENT_CARD_WELL_KNOWN_PATH\ncard_response = await httpx_client.get(AGENT_CARD_WELL_KNOWN_PATH)\nagent_card = AgentCard(**card_response.json())\n\n# Create A2A client\nclient = factory.create(card=agent_card)\n</code></pre>"},{"location":"api/http/#executing-tasks","title":"Executing Tasks","text":""},{"location":"api/http/#simple-mode-synchronous","title":"Simple Mode (Synchronous)","text":"<pre><code># Prepare task data\ntask_data = {\n    \"id\": \"task-1\",\n    \"name\": \"My Task\",\n    \"user_id\": \"user123\",\n    \"schemas\": {\n        \"method\": \"system_info_executor\"\n    },\n    \"inputs\": {}\n}\n\n# Create A2A message\ndata_part = DataPart(kind=\"data\", data={\"tasks\": [task_data]})\nmessage = Message(\n    message_id=str(uuid.uuid4()),\n    role=Role.user,\n    parts=[data_part]\n)\n\n# Send message and get response\nresponses = []\nasync for response in client.send_message(message):\n    responses.append(response)\n    if isinstance(response, Message):\n        # Extract result from response\n        for part in response.parts:\n            if part.kind == \"data\" and isinstance(part.data, dict):\n                result = part.data\n                print(f\"Task status: {result.get('status')}\")\n                print(f\"Progress: {result.get('progress')}\")\n</code></pre>"},{"location":"api/http/#streaming-mode_1","title":"Streaming Mode","text":"<pre><code># Create message with streaming enabled\nmessage = Message(\n    message_id=str(uuid.uuid4()),\n    role=Role.user,\n    parts=[data_part]\n)\n\n# Send message - will receive multiple updates\nasync for response in client.send_message(message):\n    if isinstance(response, Message):\n        # Process streaming updates\n        for part in response.parts:\n            if part.kind == \"data\":\n                update = part.data\n                print(f\"Update: {update}\")\n    elif isinstance(response, tuple):\n        # Response is (Task, Update) tuple\n        task, update = response\n        print(f\"Task {task.id}: {update}\")\n</code></pre>"},{"location":"api/http/#using-push-notifications-callback-mode","title":"Using Push Notifications (Callback Mode)","text":"<pre><code>from a2a.types import Configuration, PushNotificationConfig\n\n# Create push notification config\npush_config = PushNotificationConfig(\n    url=\"https://your-server.com/callback\",\n    headers={\n        \"Authorization\": \"Bearer your-token\"\n    }\n)\n\n# Create configuration\nconfiguration = Configuration(\n    push_notification_config=push_config\n)\n\n# Create message with configuration\nmessage = Message(\n    message_id=str(uuid.uuid4()),\n    role=Role.user,\n    parts=[data_part],\n    configuration=configuration\n)\n\n# Send message - server will use callback mode\n# Response will be immediate, updates sent to callback URL\nasync for response in client.send_message(message):\n    # Initial response only\n    print(f\"Initial response: {response}\")\n    break  # Only expect initial response in callback mode\n</code></pre>"},{"location":"api/http/#task-tree-with-dependencies","title":"Task Tree with Dependencies","text":"<pre><code># Create task tree with dependencies\ntasks = [\n    {\n        \"id\": \"parent-task\",\n        \"name\": \"Parent Task\",\n        \"user_id\": \"user123\",\n        \"dependencies\": [\n            {\"id\": \"child-1\", \"required\": True},\n            {\"id\": \"child-2\", \"required\": True}\n        ],\n        \"schemas\": {\n            \"method\": \"aggregate_results_executor\"\n        },\n        \"inputs\": {}\n    },\n    {\n        \"id\": \"child-1\",\n        \"name\": \"Child Task 1\",\n        \"parent_id\": \"parent-task\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\n            \"method\": \"system_info_executor\"\n        },\n        \"inputs\": {\"resource\": \"cpu\"}\n    },\n    {\n        \"id\": \"child-2\",\n        \"name\": \"Child Task 2\",\n        \"parent_id\": \"parent-task\",\n        \"user_id\": \"user123\",\n        \"dependencies\": [{\"id\": \"child-1\", \"required\": True}],\n        \"schemas\": {\n            \"method\": \"system_info_executor\"\n        },\n        \"inputs\": {\"resource\": \"memory\"}\n    }\n]\n\n# Create message with task tree\ndata_part = DataPart(kind=\"data\", data={\"tasks\": tasks})\nmessage = Message(\n    message_id=str(uuid.uuid4()),\n    role=Role.user,\n    parts=[data_part]\n)\n\n# Execute task tree\nasync for response in client.send_message(message):\n    # Process responses\n    pass\n</code></pre>"},{"location":"api/http/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    async for response in client.send_message(message):\n        # Process responses\n        pass\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    # Handle error appropriately\n</code></pre>"},{"location":"api/http/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom a2a.client import ClientFactory, ClientConfig\nfrom a2a.types import Message, DataPart, Role, AgentCard\nimport httpx\nimport uuid\n\nasync def main():\n    # Setup\n    httpx_client = httpx.AsyncClient(base_url=\"http://localhost:8000\")\n    config = ClientConfig(streaming=True, httpx_client=httpx_client)\n    factory = ClientFactory(config=config)\n\n    # Get agent card\n    card_response = await httpx_client.get(\"/.well-known/agent-card\")\n    agent_card = AgentCard(**card_response.json())\n    client = factory.create(card=agent_card)\n\n    # Create task\n    task_data = {\n        \"id\": \"my-task\",\n        \"name\": \"My Task\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\"method\": \"system_info_executor\"},\n        \"inputs\": {}\n    }\n\n    # Create message\n    message = Message(\n        message_id=str(uuid.uuid4()),\n        role=Role.user,\n        parts=[DataPart(kind=\"data\", data={\"tasks\": [task_data]})]\n    )\n\n    # Execute and process responses\n    async for response in client.send_message(message):\n        if isinstance(response, Message):\n            for part in response.parts:\n                if part.kind == \"data\":\n                    print(f\"Result: {part.data}\")\n\n    await httpx_client.aclose()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"api/http/#a2a-protocol-documentation","title":"A2A Protocol Documentation","text":"<p>For detailed information about the A2A Protocol, please refer to the official documentation:</p> <ul> <li>A2A Protocol Official Documentation: https://www.a2aprotocol.org/en/docs</li> <li>A2A Protocol Homepage: https://www.a2aprotocol.org</li> </ul> <p>These resources provide comprehensive information about: - A2A Protocol core concepts and architecture - Protocol specifications and data formats - Client SDK API reference - Best practices and examples - Security and authentication - Push notifications and streaming</p>"},{"location":"api/http/#see-also","title":"See Also","text":"<ul> <li>A2A Protocol Specification</li> <li>CLI Usage Guide</li> <li>Architecture Documentation</li> </ul>"},{"location":"api/python/","title":"Python API Reference","text":""},{"location":"api/python/#decorator-registration-override-parameter","title":"Decorator Registration: <code>override</code> Parameter","text":"<p>All extension and tool registration decorators accept an <code>override</code> parameter:</p> <ul> <li><code>override: bool = False</code>     If <code>True</code>, always force override any previous registration for this name or ID.     If <code>False</code> and the name/ID exists, registration is skipped.</li> </ul> <p>Example:</p> <pre><code>from apflow.core.extensions.decorators import executor_register\n\n@executor_register(override=True)\nclass MyExecutor(BaseTask):\n        ...\n</code></pre> <p>This will force override any previous registration for <code>MyExecutor</code>.</p> <p>Tool Registration Example:</p> <pre><code>from apflow.core.tools.decorators import tool_register\n\n@tool_register(name=\"custom_tool\", override=True)\nclass CustomTool(BaseTool):\n        ...\n</code></pre> <p>If a tool with the same name is already registered, setting <code>override=True</code> will force the new registration.</p> <p>Wherever you see <code>override</code> in decorator signatures, it means:</p> <p>If <code>override=True</code>, any existing registration for the same name or ID will be forcibly replaced.</p> <p>Looking for a quick syntax lookup? See the API Quick Reference for concise code patterns and usage. This document provides detailed explanations and examples.</p> <p>Complete reference for apflow's Python API. This document lists all available APIs and how to use them.</p> <p>For detailed implementation details, see: - Source code: <code>src/apflow/</code> (well-documented with docstrings) - Test cases: <code>tests/</code> (comprehensive examples of all features)</p>"},{"location":"api/python/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>TaskManager</li> <li>TaskBuilder</li> <li>ExecutableTask</li> <li>BaseTask</li> <li>TaskTreeNode</li> <li>TaskRepository</li> <li>TaskExecutor</li> <li>TaskCreator</li> <li>Extension Registry</li> <li>Hooks</li> <li>Common Patterns</li> </ol>"},{"location":"api/python/#overview","title":"Overview","text":"<p>The core API consists of:</p> <ul> <li>TaskManager: Task orchestration and execution engine</li> <li>TaskBuilder: Fluent API for creating and executing tasks</li> <li>ExecutableTask: Interface for all task executors</li> <li>BaseTask: Recommended base class for custom executors</li> <li>TaskTreeNode: Task tree structure representation</li> <li>TaskRepository: Database operations for tasks</li> <li>TaskExecutor: Singleton for task execution management</li> <li>TaskCreator: Task tree creation from arrays</li> <li>ExtensionRegistry: Extension discovery and management</li> </ul> <p>Understanding Lifecycles:</p> <p>For a complete understanding of task execution flow, database session management, and hook context lifecycle, see Task Tree Execution Lifecycle. This is essential reading for: - Implementing hooks that access the database - Understanding session scope and transaction boundaries - Debugging execution issues - Ensuring proper resource cleanup</p>"},{"location":"api/python/#quick-start-example","title":"Quick Start Example","text":"<pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    # Create database session\n    db = create_session()\n\n    # Create task manager\n    task_manager = TaskManager(db)\n\n    # Create a task\n    task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"user123\",\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Build and execute task tree\n    task_tree = TaskTreeNode(task)\n    await task_manager.distribute_task_tree(task_tree)\n\n    # Get result\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Result: {result.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"api/python/#taskmanager","title":"TaskManager","text":"<p>The main class for orchestrating and executing task trees.</p>"},{"location":"api/python/#initialization","title":"Initialization","text":"<pre><code>from apflow import TaskManager, create_session\n\ndb = create_session()\ntask_manager = TaskManager(\n    db,\n    root_task_id=None,              # Optional: Root task ID for streaming\n    pre_hooks=None,                 # Optional: List of pre-execution hooks\n    post_hooks=None,                # Optional: List of post-execution hooks\n    executor_instances=None         # Optional: Shared executor instances dict\n)\n</code></pre> <p>See: <code>src/apflow/core/execution/task_manager.py</code> for full implementation details.</p>"},{"location":"api/python/#main-methods","title":"Main Methods","text":""},{"location":"api/python/#distribute_task_treetask_tree-use_callbacktrue","title":"<code>distribute_task_tree(task_tree, use_callback=True)</code>","text":"<p>Execute a task tree with dependency management and priority scheduling.</p> <pre><code>result = await task_manager.distribute_task_tree(\n    task_tree: TaskTreeNode,\n    use_callback: bool = True\n) -&gt; TaskTreeNode\n</code></pre> <p>See: <code>tests/core/execution/test_task_manager.py</code> for comprehensive examples.</p>"},{"location":"api/python/#distribute_task_tree_with_streamingtask_tree-use_callbacktrue","title":"<code>distribute_task_tree_with_streaming(task_tree, use_callback=True)</code>","text":"<p>Execute a task tree with real-time streaming for progress updates.</p> <pre><code>await task_manager.distribute_task_tree_with_streaming(\n    task_tree: TaskTreeNode,\n    use_callback: bool = True\n) -&gt; None\n</code></pre>"},{"location":"api/python/#cancel_tasktask_id-error_messagenone","title":"<code>cancel_task(task_id, error_message=None)</code>","text":"<p>Cancel a running task execution.</p> <pre><code>result = await task_manager.cancel_task(\n    task_id: str,\n    error_message: str | None = None\n) -&gt; Dict[str, Any]\n</code></pre>"},{"location":"api/python/#properties","title":"Properties","text":"<ul> <li><code>task_repository</code> (TaskRepository): Access to task repository for database operations</li> <li><code>streaming_callbacks</code> (StreamingCallbacks): Streaming callbacks instance</li> </ul> <p>See: Source code in <code>src/apflow/core/execution/task_manager.py</code> for all available methods and detailed documentation.</p>"},{"location":"api/python/#taskbuilder","title":"TaskBuilder","text":"<p>Fluent API for creating and executing tasks with method chaining.</p>"},{"location":"api/python/#initialization_1","title":"Initialization","text":"<pre><code>from apflow.core.builders import TaskBuilder\n\nbuilder = TaskBuilder(\n    task_manager: TaskManager,\n    executor_id: str,\n    name: str | None = None,\n    user_id: str | None = None,\n    # ... other parameters\n)\n</code></pre>"},{"location":"api/python/#method-chaining","title":"Method Chaining","text":"<pre><code>result = await (\n    TaskBuilder(task_manager, \"rest_executor\")\n    .with_name(\"fetch_data\")\n    .with_user(\"user_123\")\n    .with_input(\"url\", \"https://api.example.com\")\n    .with_input(\"method\", \"GET\")\n    .depends_on(\"auth_task_id\")\n    .execute()\n)\n</code></pre>"},{"location":"api/python/#available-methods","title":"Available Methods","text":"<ul> <li><code>with_name(name: str)</code> - Set task name</li> <li><code>with_user(user_id: str)</code> - Set user ID</li> <li><code>with_parent(parent_id: str)</code> - Set parent task ID</li> <li><code>with_priority(priority: int)</code> - Set task priority (default: 2)</li> <li><code>with_inputs(inputs: Dict[str, Any])</code> - Set all input parameters</li> <li><code>with_input(key: str, value: Any)</code> - Set single input parameter</li> <li><code>with_params(params: Dict[str, Any])</code> - Set task parameters</li> <li><code>with_schemas(schemas: Dict[str, Any])</code> - Set task schemas</li> <li><code>with_dependencies(dependencies: Sequence[Dict[str, Any]])</code> - Set task dependencies</li> <li><code>depends_on(*task_ids: str)</code> - Add dependencies by task IDs</li> <li><code>copy_of(original_task_id: str)</code> - Create copy of existing task</li> <li><code>enable_streaming(context: Any | None = None)</code> - Enable streaming execution</li> <li><code>enable_demo_mode(sleep_scale: float | None = None)</code> - Enable demo mode</li> <li><code>execute()</code> - Execute the task and return result</li> </ul> <p>See: Source code in <code>src/apflow/core/builders.py</code> for complete implementation.</p>"},{"location":"api/python/#basetask","title":"BaseTask","text":"<p>Recommended base class for creating custom executors. Provides automatic registration via decorator.</p>"},{"location":"api/python/#usage","title":"Usage","text":"<pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any\nfrom pydantic import BaseModel, Field\n\nclass MyInputSchema(BaseModel):\n    \"\"\"Input schema for my executor\"\"\"\n    param: str = Field(description=\"Parameter\")\n\n@executor_register()\nclass MyExecutor(BaseTask):\n    id = \"my_executor\"\n    name = \"My Executor\"\n    description = \"Does something useful\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = MyInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        return {\"status\": \"completed\", \"result\": \"...\"}\n</code></pre> <p>See: <code>tests/extensions/tools/test_tools_decorator.py</code> and <code>docs/guides/custom-tasks.md</code> for examples.</p>"},{"location":"api/python/#executabletask","title":"ExecutableTask","text":"<p>Abstract base class for all task executors. Use <code>BaseTask</code> for simplicity, or <code>ExecutableTask</code> for more control.</p>"},{"location":"api/python/#required-interface","title":"Required Interface","text":"<ul> <li><code>id</code> (property): Unique identifier</li> <li><code>name</code> (property): Display name</li> <li><code>description</code> (property): Description</li> <li><code>execute(inputs)</code>: Main execution logic (async)</li> <li><code>get_input_schema()</code>: Return JSON Schema for inputs (automatically provided by BaseTask when <code>inputs_schema</code> is set)</li> </ul>"},{"location":"api/python/#optional-methods","title":"Optional Methods","text":"<ul> <li><code>cancel()</code>: Cancel task execution (optional)</li> </ul> <p>See: <code>src/apflow/core/interfaces/executable_task.py</code> for full interface definition.</p>"},{"location":"api/python/#tasktreenode","title":"TaskTreeNode","text":"<p>Represents a node in a task tree structure.</p>"},{"location":"api/python/#main-methods_1","title":"Main Methods","text":"<ul> <li><code>add_child(child)</code>: Add a child node</li> <li><code>calculate_progress()</code>: Calculate progress (0.0 to 1.0)</li> <li><code>calculate_status()</code>: Calculate overall status</li> </ul>"},{"location":"api/python/#properties_1","title":"Properties","text":"<ul> <li><code>task</code> (TaskModel): The task model instance</li> <li><code>children</code> (List[TaskTreeNode]): List of child nodes</li> </ul> <p>See: <code>src/apflow/core/types.py</code> for full implementation and <code>tests/core/execution/test_task_manager.py</code> for usage examples.</p>"},{"location":"api/python/#taskrepository","title":"TaskRepository","text":"<p>Database operations for tasks.</p>"},{"location":"api/python/#main-methods_2","title":"Main Methods","text":"<ul> <li><code>create_task(...)</code>: Create a new task</li> <li><code>get_task_by_id(task_id)</code>: Get task by ID</li> <li><code>get_root_task(task)</code>: Get root task</li> <li><code>build_task_tree(task)</code>: Build task tree from task</li> <li><code>update_task(task_id, **kwarg)</code> Update task fields</li> <li><code>delete_task(task_id)</code>: Physically delete a task from the database</li> <li><code>get_all_children_recursive(task_id)</code>: Recursively get all child tasks (including grandchildren)</li> <li><code>list_tasks(...)</code>: List tasks with filters</li> </ul> <p>See: <code>src/apflow/core/storage/sqlalchemy/task_repository.py</code> for all methods and <code>tests/core/storage/sqlalchemy/test_task_repository.py</code> for examples.</p> <p>Note on Task Updates: - Critical fields (<code>parent_id</code>, <code>user_id</code>, <code>dependencies</code>) are validated strictly:   - <code>parent_id</code> and <code>user_id</code>: Cannot be updated (always rejected)   - <code>dependencies</code>: Can only be updated for <code>pending</code> tasks, with validation for references, circular dependencies, and executing dependents - Other fields can be updated freely from any task status - For API-level updates with validation, use the <code>tasks.update</code> JSON-RPC endpoint via <code>TaskRoutes.handle_task_update()</code></p> <p>Note on Task Deletion: - <code>delete_task()</code> performs physical deletion (not soft-delete) - For API-level deletion with validation, use the <code>tasks.delete</code> JSON-RPC endpoint via <code>TaskRoutes.handle_task_delete()</code> - The API endpoint validates that all tasks (task + children) are pending and checks for dependencies before deletion</p>"},{"location":"api/python/#taskcreator","title":"TaskCreator","text":"<p>Create and manage task trees, links, copies, and archives.</p>"},{"location":"api/python/#main-methods_3","title":"Main Methods","text":"<ul> <li><code>create_task_tree_from_array(tasks)</code>: Create a task tree from an array of task dictionaries.</li> <li><code>from_link(original_task, ...)</code>: Create a new task as a reference (link) to an existing task. Returns a linked task or tree. Useful for deduplication and sharing results.</li> <li><code>from_copy(original_task, ...)</code>: Create a deep copy of an existing task or task tree. Supports copying children, dependencies, and selective subtree copying. Returns a new task or tree with new IDs.</li> <li><code>from_archive(original_task, ...)</code>: Create a read-only archive of an existing task or tree. Snapshots are immutable and preserve the state at the time of creation.</li> <li><code>from_mixed(original_task, ...)</code>: Create a new tree mixing links and copies, e.g., copy some tasks and link others for advanced workflows.</li> </ul>"},{"location":"api/python/#example-usage","title":"Example Usage","text":"<pre><code>from apflow.core.execution import TaskCreator\n\n# Assume db is a SQLAlchemy session\ncreator = TaskCreator(db)\n\n# 1. Create a task tree from an array\ntasks = [\n        {\"id\": \"task_1\", \"name\": \"Task 1\", \"user_id\": \"user_123\"},\n        {\"id\": \"task_2\", \"name\": \"Task 2\", \"user_id\": \"user_123\", \"parent_id\": \"task_1\"}\n]\ntree = await creator.create_task_tree_from_array(tasks)\n\n# 2. Create a linked task (reference)\nlinked = await creator.from_link(original_task, user_id=\"user_123\", parent_id=None)\n\n# 3. Create a deep copy (optionally with children)\ncopied = await creator.from_copy(original_task, user_id=\"user_123\", _recursive=True)\n\n# 4. Create a archive (frozen, read-only)\narchive = await creator.from_archive(original_task, user_id=\"user_123\", _recursive=True)\n\n# 5. Mixed: copy some, link others\nmixed = await creator.from_mixed(original_task, user_id=\"user_123\", link_task_ids=[...])\n</code></pre> <p>See: <code>src/apflow/core/execution/task_creator.py</code> for implementation and <code>tests/core/execution/test_task_creator_origin_types.py</code> for examples.</p>"},{"location":"api/python/#taskmodel","title":"TaskModel","text":"<p>Database model for tasks.</p>"},{"location":"api/python/#main-fields","title":"Main Fields","text":"<ul> <li><code>id</code>, <code>parent_id</code>, <code>task_tree_id</code>, <code>user_id</code>, <code>name</code>, <code>status</code>, <code>priority</code></li> <li><code>dependencies</code>, <code>inputs</code>, <code>params</code>, <code>result</code>, <code>error</code>, <code>schemas</code></li> <li><code>progress</code>, <code>has_children</code></li> <li><code>created_at</code>, <code>started_at</code>, <code>updated_at</code>, <code>completed_at</code></li> <li><code>original_task_id</code>, <code>task_tree_id</code>, <code>origin_type</code>, <code>has_references</code></li> </ul>"},{"location":"api/python/#methods","title":"Methods","text":"<ul> <li><code>to_dict()</code>: Convert model to dictionary</li> </ul> <p>See: <code>src/apflow/core/storage/sqlalchemy/models.py</code> for full model definition.</p>"},{"location":"api/python/#taskstatus","title":"TaskStatus","text":"<p>Task status constants and utilities.</p>"},{"location":"api/python/#constants","title":"Constants","text":"<ul> <li><code>PENDING</code>, <code>IN_PROGRESS</code>, <code>COMPLETED</code>, <code>FAILED</code>, <code>CANCELLED</code></li> </ul>"},{"location":"api/python/#methods_1","title":"Methods","text":"<ul> <li><code>is_terminal(status)</code>: Check if status is terminal</li> <li><code>is_active(status)</code>: Check if status is active</li> </ul> <p>See: <code>src/apflow/core/storage/sqlalchemy/models.py</code> for implementation.</p>"},{"location":"api/python/#utility-functions","title":"Utility Functions","text":""},{"location":"api/python/#session-management","title":"Session Management","text":"<ul> <li><code>create_pooled_session()</code>: Create a pooled database session context manager (recommended)</li> <li><code>create_session()</code>: Create a new raw database session</li> <li><code>get_default_session()</code>: Deprecated. Use <code>create_pooled_session()</code> instead.</li> </ul>"},{"location":"api/python/#extension-registry","title":"Extension Registry","text":"<ul> <li><code>executor_register()</code>: Decorator to register executors (recommended)</li> <li><code>register_pre_hook(hook)</code>: Register pre-execution hook</li> <li><code>register_post_hook(hook)</code>: Register post-execution hook</li> <li><code>register_task_tree_hook(event, hook)</code>: Register task tree lifecycle hook</li> <li><code>get_registry()</code>: Get extension registry instance</li> <li><code>get_available_executors()</code>: Get list of available executors based on APFLOW_EXTENSIONS configuration</li> </ul>"},{"location":"api/python/#getting-available-executors","title":"Getting Available Executors","text":"<p>The <code>get_available_executors()</code> function provides discovery of available executor types. This is useful for validating task schemas, generating UI options, or restricting executor access.</p> <p>Function Signature: <pre><code>def get_available_executors() -&gt; dict[str, Any]:\n    \"\"\"\n    Get list of available executors based on APFLOW_EXTENSIONS configuration.\n\n    Returns:\n        Dictionary with:\n            - executors: List of available executor metadata\n            - count: Number of available executors\n            - restricted: Boolean indicating if access is restricted\n            - allowed_ids: List of allowed executor IDs (if restricted)\n    \"\"\"\n</code></pre></p> <p>Basic Usage: <pre><code>from apflow.api.extensions import get_available_executors\n\n# Get all available executors\nresult = get_available_executors()\n\nprint(f\"Available executors: {result['count']}\")\nprint(f\"Restricted: {result['restricted']}\")\n\nfor executor in result['executors']:\n    print(f\"  - {executor['id']}: {executor['name']}\")\n    print(f\"    Extension: {executor['extension']}\")\n    print(f\"    Description: {executor['description']}\")\n</code></pre></p> <p>Response Structure: <pre><code>{\n    \"executors\": [\n        {\n            \"id\": \"system_info_executor\",\n            \"name\": \"System Info Executor\",\n            \"extension\": \"stdio\",\n            \"description\": \"Retrieve system information like CPU, memory, disk usage\"\n        },\n        {\n            \"id\": \"command_executor\",\n            \"name\": \"Command Executor\",\n            \"extension\": \"stdio\",\n            \"description\": \"Execute shell commands on the local system\"\n        },\n        # ... more executors\n    ],\n    \"count\": 2,\n    \"restricted\": False,\n}\n</code></pre></p> <p>With APFLOW_EXTENSIONS Restriction: When <code>APFLOW_EXTENSIONS=stdio,http</code> is set, only executors from those extensions are returned: <pre><code>result = get_available_executors()\n# result['restricted'] == True\n# result['allowed_ids'] == ['system_info_executor', 'command_executor', 'rest_executor']\n</code></pre></p> <p>Use Cases: - Validate task schemas against available executors before execution - Generate API/UI responses showing which executors users can access - Enforce security restrictions by limiting executor availability - Debug executor availability issues</p>"},{"location":"api/python/#hook-database-access","title":"Hook Database Access","text":"<ul> <li><code>get_hook_repository()</code>: Get TaskRepository instance in hook context (recommended)</li> <li><code>get_hook_session()</code>: Get database session in hook context</li> </ul> <p>Hooks can access the database using the same session as TaskManager:</p> <pre><code>from apflow import register_pre_hook, get_hook_repository\n\n@register_pre_hook\nasync def my_hook(task):\n    # Get repository from hook context\n    repo = get_hook_repository()\n    if repo:\n        # Modify task fields\n        await repo.update_task(task.id, priority=10)\n        # Query other tasks\n        pending = await repo.get_tasks_by_status(\"pending\")\n</code></pre> <p>See: <code>src/apflow/core/decorators.py</code> and <code>src/apflow/core/extensions/registry.py</code> for implementation.</p>"},{"location":"api/python/#type-definitions","title":"Type Definitions","text":"<ul> <li><code>TaskPreHook</code>: Type alias for pre-execution hook functions</li> <li><code>TaskPostHook</code>: Type alias for post-execution hook functions</li> </ul> <p>See: <code>src/apflow/core/extensions/types.py</code> for type definitions.</p>"},{"location":"api/python/#error-handling","title":"Error Handling","text":""},{"location":"api/python/#common-exceptions","title":"Common Exceptions","text":"<ul> <li><code>ValueError</code>: Invalid input parameters</li> <li><code>RuntimeError</code>: Execution errors</li> <li><code>KeyError</code>: Missing required fields</li> </ul>"},{"location":"api/python/#error-response-format","title":"Error Response Format","text":"<p>Tasks that fail return:</p> <pre><code>{\n    \"status\": \"failed\",\n    \"error\": \"Error message\",\n    \"error_type\": \"ExceptionType\"\n}\n</code></pre>"},{"location":"api/python/#common-patterns","title":"Common Patterns","text":""},{"location":"api/python/#pattern-1-simple-task-execution","title":"Pattern 1: Simple Task Execution","text":"<pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create task\n    task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"user123\",\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Build tree\n    tree = TaskTreeNode(task)\n\n    # Execute\n    await task_manager.distribute_task_tree(tree)\n\n    # Get result\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Status: {result.status}\")\n    print(f\"Result: {result.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"api/python/#pattern-2-sequential-tasks-dependencies","title":"Pattern 2: Sequential Tasks (Dependencies)","text":"<pre><code># Create tasks with dependencies\ntask1 = await task_manager.task_repository.create_task(\n    name=\"fetch_data\",\n    user_id=\"user123\",\n    priority=1\n)\n\ntask2 = await task_manager.task_repository.create_task(\n    name=\"process_data\",\n    user_id=\"user123\",\n    parent_id=task1.id,\n    dependencies=[{\"id\": task1.id, \"required\": True}],  # Waits for task1\n    priority=2,\n    inputs={\"data\": []}  # Will be populated from task1 result\n)\n\ntask3 = await task_manager.task_repository.create_task(\n    name=\"save_results\",\n    user_id=\"user123\",\n    parent_id=task1.id,\n    dependencies=[{\"id\": task2.id, \"required\": True}],  # Waits for task2\n    priority=3\n)\n\n# Build tree\nroot = TaskTreeNode(task1)\nroot.add_child(TaskTreeNode(task2))\nroot.add_child(TaskTreeNode(task3))\n\n# Execute (order: task1 \u2192 task2 \u2192 task3)\nawait task_manager.distribute_task_tree(root)\n</code></pre>"},{"location":"api/python/#pattern-3-parallel-tasks","title":"Pattern 3: Parallel Tasks","text":"<pre><code># Create root\nroot_task = await task_manager.task_repository.create_task(\n    name=\"root\",\n    user_id=\"user123\",\n    priority=1\n)\n\n# Create parallel tasks (no dependencies between them)\ntask1 = await task_manager.task_repository.create_task(\n    name=\"task1\",\n    user_id=\"user123\",\n    parent_id=root_task.id,\n    priority=2\n)\n\ntask2 = await task_manager.task_repository.create_task(\n    name=\"task2\",\n    user_id=\"user123\",\n    parent_id=root_task.id,\n    priority=2  # Same priority, no dependencies = parallel\n)\n\ntask3 = await task_manager.task_repository.create_task(\n    name=\"task3\",\n    user_id=\"user123\",\n    parent_id=root_task.id,\n    priority=2\n)\n\n# Build tree\nroot = TaskTreeNode(root_task)\nroot.add_child(TaskTreeNode(task1))\nroot.add_child(TaskTreeNode(task2))\nroot.add_child(TaskTreeNode(task3))\n\n# Execute (all three run in parallel)\nawait task_manager.distribute_task_tree(root)\n</code></pre>"},{"location":"api/python/#pattern-4-error-handling","title":"Pattern 4: Error Handling","text":"<pre><code># Execute task tree\nawait task_manager.distribute_task_tree(task_tree)\n\n# Check all tasks for errors\ndef check_task_status(task_id):\n    task = await task_manager.task_repository.get_task_by_id(task_id)\n    if task.status == \"failed\":\n        print(f\"Task {task_id} failed: {task.error}\")\n        return False\n    elif task.status == \"completed\":\n        print(f\"Task {task_id} completed: {task.result}\")\n        return True\n    return None\n\n# Check root task\nroot_status = check_task_status(root_task.id)\n\n# Check all children\nfor child in task_tree.children:\n    check_task_status(child.task.id)\n</code></pre>"},{"location":"api/python/#pattern-5-using-taskexecutor","title":"Pattern 5: Using TaskExecutor","text":"<pre><code>from apflow.core.execution.task_executor import TaskExecutor\n\n# Get singleton instance\nexecutor = TaskExecutor()\n\n# Execute tasks from definitions\ntasks = [\n    {\n        \"id\": \"task1\",\n        \"name\": \"my_executor\",\n        \"user_id\": \"user123\",\n        \"inputs\": {\"key\": \"value\"}\n    },\n    {\n        \"id\": \"task2\",\n        \"name\": \"my_executor\",\n        \"user_id\": \"user123\",\n        \"parent_id\": \"task1\",\n        \"dependencies\": [{\"id\": \"task1\", \"required\": True}],\n        \"inputs\": {\"key\": \"value2\"}\n    }\n]\n\n# Execute\nresult = await executor.execute_tasks(\n    tasks=tasks,\n    root_task_id=\"root_123\",\n    use_streaming=False\n)\n\nprint(f\"Execution result: {result}\")\n</code></pre>"},{"location":"api/python/#pattern-6-custom-executor-with-error-handling","title":"Pattern 6: Custom Executor with Error Handling","text":"<pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any\nfrom pydantic import BaseModel, Field\n\nclass RobustInputSchema(BaseModel):\n    \"\"\"Input schema for robust executor\"\"\"\n    data: str = Field(description=\"Data to process\")\n\n@executor_register()\nclass RobustExecutor(BaseTask):\n    id = \"robust_executor\"\n    name = \"Robust Executor\"\n    description = \"Executor with comprehensive error handling\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = RobustInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        try:\n            # Validate inputs against Pydantic schema\n            self.check_input_schema(inputs)\n\n            # Process\n            result = self._process(inputs[\"data\"])\n\n            return {\n                \"status\": \"completed\",\n                \"result\": result\n            }\n        except ValueError as e:\n            return {\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"error_type\": \"ValueError\"\n            }\n        except Exception as e:\n            return {\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"error_type\": type(e).__name__\n            }\n\n    def _process(self, data: str) -&gt; Dict[str, Any]:\n        # Your processing logic\n        return {\"processed\": data}\n</code></pre>"},{"location":"api/python/#a2a-protocol-integration","title":"A2A Protocol Integration","text":"<p>apflow implements the A2A (Agent-to-Agent) Protocol standard, allowing seamless integration with other A2A-compatible agents and services.</p>"},{"location":"api/python/#using-a2a-client-sdk","title":"Using A2A Client SDK","text":"<p>The A2A protocol provides an official client SDK for easy integration. Here's how to use it with apflow:</p>"},{"location":"api/python/#installation","title":"Installation","text":"<pre><code>pip install a2a\n</code></pre>"},{"location":"api/python/#basic-example","title":"Basic Example","text":"<pre><code>from a2a.client import ClientFactory, ClientConfig\nfrom a2a.types import Message, DataPart, Role, AgentCard\nimport httpx\nimport uuid\nimport asyncio\n\nasync def execute_task_via_a2a():\n    # Create HTTP client\n    httpx_client = httpx.AsyncClient(base_url=\"http://localhost:8000\")\n\n    # Create A2A client config\n    config = ClientConfig(\n        streaming=True,\n        polling=False,\n        httpx_client=httpx_client\n    )\n\n    # Create client factory\n    factory = ClientFactory(config=config)\n\n    # Fetch agent card\n    from a2a.utils.constants import AGENT_CARD_WELL_KNOWN_PATH\n    card_response = await httpx_client.get(AGENT_CARD_WELL_KNOWN_PATH)\n    agent_card = AgentCard(**card_response.json())\n\n    # Create A2A client\n    client = factory.create(card=agent_card)\n\n    # Prepare task data\n    task_data = {\n        \"id\": \"task-1\",\n        \"name\": \"My Task\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\n            \"method\": \"system_info_executor\"\n        },\n        \"inputs\": {}\n    }\n\n    # Create A2A message\n    data_part = DataPart(kind=\"data\", data={\"tasks\": [task_data]})\n    message = Message(\n        message_id=str(uuid.uuid4()),\n        role=Role.user,\n        parts=[data_part]\n    )\n\n    # Send message and process responses\n    async for response in client.send_message(message):\n        if isinstance(response, Message):\n            for part in response.parts:\n                if part.kind == \"data\" and isinstance(part.data, dict):\n                    result = part.data\n                    print(f\"Status: {result.get('status')}\")\n                    print(f\"Progress: {result.get('progress')}\")\n\n    await httpx_client.aclose()\n\n# Run\nasyncio.run(execute_task_via_a2a())\n</code></pre>"},{"location":"api/python/#push-notification-configuration","title":"Push Notification Configuration","text":"<p>You can use push notifications to receive task execution updates via callback URL:</p> <pre><code>from a2a.types import Configuration, PushNotificationConfig\n\n# Create push notification config\npush_config = PushNotificationConfig(\n    url=\"https://your-server.com/callback\",\n    headers={\n        \"Authorization\": \"Bearer your-token\"\n    }\n)\n\n# Create configuration\nconfiguration = Configuration(\n    push_notification_config=push_config\n)\n\n# Create message with configuration\nmessage = Message(\n    message_id=str(uuid.uuid4()),\n    role=Role.user,\n    parts=[data_part],\n    configuration=configuration\n)\n\n# Send message - server will use callback mode\nasync for response in client.send_message(message):\n    # Initial response only\n    print(f\"Initial response: {response}\")\n    break\n</code></pre> <p>The server will send task status updates to your callback URL as the task executes.</p>"},{"location":"api/python/#cancelling-tasks","title":"Cancelling Tasks","text":"<p>You can cancel a running task using the A2A Protocol <code>cancel</code> method:</p> <pre><code>from a2a.client import ClientFactory, ClientConfig\nfrom a2a.types import Message, DataPart, Role, AgentCard, RequestContext\nimport httpx\nimport uuid\nimport asyncio\n\nasync def cancel_task_via_a2a():\n    # Create HTTP client\n    httpx_client = httpx.AsyncClient(base_url=\"http://localhost:8000\")\n\n    # Create A2A client config\n    config = ClientConfig(\n        streaming=True,\n        polling=False,\n        httpx_client=httpx_client\n    )\n\n    # Create client factory\n    factory = ClientFactory(config=config)\n\n    # Fetch agent card\n    from a2a.utils.constants import AGENT_CARD_WELL_KNOWN_PATH\n    card_response = await httpx_client.get(AGENT_CARD_WELL_KNOWN_PATH)\n    agent_card = AgentCard(**card_response.json())\n\n    # Create A2A client\n    client = factory.create(card=agent_card)\n\n    # Create cancel request\n    # Task ID can be provided in multiple ways (priority order):\n    # 1. task_id in RequestContext\n    # 2. context_id in RequestContext\n    # 3. metadata.task_id\n    # 4. metadata.context_id\n    cancel_message = Message(\n        message_id=str(uuid.uuid4()),\n        role=Role.user,\n        parts=[],  # Empty parts for cancel\n        task_id=\"my-running-task\",  # Task ID to cancel\n        metadata={\n            \"error_message\": \"User requested cancellation\"  # Optional custom message\n        }\n    )\n\n    # Send cancel request and process responses\n    async for response in client.send_message(cancel_message):\n        if isinstance(response, Message):\n            for part in response.parts:\n                if part.kind == \"data\" and isinstance(part.data, dict):\n                    result = part.data\n                    status = result.get(\"status\")\n                    if status == \"cancelled\":\n                        print(f\"Task cancelled successfully: {result.get('message')}\")\n                        if \"token_usage\" in result:\n                            print(f\"Token usage: {result['token_usage']}\")\n                        if \"result\" in result:\n                            print(f\"Partial result: {result['result']}\")\n                    elif status == \"failed\":\n                        print(f\"Cancellation failed: {result.get('error', result.get('message'))}\")\n\n    await httpx_client.aclose()\n\n# Run\nasyncio.run(cancel_task_via_a2a())\n</code></pre> <p>Notes: - The <code>cancel()</code> method sends a <code>TaskStatusUpdateEvent</code> through the EventQueue - Task ID extraction follows priority: <code>task_id</code> &gt; <code>context_id</code> &gt; <code>metadata.task_id</code> &gt; <code>metadata.context_id</code> - If the task supports cancellation, the executor's <code>cancel()</code> method will be called - Token usage and partial results are preserved if available - The response includes <code>protocol: \"a2a\"</code> in the event data</p>"},{"location":"api/python/#streaming-mode","title":"Streaming Mode","text":"<p>Enable streaming mode to receive real-time progress updates:</p> <pre><code># Create message with streaming enabled via metadata\nmessage = Message(\n    message_id=str(uuid.uuid4()),\n    role=Role.user,\n    parts=[data_part],\n    metadata={\"stream\": True}\n)\n\n# Send message - will receive multiple updates\nasync for response in client.send_message(message):\n    if isinstance(response, Message):\n        # Process streaming updates\n        for part in response.parts:\n            if part.kind == \"data\":\n                update = part.data\n                print(f\"Update: {update}\")\n</code></pre>"},{"location":"api/python/#task-tree-execution","title":"Task Tree Execution","text":"<p>Execute complex task trees with dependencies:</p> <pre><code>tasks = [\n    {\n        \"id\": \"parent-task\",\n        \"name\": \"Parent Task\",\n        \"user_id\": \"user123\",\n        \"dependencies\": [\n            {\"id\": \"child-1\", \"required\": True},\n            {\"id\": \"child-2\", \"required\": True}\n        ],\n        \"schemas\": {\n            \"method\": \"aggregate_results_executor\"\n        },\n        \"inputs\": {}\n    },\n    {\n        \"id\": \"child-1\",\n        \"name\": \"Child Task 1\",\n        \"parent_id\": \"parent-task\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\n            \"method\": \"system_info_executor\"\n        },\n        \"inputs\": {\"resource\": \"cpu\"}\n    },\n    {\n        \"id\": \"child-2\",\n        \"name\": \"Child Task 2\",\n        \"parent_id\": \"parent-task\",\n        \"user_id\": \"user123\",\n        \"dependencies\": [{\"id\": \"child-1\", \"required\": True}],\n        \"schemas\": {\n            \"method\": \"system_info_executor\"\n        },\n        \"inputs\": {\"resource\": \"memory\"}\n    }\n]\n\n# Create message with task tree\ndata_part = DataPart(kind=\"data\", data={\"tasks\": tasks})\nmessage = Message(\n    message_id=str(uuid.uuid4()),\n    role=Role.user,\n    parts=[data_part]\n)\n\n# Execute task tree\nasync for response in client.send_message(message):\n    # Process responses\n    pass\n</code></pre>"},{"location":"api/python/#a2a-protocol-documentation","title":"A2A Protocol Documentation","text":"<p>For detailed information about the A2A Protocol, please refer to the official documentation:</p> <ul> <li>A2A Protocol Official Documentation: https://www.a2aprotocol.org/en/docs</li> <li>A2A Protocol Homepage: https://www.a2aprotocol.org</li> </ul>"},{"location":"api/python/#see-also","title":"See Also","text":"<ul> <li>Task Orchestration Guide</li> <li>Custom Tasks Guide</li> <li>Architecture Documentation</li> <li>HTTP API Reference</li> </ul>"},{"location":"api/quick-reference/","title":"API Quick Reference","text":"<p>Need more detailed explanations or advanced usage? See the Python API Reference for in-depth documentation and examples. This page is a concise cheat sheet for common patterns.</p> <p>Quick reference cheat sheet for apflow APIs. Perfect for when you know what you need but need the exact syntax.</p>"},{"location":"api/quick-reference/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core APIs</li> <li>Task Management</li> <li>Executor Discovery</li> <li>Custom Executors</li> <li>Task Orchestration</li> <li>Hooks</li> <li>Storage</li> <li>Common Patterns</li> </ol>"},{"location":"api/quick-reference/#decorator-registration-override-parameter","title":"Decorator Registration: <code>override</code> Parameter","text":"<p>All extension and tool registration decorators accept an <code>override</code> parameter:</p> <ul> <li><code>override: bool = False</code>     If <code>True</code>, always force override any previous registration for this name or ID.     If <code>False</code> and the name/ID exists, registration is skipped.</li> </ul> <p>Example:</p> <pre><code>from apflow.core.extensions.decorators import executor_register\n\n@executor_register(override=True)\nclass MyExecutor(BaseTask):\n        ...\n</code></pre> <p>This will force override any previous registration for <code>MyExecutor</code>.</p> <p>Tool Registration Example:</p> <pre><code>from apflow.core.tools.decorators import tool_register\n\n@tool_register(name=\"custom_tool\", override=True)\nclass CustomTool(BaseTool):\n        ...\n</code></pre> <p>If a tool with the same name is already registered, setting <code>override=True</code> will force the new registration.</p> <p>Wherever you see <code>override</code> in decorator signatures, it means:</p> <p>If <code>override=True</code>, any existing registration for the same name or ID will be forcibly replaced.</p>"},{"location":"api/quick-reference/#core-apis","title":"Core APIs","text":""},{"location":"api/quick-reference/#import-core-components","title":"Import Core Components","text":"<pre><code>from apflow import (\n    TaskManager,\n    TaskTreeNode,\n    create_session,\n    BaseTask,\n    executor_register\n)\n</code></pre>"},{"location":"api/quick-reference/#create-database-session","title":"Create Database Session","text":"<pre><code># DuckDB (default, no setup needed)\ndb = create_session()\n\n# PostgreSQL\nimport os\nos.environ[\"DATABASE_URL\"] = \"postgresql+asyncpg://user:password@localhost/dbname\"\ndb = create_session()\n</code></pre>"},{"location":"api/quick-reference/#create-taskmanager","title":"Create TaskManager","text":"<pre><code>db = create_session()\ntask_manager = TaskManager(db)\n</code></pre>"},{"location":"api/quick-reference/#task-management","title":"Task Management","text":""},{"location":"api/quick-reference/#create-task","title":"Create Task","text":"<pre><code>task = await task_manager.task_repository.create_task(\n    name=\"executor_id\",           # Required: Executor ID\n    user_id=\"user123\",            # Required: User identifier\n    parent_id=None,               # Optional: Parent task ID\n    priority=2,                   # Optional: Priority (0-3, default: 2)\n    dependencies=[],              # Optional: List of dependencies\n    inputs={},                    # Optional: Input parameters\n    schemas={},                   # Optional: Task schemas\n    status=\"pending\"              # Optional: Initial status\n)\n</code></pre>"},{"location":"api/quick-reference/#get-task-by-id","title":"Get Task by ID","text":"<pre><code>task = await task_manager.task_repository.get_task_by_id(task_id)\n</code></pre>"},{"location":"api/quick-reference/#update-task","title":"Update Task","text":"<pre><code># Update status and related fields\nawait task_repository.update_task(\n    task_id,\n    status=\"completed\",\n    result={\"data\": \"result\"},\n    progress=1.0\n)\n</code></pre> <p>Critical Field Validation: - <code>parent_id</code> and <code>user_id</code>: Cannot be updated (always rejected) - <code>dependencies</code>: Can only be updated for <code>pending</code> tasks, with validation:   - All dependency references must exist in the same task tree   - No circular dependencies allowed   - No dependent tasks can be executing - Other fields: Can be updated freely from any status</p>"},{"location":"api/quick-reference/#delete-task","title":"Delete Task","text":"<pre><code>await task_manager.task_repository.delete_task(task_id)\n</code></pre>"},{"location":"api/quick-reference/#list-tasks","title":"List Tasks","text":"<pre><code>tasks = await task_manager.task_repository.list_tasks(\n    user_id=\"user123\",\n    status=\"completed\",\n    limit=100\n)\n</code></pre>"},{"location":"api/quick-reference/#executor-discovery","title":"Executor Discovery","text":""},{"location":"api/quick-reference/#get-available-executors","title":"Get Available Executors","text":"<pre><code>from apflow.api.extensions import get_available_executors\n\n# Get all available executors\nresult = get_available_executors()\n\n# Access executor data\nexecutors = result[\"executors\"]  # List of executor metadata\ncount = result[\"count\"]           # Number of executors\nrestricted = result[\"restricted\"] # Whether access is restricted\n\nfor executor in executors:\n    print(f\"{executor['id']}: {executor['name']} ({executor['extension']})\")\n</code></pre>"},{"location":"api/quick-reference/#http-api","title":"HTTP API","text":"<pre><code># Get available executors via HTTP API\ncurl -X POST http://localhost:8000/system \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"method\": \"system.executors\", \"params\": {}, \"id\": \"1\"}'\n</code></pre>"},{"location":"api/quick-reference/#cli","title":"CLI","text":"<pre><code># List all available executors (table format)\napflow executors list\n\n# JSON format (for scripts)\napflow executors list --format json\n\n# Just executor IDs\napflow executors list --format ids\n\n# With descriptions\napflow executors list --verbose\n</code></pre>"},{"location":"api/quick-reference/#custom-executors","title":"Custom Executors","text":""},{"location":"api/quick-reference/#basic-executor-template","title":"Basic Executor Template","text":"<pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any\nfrom pydantic import BaseModel, Field\n\nclass MyInputSchema(BaseModel):\n    \"\"\"Input schema for my executor\"\"\"\n    param: str = Field(description=\"Parameter\")\n\n@executor_register()\nclass MyExecutor(BaseTask):\n    id = \"my_executor\"\n    name = \"My Executor\"\n    description = \"Does something\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = MyInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        # Your logic here\n        return {\"status\": \"completed\", \"result\": \"...\"}\n</code></pre>"},{"location":"api/quick-reference/#use-custom-executor","title":"Use Custom Executor","text":"<pre><code># Import to register\nfrom my_module import MyExecutor\n\n# Use it\ntask = await task_manager.task_repository.create_task(\n    name=\"my_executor\",  # Must match executor id\n    user_id=\"user123\",\n    inputs={\"param\": \"value\"}\n)\n</code></pre>"},{"location":"api/quick-reference/#llm-executor-llm_executor","title":"LLM Executor (<code>llm_executor</code>)","text":"<pre><code># Create task using LLM executor\ntask = await task_manager.task_repository.create_task(\n    name=\"llm_executor\",\n    user_id=\"user123\",\n    inputs={\n        \"model\": \"gpt-4o\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Explain AI.\"}],\n        \"temperature\": 0.7,\n        \"max_tokens\": 1000\n    }\n)\n</code></pre>"},{"location":"api/quick-reference/#task-orchestration","title":"Task Orchestration","text":""},{"location":"api/quick-reference/#build-task-tree","title":"Build Task Tree","text":"<pre><code># Single task\ntask_tree = TaskTreeNode(task)\n\n# Multiple tasks\nroot = TaskTreeNode(root_task)\nroot.add_child(TaskTreeNode(child_task1))\nroot.add_child(TaskTreeNode(child_task2))\n</code></pre>"},{"location":"api/quick-reference/#execute-task-tree","title":"Execute Task Tree","text":"<pre><code># Without streaming\nresult = await task_manager.distribute_task_tree(task_tree)\n\n# With streaming\nawait task_manager.distribute_task_tree_with_streaming(\n    task_tree,\n    use_callback=True\n)\n</code></pre>"},{"location":"api/quick-reference/#dependencies","title":"Dependencies","text":"<pre><code># Required dependency\ntask2 = await task_manager.task_repository.create_task(\n    name=\"task2\",\n    dependencies=[{\"id\": task1.id, \"required\": True}],\n    ...\n)\n\n# Optional dependency\ntask2 = await task_manager.task_repository.create_task(\n    name=\"task2\",\n    dependencies=[{\"id\": task1.id, \"required\": False}],\n    ...\n)\n\n# Multiple dependencies\ntask3 = await task_manager.task_repository.create_task(\n    name=\"task3\",\n    dependencies=[\n        {\"id\": task1.id, \"required\": True},\n        {\"id\": task2.id, \"required\": True}\n    ],\n    ...\n)\n</code></pre>"},{"location":"api/quick-reference/#priorities","title":"Priorities","text":"<pre><code># Priority levels\nURGENT = 0   # Highest priority\nHIGH = 1\nNORMAL = 2   # Default\nLOW = 3      # Lowest priority\n\ntask = await task_manager.task_repository.create_task(\n    name=\"task\",\n    priority=URGENT,\n    ...\n)\n</code></pre>"},{"location":"api/quick-reference/#cancel-task","title":"Cancel Task","text":"<pre><code>result = await task_manager.cancel_task(\n    task_id=\"task_123\",\n    error_message=\"User requested cancellation\"\n)\n</code></pre>"},{"location":"api/quick-reference/#hooks","title":"Hooks","text":""},{"location":"api/quick-reference/#pre-execution-hook","title":"Pre-Execution Hook","text":"<pre><code>from apflow import register_pre_hook\n\n@register_pre_hook\nasync def validate_inputs(task):\n    \"\"\"Validate inputs before execution\"\"\"\n    if task.inputs and \"url\" in task.inputs:\n        url = task.inputs[\"url\"]\n        if not url.startswith((\"http://\", \"https://\")):\n            task.inputs[\"url\"] = f\"https://{url}\"\n</code></pre> <p>Note: Changes to <code>task.inputs</code> are automatically persisted!</p>"},{"location":"api/quick-reference/#post-execution-hook","title":"Post-Execution Hook","text":"<pre><code>from apflow import register_post_hook\n\n@register_post_hook\nasync def log_results(task, inputs, result):\n    \"\"\"Log results after execution\"\"\"\n    print(f\"Task {task.id} completed: {result}\")\n</code></pre>"},{"location":"api/quick-reference/#hook-database-access","title":"Hook Database Access","text":"<pre><code>from apflow import register_pre_hook, get_hook_repository\n\n@register_pre_hook\nasync def modify_task_with_db(task):\n    \"\"\"Access database in hook\"\"\"\n    # Get repository from hook context\n    repo = get_hook_repository()\n    if repo:\n        # Update task fields\n        await repo.update_task(task.id, name=\"New Name\")\n\n        # Query other tasks\n        pending_tasks = await repo.get_tasks_by_status(\"pending\")\n        print(f\"Found {len(pending_tasks)} pending tasks\")\n</code></pre> <p>Key Features: - Hooks share the same database session as TaskManager - No need to create separate sessions - Changes are visible across all hooks in the execution - Thread-safe context isolation</p>"},{"location":"api/quick-reference/#dynamic-hooks-and-env-loading-configmanager","title":"Dynamic Hooks and Env Loading (ConfigManager)","text":"<p>Prefer decorators for static hooks. Use <code>ConfigManager</code> when you need to wire hooks at runtime or in tests and to load env files in one place:</p> <pre><code>from pathlib import Path\n\nfrom apflow.core.config_manager import get_config_manager\n\nconfig = get_config_manager()\n\n# Register a hook dynamically (e.g., from config or tests)\nconfig.register_pre_hook(lambda task: task.inputs.update({\"source\": \"runtime\"}))\n\n# Tweak demo timings globally\nconfig.set_demo_sleep_scale(0.5)\n\n# Load .env files once, then start CLI/API\nconfig.load_env_files([Path(\".env\"), Path(\".env.local\")], override=False)\n</code></pre>"},{"location":"api/quick-reference/#use-hooks-with-taskmanager","title":"Use Hooks with TaskManager","text":"<pre><code>from apflow import TaskPreHook, TaskPostHook\n\npre_hooks = [validate_inputs]\npost_hooks = [log_results]\n\ntask_manager = TaskManager(\n    db,\n    pre_hooks=pre_hooks,\n    post_hooks=post_hooks\n)\n</code></pre>"},{"location":"api/quick-reference/#storage","title":"Storage","text":""},{"location":"api/quick-reference/#custom-taskmodel","title":"Custom TaskModel","text":"<pre><code>from apflow.core.storage.sqlalchemy.models import TaskModel\nfrom apflow import set_task_model_class\nfrom sqlalchemy import Column, String\n\nclass CustomTaskModel(TaskModel):\n    __tablename__ = \"apflow_tasks\"\n    project_id = Column(String(255), nullable=True, index=True)\n\n# Set before creating tasks\nset_task_model_class(CustomTaskModel)\n</code></pre>"},{"location":"api/quick-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"api/quick-reference/#pattern-1-simple-task","title":"Pattern 1: Simple Task","text":"<pre><code># Create task\ntask = await task_manager.task_repository.create_task(\n    name=\"executor_id\",\n    user_id=\"user123\",\n    inputs={\"key\": \"value\"}\n)\n\n# Build tree\ntask_tree = TaskTreeNode(task)\n\n# Execute\nawait task_manager.distribute_task_tree(task_tree)\n\n# Get result\nresult = await task_manager.task_repository.get_task_by_id(task.id)\nprint(f\"Result: {result.result}\")\n</code></pre>"},{"location":"api/quick-reference/#pattern-2-sequential-tasks","title":"Pattern 2: Sequential Tasks","text":"<pre><code># Task 1\ntask1 = await task_manager.task_repository.create_task(\n    name=\"task1\",\n    user_id=\"user123\",\n    priority=1\n)\n\n# Task 2 depends on Task 1\ntask2 = await task_manager.task_repository.create_task(\n    name=\"task2\",\n    user_id=\"user123\",\n    parent_id=task1.id,\n    dependencies=[{\"id\": task1.id, \"required\": True}],\n    priority=2\n)\n\n# Build tree\nroot = TaskTreeNode(task1)\nroot.add_child(TaskTreeNode(task2))\n\n# Execute (Task 2 waits for Task 1)\nawait task_manager.distribute_task_tree(root)\n</code></pre>"},{"location":"api/quick-reference/#pattern-3-parallel-tasks","title":"Pattern 3: Parallel Tasks","text":"<pre><code># Root task\nroot_task = await task_manager.task_repository.create_task(\n    name=\"root\",\n    user_id=\"user123\",\n    priority=1\n)\n\n# Task 1 (no dependencies)\ntask1 = await task_manager.task_repository.create_task(\n    name=\"task1\",\n    user_id=\"user123\",\n    parent_id=root_task.id,\n    priority=2\n)\n\n# Task 2 (no dependencies, runs in parallel with Task 1)\ntask2 = await task_manager.task_repository.create_task(\n    name=\"task2\",\n    user_id=\"user123\",\n    parent_id=root_task.id,\n    priority=2\n)\n\n# Build tree\nroot = TaskTreeNode(root_task)\nroot.add_child(TaskTreeNode(task1))\nroot.add_child(TaskTreeNode(task2))\n\n# Execute (both run in parallel)\nawait task_manager.distribute_task_tree(root)\n</code></pre>"},{"location":"api/quick-reference/#pattern-4-fan-in-multiple-dependencies","title":"Pattern 4: Fan-In (Multiple Dependencies)","text":"<pre><code># Task 1\ntask1 = await task_manager.task_repository.create_task(...)\n\n# Task 2\ntask2 = await task_manager.task_repository.create_task(...)\n\n# Task 3 depends on both\ntask3 = await task_manager.task_repository.create_task(\n    name=\"task3\",\n    dependencies=[\n        {\"id\": task1.id, \"required\": True},\n        {\"id\": task2.id, \"required\": True}\n    ],\n    ...\n)\n</code></pre>"},{"location":"api/quick-reference/#pattern-5-error-handling","title":"Pattern 5: Error Handling","text":"<pre><code># Execute\nawait task_manager.distribute_task_tree(task_tree)\n\n# Check status\ntask = await task_manager.task_repository.get_task_by_id(task_id)\n\nif task.status == \"failed\":\n    print(f\"Error: {task.error}\")\n    # Handle error\nelif task.status == \"completed\":\n    print(f\"Result: {task.result}\")\n</code></pre>"},{"location":"api/quick-reference/#pattern-6-using-taskexecutor","title":"Pattern 6: Using TaskExecutor","text":"<pre><code>from apflow.core.execution.task_executor import TaskExecutor\n\n# Get singleton instance\nexecutor = TaskExecutor()\n\n# Execute tasks from definitions\ntasks = [\n    {\n        \"id\": \"task1\",\n        \"name\": \"executor_id\",\n        \"user_id\": \"user123\",\n        \"inputs\": {\"key\": \"value\"}\n    }\n]\n\nresult = await executor.execute_tasks(\n    tasks=tasks,\n    root_task_id=\"root_123\",\n    use_streaming=False\n)\n</code></pre>"},{"location":"api/quick-reference/#pattern-7-generate-task-tree-from-natural-language","title":"Pattern 7: Generate Task Tree from Natural Language","text":"<p>Python API Usage:</p> <pre><code>from apflow import TaskManager, TaskCreator, create_session\nfrom apflow.core.types import TaskTreeNode\nfrom apflow.extensions.generate import GenerateExecutor\n\ndb = create_session()\ntask_manager = TaskManager(db)\n\n# Step 1: Generate task tree using generate_executor\ngenerate_task = await task_manager.task_repository.create_task(\n    name=\"generate_executor\",\n    user_id=\"user123\",\n    inputs={\n        \"requirement\": \"Fetch data from API, process it, and save to database\",\n        \"user_id\": \"user123\",\n        \"llm_provider\": \"openai\",  # Optional\n        \"model\": \"gpt-4o\"  # Optional\n    }\n)\n\n# Step 2: Execute generate_executor\ngenerate_tree = TaskTreeNode(generate_task)\nawait task_manager.distribute_task_tree(generate_tree)\n\n# Step 3: Get generated tasks array\nresult = await task_manager.task_repository.get_task_by_id(generate_task.id)\ngenerated_tasks = result.result[\"tasks\"]\n\n# Step 4: Create and execute the generated task tree\ncreator = TaskCreator(db)\nfinal_task_tree = await creator.create_task_tree_from_array(generated_tasks)\nawait task_manager.distribute_task_tree(final_task_tree)\n</code></pre> <p>JSON-RPC API Usage:</p> <pre><code>import requests\n\n# Generate task tree via API\nresponse = requests.post(\n    \"http://localhost:8000/tasks\",\n    json={\n        \"jsonrpc\": \"2.0\",\n        \"method\": \"tasks.generate\",\n        \"params\": {\n            \"requirement\": \"Fetch data from API, process it, and save to database\",\n            \"user_id\": \"user123\",\n            \"save\": True  # Automatically save to database\n        },\n        \"id\": \"generate-1\"\n    }\n)\n\nresult = response.json()[\"result\"]\ngenerated_tasks = result[\"tasks\"]\nroot_task_id = result.get(\"root_task_id\")  # Present if save=true\n\n# If save=true, tasks are already saved and ready for execution\n# If save=false, use TaskCreator.create_task_tree_from_array(generated_tasks)\n</code></pre> <p>CLI Usage Examples:</p> <pre><code># Get task statistics by status from database\napflow tasks count\n\n# List tasks from database (defaults to root tasks)\napflow tasks list --user-id user123\n\n# Generate and preview task tree\napflow generate task-tree \"Fetch data from API and process it\"\n\n# Complex processing flow\napflow generate task-tree \"Call REST API to get user data, process response with Python script, validate processed data, and save to file\"\n\n# Fan-out fan-in pattern\napflow generate task-tree \"Fetch data from API, process it in two different ways in parallel (filter and aggregate), merge both results, and save to database\"\n\n# Complete business scenario\napflow generate task-tree \"Monitor system resources (CPU, memory, disk) in parallel, analyze metrics, generate report, and send notification if threshold exceeded\"\n\n# Save output to file\napflow generate task-tree \"Your requirement\" --output tasks.json\n\n# Save to database\napflow generate task-tree \"Your requirement\" --save --user-id user123\n</code></pre> <p>Tips for Better Generation: - Use specific keywords: \"parallel\", \"sequential\", \"merge\", \"aggregate\" help guide the generation - Describe data flow: Explain how data moves between steps - Mention executors: Specify operations like \"API\", \"database\", \"file\", \"command\" for better executor selection - Be detailed: More context leads to more accurate task trees</p>"},{"location":"api/quick-reference/#extension-registry","title":"Extension Registry","text":""},{"location":"api/quick-reference/#get-registry","title":"Get Registry","text":"<pre><code>from apflow.core.extensions import get_registry\n\nregistry = get_registry()\n</code></pre>"},{"location":"api/quick-reference/#list-executors","title":"List Executors","text":"<pre><code>from apflow.core.extensions import ExtensionCategory\n\nexecutors = registry.list_by_category(ExtensionCategory.EXECUTOR)\nfor executor in executors:\n    print(f\"ID: {executor.id}, Name: {executor.name}\")\n</code></pre>"},{"location":"api/quick-reference/#get-executor-by-id","title":"Get Executor by ID","text":"<pre><code>executor = registry.get_by_id(\"executor_id\")\n</code></pre>"},{"location":"api/quick-reference/#create-executor-instance","title":"Create Executor Instance","text":"<pre><code>executor_instance = registry.create_executor_instance(\n    extension_id=\"executor_id\",\n    inputs={\"key\": \"value\"}\n)\n</code></pre>"},{"location":"api/quick-reference/#crewai-integration","title":"CrewAI Integration","text":""},{"location":"api/quick-reference/#create-crew","title":"Create Crew","text":"<pre><code>from apflow.extensions.crewai import CrewaiExecutor\nfrom apflow.core.extensions import get_registry\n\ncrew = CrewaiExecutor(\n    id=\"my_crew\",\n    name=\"My Crew\",\n    description=\"Does AI analysis\",\n    agents=[\n        {\n            \"role\": \"Analyst\",\n            \"goal\": \"Analyze data\",\n            \"backstory\": \"You are an expert analyst\"\n        }\n    ],\n    tasks=[\n        {\n            \"description\": \"Analyze: {text}\",\n            \"agent\": \"Analyst\"\n        }\n    ]\n)\n\n# Register\nget_registry().register(crew)\n</code></pre>"},{"location":"api/quick-reference/#use-crew","title":"Use Crew","text":"<pre><code>task = await task_manager.task_repository.create_task(\n    name=\"my_crew\",\n    user_id=\"user123\",\n    inputs={\"text\": \"Analyze this data\"}\n)\n</code></pre>"},{"location":"api/quick-reference/#cli-commands","title":"CLI Commands","text":""},{"location":"api/quick-reference/#run-tasks","title":"Run Tasks","text":"<pre><code># Execute tasks from file\napflow run flow --tasks-file tasks.json\n\n# Execute single task\napflow run task --task-id task_123\n</code></pre>"},{"location":"api/quick-reference/#start-server","title":"Start Server","text":"<pre><code># Start API server\napflow serve --port 8000\n\n# Start with specific host\napflow serve --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"api/quick-reference/#task-management_1","title":"Task Management","text":"<pre><code># List tasks from database\napflow tasks list --user-id user123\n\n# Get task statistics\napflow tasks count\n\n# Get task status\napflow tasks status task_123\n\n# Watch task progress\napflow tasks watch --task-id task_123\n</code></pre>"},{"location":"api/quick-reference/#common-input-schema-patterns-pydantic","title":"Common Input Schema Patterns (Pydantic)","text":"<p>Define input schemas as Pydantic BaseModel classes and set them as <code>inputs_schema: ClassVar[type[BaseModel]]</code> on the executor.</p>"},{"location":"api/quick-reference/#string-with-validation","title":"String with Validation","text":"<pre><code>url: str = Field(description=\"URL\", min_length=1, max_length=2048, pattern=r\"^https?://\")\n</code></pre>"},{"location":"api/quick-reference/#number-with-range","title":"Number with Range","text":"<pre><code>timeout: int = Field(default=30, description=\"Timeout in seconds\", ge=1, le=300)\n</code></pre>"},{"location":"api/quick-reference/#enum","title":"Enum","text":"<pre><code>from typing import Literal\n\noption: Literal[\"option1\", \"option2\", \"option3\"] = Field(\n    default=\"option1\", description=\"Select option\"\n)\n</code></pre>"},{"location":"api/quick-reference/#array","title":"Array","text":"<pre><code>items: list[str] = Field(description=\"List of items\", min_length=1)\n</code></pre>"},{"location":"api/quick-reference/#object","title":"Object","text":"<pre><code>config: Dict[str, str] = Field(description=\"Configuration object\")\n</code></pre>"},{"location":"api/quick-reference/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"api/quick-reference/#return-error-in-result","title":"Return Error in Result","text":"<pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    try:\n        result = await self._process(inputs)\n        return {\"status\": \"completed\", \"result\": result}\n    except ValueError as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": \"validation_error\"\n        }\n    except Exception as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": \"execution_error\"\n        }\n</code></pre>"},{"location":"api/quick-reference/#raise-exception","title":"Raise Exception","text":"<pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    if not inputs.get(\"required_param\"):\n        raise ValueError(\"required_param is required\")\n\n    # Continue execution\n    return {\"status\": \"completed\", \"result\": \"...\"}\n</code></pre>"},{"location":"api/quick-reference/#quick-tips","title":"Quick Tips","text":"<ul> <li>Task name must match executor ID: <code>name=\"executor_id\"</code></li> <li>Dependencies control execution order: Use <code>dependencies</code>, not <code>parent_id</code></li> <li>Lower priority numbers = higher priority: 0 is highest, 3 is lowest</li> <li>Import executors to register them: <code>from my_module import MyExecutor</code></li> <li>Always use async for I/O: Use <code>aiohttp</code>, <code>aiofiles</code>, etc.</li> <li>Validate inputs early: Check at the start of <code>execute()</code></li> <li>Return consistent results: Always return <code>{\"status\": \"...\", ...}</code></li> </ul>"},{"location":"api/quick-reference/#see-also","title":"See Also","text":"<ul> <li>Python API Reference - Complete API documentation</li> <li>Task Orchestration Guide - Orchestration patterns</li> <li>Custom Tasks Guide - Create executors</li> <li>Examples - Practical examples</li> </ul> <p>Need more details? Check the Full API Documentation</p>"},{"location":"architecture/configuration/","title":"Table Name Configuration","text":""},{"location":"architecture/configuration/#overview","title":"Overview","text":"<p>The <code>TaskModel</code> table name is configurable via environment variable to avoid conflicts with other frameworks or to align with your naming conventions.</p>"},{"location":"architecture/configuration/#default-table-name","title":"Default Table Name","text":"<p>Default: <code>apflow_tasks</code></p> <p>The table name uses the prefix <code>apflow_</code> (short for \"apflow\") to distinguish it from: - A2A Protocol's default <code>tasks</code> table (for execution instances) - Other frameworks that might use <code>tasks</code> table</p>"},{"location":"architecture/configuration/#configuration","title":"Configuration","text":""},{"location":"architecture/configuration/#environment-variable","title":"Environment Variable","text":"<p>Set the <code>APFLOW_TASK_TABLE_NAME</code> environment variable to customize the table name:</p> <pre><code>export APFLOW_TASK_TABLE_NAME=\"my_custom_tasks\"\n</code></pre>"},{"location":"architecture/configuration/#example-usage","title":"Example Usage","text":"<pre><code>import os\nos.environ[\"APFLOW_TASK_TABLE_NAME\"] = \"my_custom_tasks\"\n\n# Import after setting environment variable\nfrom apflow.core.storage.sqlalchemy.models import TaskModel\n\n# TaskModel will use \"my_custom_tasks\" as table name\nprint(TaskModel.__tablename__)  # Output: \"my_custom_tasks\"\n</code></pre> <p>Important: You must set the environment variable before importing the model for the first time.</p>"},{"location":"architecture/configuration/#table-purpose","title":"Table Purpose","text":"<p>The <code>TaskModel</code> table stores: - Task definitions: Orchestration metadata (dependencies, priority, schemas) - Execution results: Latest execution status, result, error, progress - Task tree structure: Parent-child relationships</p> <p>This is different from A2A Protocol's <code>tasks</code> table, which stores: - Execution instances: LLM message context, history, artifacts - Dynamic execution data: Per-execution information</p>"},{"location":"architecture/configuration/#migration","title":"Migration","text":"<p>If you need to rename an existing table, use SQL:</p> <pre><code>-- Rename existing table\nALTER TABLE tasks RENAME TO apflow_tasks;\n\n-- Or if using custom name\nALTER TABLE tasks RENAME TO my_custom_tasks;\n</code></pre>"},{"location":"architecture/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Use prefix: Consider using a prefix (e.g., <code>myapp_tasks</code>) to avoid conflicts</li> <li>Set early: Set the environment variable before any imports</li> <li>Document: Document your custom table name in your project documentation</li> <li>Consistency: Use the same table name across all environments (dev, staging, prod)</li> </ol>"},{"location":"architecture/configuration/#examples","title":"Examples","text":""},{"location":"architecture/configuration/#development","title":"Development","text":"<pre><code>export APFLOW_TASK_TABLE_NAME=\"apflow_tasks_dev\"\n</code></pre>"},{"location":"architecture/configuration/#production","title":"Production","text":"<pre><code>export APFLOW_TASK_TABLE_NAME=\"apflow_tasks_prod\"\n</code></pre>"},{"location":"architecture/configuration/#multi-tenant","title":"Multi-tenant","text":"<pre><code>export APFLOW_TASK_TABLE_NAME=\"apflow_tasks_tenant_1\"\n</code></pre>"},{"location":"architecture/diagrams/","title":"Architecture Diagrams","text":"<p>This document contains visual diagrams that illustrate the architecture, execution flows, and key processes of apflow.</p>"},{"location":"architecture/diagrams/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Task Execution Sequence Diagram</li> <li>Task Orchestration Flow Diagram</li> <li>A2A Protocol Interaction Sequence Diagram</li> <li>Task Lifecycle State Diagram</li> <li>Dependency Resolution Flow Diagram</li> </ol>"},{"location":"architecture/diagrams/#task-execution-sequence-diagram","title":"Task Execution Sequence Diagram","text":"<p>This diagram shows the complete flow from API request to task completion, including all major components involved in task execution.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant APIServer as A2A Protocol Server\n    participant AgentExecutor as AgentExecutor\n    participant TaskRoutes as TaskRoutes\n    participant TaskExecutor as TaskExecutor\n    participant TaskManager as TaskManager\n    participant Executor as Executor (CrewaiExecutor/HTTP/etc)\n    participant Storage as Storage (Database)\n\n    Client-&gt;&gt;APIServer: POST /tasks.execute\n    APIServer-&gt;&gt;AgentExecutor: execute(context, event_queue)\n    AgentExecutor-&gt;&gt;TaskRoutes: handle_task_execute(params)\n    TaskRoutes-&gt;&gt;TaskExecutor: execute_task_tree(task_tree)\n\n    TaskExecutor-&gt;&gt;TaskExecutor: Mark tasks for re-execution\n    TaskExecutor-&gt;&gt;TaskManager: Create TaskManager instance\n    TaskExecutor-&gt;&gt;TaskManager: distribute_task_tree(task_tree)\n\n    loop For each task in tree\n        TaskManager-&gt;&gt;TaskManager: Check dependencies\n        TaskManager-&gt;&gt;TaskManager: Check priority\n        alt Dependencies satisfied\n            TaskManager-&gt;&gt;Storage: Update task status (in_progress)\n            TaskManager-&gt;&gt;Executor: Execute task\n            Executor--&gt;&gt;TaskManager: Return result\n            TaskManager-&gt;&gt;Storage: Update task status (completed)\n            TaskManager-&gt;&gt;TaskManager: Check dependent tasks\n        else Dependencies not satisfied\n            TaskManager-&gt;&gt;TaskManager: Wait for dependencies\n        end\n    end\n\n    TaskManager--&gt;&gt;TaskExecutor: All tasks completed\n    TaskExecutor--&gt;&gt;TaskRoutes: Return result\n    TaskRoutes--&gt;&gt;AgentExecutor: Return response\n    AgentExecutor--&gt;&gt;APIServer: Return result\n    APIServer--&gt;&gt;Client: JSON Response / SSE Stream</code></pre>"},{"location":"architecture/diagrams/#task-orchestration-flow-diagram","title":"Task Orchestration Flow Diagram","text":"<p>This diagram illustrates how TaskManager orchestrates task execution, including dependency resolution, priority scheduling, and state management.</p> <pre><code>flowchart TD\n    Start([Start: Task Tree Execution]) --&gt; LoadTree[Load Task Tree]\n    LoadTree --&gt; MarkReexec[Mark Tasks for Re-execution]\n    MarkReexec --&gt; InitManager[Initialize TaskManager]\n    InitManager --&gt; ProcessRoot[Process Root Task]\n\n    ProcessRoot --&gt; CheckStatus{Task Status?}\n    CheckStatus --&gt;|pending| CheckDeps[Check Dependencies]\n    CheckStatus --&gt;|failed| CheckDeps\n    CheckStatus --&gt;|completed| CheckReexec{Marked for&lt;br/&gt;Re-execution?}\n    CheckStatus --&gt;|in_progress| CheckReexec\n\n    CheckReexec --&gt;|Yes| CheckDeps\n    CheckReexec --&gt;|No| SkipTask[Skip Task]\n\n    CheckDeps --&gt; AllDepsSatisfied{All Dependencies&lt;br/&gt;Satisfied?}\n    AllDepsSatisfied --&gt;|No| WaitDeps[Wait for Dependencies]\n    WaitDeps --&gt; CheckDeps\n\n    AllDepsSatisfied --&gt;|Yes| CheckPriority[Check Priority]\n    CheckPriority --&gt; ReadyQueue[Add to Ready Queue]\n    ReadyQueue --&gt; SortByPriority[Sort by Priority]\n    SortByPriority --&gt; ExecuteTask[Execute Task]\n\n    ExecuteTask --&gt; UpdateStatus1[Update Status: in_progress]\n    UpdateStatus1 --&gt; RunExecutor[Run Executor]\n    RunExecutor --&gt; ExecSuccess{Execution&lt;br/&gt;Successful?}\n\n    ExecSuccess --&gt;|Yes| UpdateStatus2[Update Status: completed]\n    ExecSuccess --&gt;|No| UpdateStatus3[Update Status: failed]\n\n    UpdateStatus2 --&gt; MergeResults[Merge Dependency Results]\n    UpdateStatus3 --&gt; MergeResults\n    MergeResults --&gt; UpdateStorage[Update Storage]\n    UpdateStorage --&gt; CheckDependents[Check Dependent Tasks]\n\n    CheckDependents --&gt; MoreTasks{More Tasks&lt;br/&gt;in Tree?}\n    MoreTasks --&gt;|Yes| ProcessRoot\n    MoreTasks --&gt;|No| AllComplete{All Tasks&lt;br/&gt;Complete?}\n\n    SkipTask --&gt; MoreTasks\n    AllComplete --&gt;|Yes| End([End: Execution Complete])\n    AllComplete --&gt;|No| WaitDeps</code></pre>"},{"location":"architecture/diagrams/#a2a-protocol-interaction-sequence-diagram","title":"A2A Protocol Interaction Sequence Diagram","text":"<p>This diagram shows how the A2A Protocol Server handles requests, from client request to task execution and response.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant A2AServer as A2A Protocol Server\n    participant AgentExecutor as AgentExecutor\n    participant TaskRoutes as TaskRoutes\n    participant TaskExecutor as TaskExecutor\n    participant TaskManager as TaskManager\n    participant EventQueue as EventQueue (SSE/WebSocket)\n\n    Client-&gt;&gt;A2AServer: HTTP POST / (JSON-RPC)\n    Note over Client,A2AServer: Request: {\"method\": \"tasks.execute\", \"params\": {...}}\n\n    A2AServer-&gt;&gt;AgentExecutor: execute(context, event_queue)\n    Note over A2AServer,AgentExecutor: RequestContext contains method, params, metadata\n\n    AgentExecutor-&gt;&gt;AgentExecutor: Extract method from context\n    alt Method is \"tasks.execute\"\n        AgentExecutor-&gt;&gt;AgentExecutor: Check streaming mode\n        alt Streaming Mode\n            AgentExecutor-&gt;&gt;TaskRoutes: handle_task_execute(params, streaming=True)\n            TaskRoutes-&gt;&gt;TaskExecutor: execute_task_tree(task_tree, use_streaming=True)\n            TaskExecutor-&gt;&gt;TaskManager: distribute_task_tree_with_streaming()\n            TaskManager-&gt;&gt;EventQueue: Stream progress updates\n            EventQueue--&gt;&gt;Client: SSE Events (real-time)\n            Note over EventQueue,Client: Multiple events: status, progress, result\n        else Simple Mode\n            AgentExecutor-&gt;&gt;TaskRoutes: handle_task_execute(params, streaming=False)\n            TaskRoutes-&gt;&gt;TaskExecutor: execute_task_tree(task_tree)\n            TaskExecutor-&gt;&gt;TaskManager: distribute_task_tree()\n            TaskManager--&gt;&gt;TaskExecutor: Execution complete\n            TaskExecutor--&gt;&gt;TaskRoutes: Return result\n            TaskRoutes--&gt;&gt;AgentExecutor: Return response\n            AgentExecutor--&gt;&gt;A2AServer: Return Task object\n            A2AServer--&gt;&gt;Client: JSON Response\n        end\n    else Other Methods (tasks.create, tasks.get, etc.)\n        AgentExecutor-&gt;&gt;TaskRoutes: Route to appropriate handler\n        TaskRoutes--&gt;&gt;AgentExecutor: Return result\n        AgentExecutor--&gt;&gt;A2AServer: Return response\n        A2AServer--&gt;&gt;Client: JSON Response\n    end</code></pre>"},{"location":"architecture/diagrams/#task-lifecycle-state-diagram","title":"Task Lifecycle State Diagram","text":"<p>This diagram shows all possible state transitions for a task during its lifecycle.</p> <pre><code>stateDiagram-v2\n    [*] --&gt; pending: Task Created\n\n    pending --&gt; in_progress: Execution Started\n    pending --&gt; cancelled: User Cancellation\n\n    in_progress --&gt; completed: Execution Successful\n    in_progress --&gt; failed: Execution Failed\n    in_progress --&gt; cancelled: User Cancellation\n\n    completed --&gt; [*]: Task Finished\n    failed --&gt; [*]: Task Finished\n    cancelled --&gt; [*]: Task Finished\n\n    note right of pending\n        Initial state after task creation.\n        Waiting for dependencies to be satisfied.\n    end note\n\n    note right of in_progress\n        Task is currently executing.\n        Executor is running the task logic.\n    end note\n\n    note right of completed\n        Task finished successfully.\n        Result is available in task.result.\n    end note\n\n    note right of failed\n        Task execution failed.\n        Error details in task.error.\n    end note\n\n    note right of cancelled\n        Task was cancelled before completion.\n        Can be cancelled from any active state.\n    end note</code></pre>"},{"location":"architecture/diagrams/#dependency-resolution-flow-diagram","title":"Dependency Resolution Flow Diagram","text":"<p>This diagram illustrates how the system resolves task dependencies, waits for dependencies to complete, and merges dependency results into task inputs.</p> <pre><code>flowchart TD\n    Start([Task Ready to Execute]) --&gt; GetDeps[Get Task Dependencies]\n    GetDeps --&gt; HasDeps{Has&lt;br/&gt;Dependencies?}\n\n    HasDeps --&gt;|No| ExecuteTask[Execute Task Immediately]\n    HasDeps --&gt;|Yes| CheckDepStatus[Check Each Dependency Status]\n\n    CheckDepStatus --&gt; AllRequiredComplete{All Required&lt;br/&gt;Dependencies&lt;br/&gt;Complete?}\n\n    AllRequiredComplete --&gt;|No| CheckOptional{Has Optional&lt;br/&gt;Dependencies?}\n    AllRequiredComplete --&gt;|Yes| MergeResults[Merge Dependency Results]\n\n    CheckOptional --&gt;|Yes| CheckOptionalStatus{Optional Dependencies&lt;br/&gt;Complete or Failed?}\n    CheckOptional --&gt;|No| WaitForDeps[Wait for Required Dependencies]\n\n    CheckOptionalStatus --&gt;|Complete| MergeResults\n    CheckOptionalStatus --&gt;|Failed| MergeResults\n    CheckOptionalStatus --&gt;|In Progress| WaitForDeps\n\n    WaitForDeps --&gt; CheckDepStatus\n\n    MergeResults --&gt; CollectResults[Collect Results from Dependencies]\n    CollectResults --&gt; MergeWithInputs[Merge with Task Inputs]\n\n    MergeWithInputs --&gt; ExecuteTask\n\n    ExecuteTask --&gt; TaskComplete([Task Execution Complete])\n\n    style Start fill:#e1f5ff\n    style ExecuteTask fill:#c8e6c9\n    style TaskComplete fill:#c8e6c9\n    style WaitForDeps fill:#fff9c4\n    style MergeResults fill:#e1bee7</code></pre>"},{"location":"architecture/diagrams/#component-interaction-overview","title":"Component Interaction Overview","text":"<p>This diagram provides a high-level view of how major components interact in the system.</p> <pre><code>flowchart LR\n    subgraph External[\"External Interface\"]\n        Client[Client Applications]\n        CLI[CLI Tools]\n    end\n\n    subgraph API[\"API Layer\"]\n        A2A[A2A Protocol Server]\n        Routes[TaskRoutes]\n    end\n\n    subgraph Core[\"Core Orchestration\"]\n        Executor[TaskExecutor]\n        Manager[TaskManager]\n    end\n\n    subgraph Execution[\"Execution Layer\"]\n        CrewAI[CrewaiExecutor]\n        HTTP[HTTPExecutor]\n        Custom[Custom Executors]\n    end\n\n    subgraph Support[\"Support Layer\"]\n        DB[(Database)]\n        Stream[Streaming]\n    end\n\n    Client --&gt; A2A\n    CLI --&gt; Executor\n    A2A --&gt; Routes\n    Routes --&gt; Executor\n    Executor --&gt; Manager\n    Manager --&gt; CrewAI\n    Manager --&gt; HTTP\n    Manager --&gt; Custom\n    Manager --&gt; DB\n    Manager --&gt; Stream\n    Stream --&gt; A2A\n\n    style External fill:#e1f5ff\n    style API fill:#fff4e1\n    style Core fill:#e8f5e9\n    style Execution fill:#f3e5f5\n    style Support fill:#e0f2f1</code></pre>"},{"location":"architecture/diagrams/#notes-on-diagram-usage","title":"Notes on Diagram Usage","text":"<p>These diagrams are designed to:</p> <ol> <li>Help developers understand the system architecture and data flow</li> <li>Guide implementation by showing the sequence of operations</li> <li>Aid debugging by visualizing the execution path</li> <li>Support documentation for new contributors</li> </ol> <p>All diagrams use Mermaid syntax and should render correctly in MkDocs when using: - <code>mkdocs-mermaid2-plugin</code>, or - Material for MkDocs theme with Mermaid support</p> <p>For more details on specific components, see: - Architecture Overview - Detailed component descriptions - Task Orchestration Guide - Task orchestration patterns - Core Concepts - Fundamental concepts</p>"},{"location":"architecture/directory-structure/","title":"Directory Structure","text":"<p>This document describes the directory structure of the <code>apflow</code> project.</p>"},{"location":"architecture/directory-structure/#core-framework-core","title":"Core Framework (<code>core/</code>)","text":"<p>The core framework provides task orchestration and execution specifications. All core modules are always included when installing <code>apflow</code>.</p> <pre><code>core/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 types.py                # Core type definitions (TaskStatus, TaskTreeNode, webhook types)\n\u251c\u2500\u2500 builders.py\n\u251c\u2500\u2500 config_manager.py\n\u251c\u2500\u2500 decorators.py\n\u251c\u2500\u2500 base/\n\u2502   \u2514\u2500\u2500 base_task.py\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 registry.py\n\u251c\u2500\u2500 dependency/\n\u2502   \u2514\u2500\u2500 dependency_validator.py\n\u251c\u2500\u2500 execution/\n\u2502   \u251c\u2500\u2500 errors.py\n\u2502   \u251c\u2500\u2500 executor_registry.py\n\u2502   \u251c\u2500\u2500 streaming_callbacks.py\n\u2502   \u251c\u2500\u2500 task_creator.py\n\u2502   \u251c\u2500\u2500 task_executor.py\n\u2502   \u251c\u2500\u2500 task_manager.py\n\u2502   \u2514\u2500\u2500 task_tracker.py\n\u251c\u2500\u2500 extensions/\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u251c\u2500\u2500 decorators.py\n\u2502   \u251c\u2500\u2500 executor_metadata.py\n\u2502   \u251c\u2500\u2500 hook.py\n\u2502   \u251c\u2500\u2500 manager.py\n\u2502   \u251c\u2500\u2500 protocol.py\n\u2502   \u251c\u2500\u2500 registry.py\n\u2502   \u251c\u2500\u2500 scanner.py            # Extension auto-discovery\n\u2502   \u251c\u2500\u2500 storage.py\n\u2502   \u2514\u2500\u2500 types.py\n\u251c\u2500\u2500 interfaces/\n\u2502   \u2514\u2500\u2500 executable_task.py\n\u251c\u2500\u2500 storage/\n\u2502   \u251c\u2500\u2500 context.py\n\u2502   \u251c\u2500\u2500 factory.py\n\u2502   \u251c\u2500\u2500 migrate.py\n\u2502   \u251c\u2500\u2500 dialects/\n\u2502   \u2502   \u251c\u2500\u2500 duckdb.py\n\u2502   \u2502   \u251c\u2500\u2500 postgres.py\n\u2502   \u2502   \u2514\u2500\u2500 registry.py\n\u2502   \u251c\u2500\u2500 migrations/\n\u2502   \u2502   \u251c\u2500\u2500 001_add_task_tree_fields.py\n\u2502   \u2502   \u2514\u2500\u2500 002_add_scheduling_fields.py\n\u2502   \u2514\u2500\u2500 sqlalchemy/\n\u2502       \u251c\u2500\u2500 models.py\n\u2502       \u251c\u2500\u2500 schedule_calculator.py  # Schedule time calculation\n\u2502       \u2514\u2500\u2500 task_repository.py\n\u251c\u2500\u2500 tools/\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u251c\u2500\u2500 decorators.py\n\u2502   \u2514\u2500\u2500 registry.py\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 helpers.py\n\u2502   \u251c\u2500\u2500 llm_key_context.py\n\u2502   \u251c\u2500\u2500 llm_key_injector.py\n\u2502   \u251c\u2500\u2500 logger.py\n\u2502   \u2514\u2500\u2500 project_detection.py\n\u2514\u2500\u2500 validator/\n    \u251c\u2500\u2500 dependency_validator.py\n    \u2514\u2500\u2500 user_validator.py\n</code></pre>"},{"location":"architecture/directory-structure/#scheduler-scheduler","title":"Scheduler (<code>scheduler/</code>)","text":"<p>Task scheduling subsystem with internal polling scheduler and external gateway integrations.</p> <p>Installation: <code>pip install apflow[a2a]</code> (uses API for task execution)</p> <pre><code>scheduler/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base.py                 # Base scheduler interface\n\u251c\u2500\u2500 internal.py             # InternalScheduler - polls DB for due tasks\n\u2514\u2500\u2500 gateway/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 ical.py             # iCalendar export for scheduled tasks\n    \u2514\u2500\u2500 webhook.py          # Webhook gateway for external schedulers\n</code></pre>"},{"location":"architecture/directory-structure/#extensions-extensions","title":"Extensions (<code>extensions/</code>)","text":"<p>Framework extensions are optional features that require extra dependencies and are installed separately.</p>"},{"location":"architecture/directory-structure/#crewai-crewai-llm-task-support","title":"[crewai] - CrewAI LLM Task Support","text":"<pre><code>extensions/crewai/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 crewai_executor.py     # CrewaiExecutor - CrewAI wrapper\n\u251c\u2500\u2500 batch_crewai_executor.py    # BatchCrewaiExecutor - batch execution of multiple crews\n\u2514\u2500\u2500 types.py            # CrewaiExecutorState, BatchState\n</code></pre> <p>Installation: <code>pip install apflow[crewai]</code></p>"},{"location":"architecture/directory-structure/#stdio-stdio-executors","title":"[stdio] - Stdio Executors","text":"<pre><code>extensions/stdio/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 command_executor.py      # CommandExecutor - local command execution\n\u2514\u2500\u2500 system_info_executor.py  # SystemInfoExecutor - system resource queries\n</code></pre> <p>Installation: Included in core (no extra required)</p>"},{"location":"architecture/directory-structure/#core-core-extensions","title":"[core] - Core Extensions","text":"<pre><code>extensions/core/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 aggregate_results_executor.py  # AggregateResultsExecutor - dependency result aggregation\n</code></pre> <p>Installation: Included in core (no extra required)</p>"},{"location":"architecture/directory-structure/#apflow-apflow-api-executor","title":"[apflow] - APFlow API Executor","text":"<pre><code>extensions/apflow/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 api_executor.py          # ApiExecutor - execute tasks via APFlow API\n</code></pre> <p>Installation: Included in core (no extra required)</p>"},{"location":"architecture/directory-structure/#email-email-executor","title":"[email] - Email Executor","text":"<pre><code>extensions/email/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 send_email_executor.py     # SendEmailExecutor - Resend API and SMTP\n</code></pre> <p>Installation: <code>pip install apflow[email]</code></p>"},{"location":"architecture/directory-structure/#hooks-hook-extensions","title":"[hooks] - Hook Extensions","text":"<pre><code>extensions/hooks/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 pre_execution_hook.py   # Pre-execution hook implementation\n\u2514\u2500\u2500 post_execution_hook.py  # Post-execution hook implementation\n</code></pre> <p>Installation: Included in core (no extra required)</p>"},{"location":"architecture/directory-structure/#storage-storage-extensions","title":"[storage] - Storage Extensions","text":"<pre><code>extensions/storage/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 duckdb_storage.py   # DuckDB storage implementation\n\u2514\u2500\u2500 postgres_storage.py # PostgreSQL storage implementation\n</code></pre> <p>Installation: Included in core (no extra required)</p>"},{"location":"architecture/directory-structure/#tools-tool-extensions","title":"[tools] - Tool Extensions","text":"<pre><code>extensions/tools/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 github_tools.py          # GitHub analysis tools\n\u2514\u2500\u2500 limited_scrape_tools.py   # Limited website scraping tools\n</code></pre> <p>Installation: Included in core (no extra required)</p>"},{"location":"architecture/directory-structure/#http-http-executor","title":"[http] - HTTP Executor","text":"<pre><code>extensions/http/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 rest_executor.py         # RestExecutor - REST API calls\n</code></pre> <p>Installation: Included in core (no extra required)</p>"},{"location":"architecture/directory-structure/#docker-docker-executor","title":"[docker] - Docker Executor","text":"<pre><code>extensions/docker/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 docker_executor.py       # DockerExecutor - Docker container execution\n</code></pre> <p>Installation: <code>pip install apflow[docker]</code></p>"},{"location":"architecture/directory-structure/#grpc-grpc-executor","title":"[grpc] - gRPC Executor","text":"<pre><code>extensions/grpc/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 grpc_executor.py         # GrpcExecutor - gRPC service calls\n</code></pre> <p>Installation: <code>pip install apflow[grpc]</code></p>"},{"location":"architecture/directory-structure/#ssh-ssh-executor","title":"[ssh] - SSH Executor","text":"<pre><code>extensions/ssh/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 ssh_executor.py          # SshExecutor - remote command execution via SSH\n</code></pre> <p>Installation: <code>pip install apflow[ssh]</code></p>"},{"location":"architecture/directory-structure/#websocket-websocket-executor","title":"[websocket] - WebSocket Executor","text":"<pre><code>extensions/websocket/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 websocket_executor.py    # WebSocketExecutor - WebSocket communication\n</code></pre> <p>Installation: <code>pip install apflow[websocket]</code></p>"},{"location":"architecture/directory-structure/#mcp-mcp-executor","title":"[mcp] - MCP Executor","text":"<pre><code>extensions/mcp/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 mcp_executor.py          # McpExecutor - Model Context Protocol execution\n</code></pre> <p>Installation: <code>pip install apflow[mcp]</code></p>"},{"location":"architecture/directory-structure/#llm-llm-executor","title":"[llm] - LLM Executor","text":"<pre><code>extensions/llm/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 llm_executor.py          # LlmExecutor - direct LLM API calls\n</code></pre> <p>Installation: <code>pip install apflow[llm]</code></p>"},{"location":"architecture/directory-structure/#llm_key_config-llm-key-configuration","title":"[llm_key_config] - LLM Key Configuration","text":"<pre><code>extensions/llm_key_config/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 config_manager.py        # LLM API key configuration management\n</code></pre> <p>Installation: Included in core (no extra required)</p>"},{"location":"architecture/directory-structure/#scrape-web-scraping-executor","title":"[scrape] - Web Scraping Executor","text":"<pre><code>extensions/scrape/\n\u251c\u2500\u2500 __init__.py\n\u2514\u2500\u2500 scrape_executor.py       # ScrapeExecutor - web page scraping\n</code></pre> <p>Installation: <code>pip install apflow[scrape]</code></p>"},{"location":"architecture/directory-structure/#generate-task-tree-generation","title":"[generate] - Task Tree Generation","text":"<pre><code>extensions/generate/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 docs_loader.py           # Documentation loading for LLM context\n\u251c\u2500\u2500 executor_info.py         # Executor metadata extraction\n\u251c\u2500\u2500 generate_executor.py     # GenerateExecutor - LLM-based task tree generation\n\u251c\u2500\u2500 llm_client.py            # LLM API client\n\u251c\u2500\u2500 multi_phase_crew.py      # Multi-phase CrewAI generation\n\u251c\u2500\u2500 principles_extractor.py  # Design principles extraction\n\u2514\u2500\u2500 schema_formatter.py      # Schema formatting for LLM prompts\n</code></pre> <p>Installation: <code>pip install apflow[generate]</code></p>"},{"location":"architecture/directory-structure/#api-service-api","title":"API Service (<code>api/</code>)","text":"<p>Unified external API service layer supporting multiple network protocols.</p> <p>Current Implementation: A2A Protocol Server (Agent-to-Agent communication protocol) - Supports HTTP, SSE, and WebSocket transport layers - Implements A2A Protocol standard for agent-to-agent communication</p> <p>Future Extensions: May include additional protocols such as REST API endpoints</p> <p>Installation: <code>pip install apflow[a2a]</code></p> <pre><code>api/\n\u251c\u2500\u2500 __init__.py            # API module exports\n\u251c\u2500\u2500 main.py                # CLI entry point (main() function and uvicorn server)\n\u251c\u2500\u2500 app.py                 # Application creation (create_app_by_protocol, create_a2a_server, create_mcp_server)\n\u251c\u2500\u2500 capabilities.py        # Server capability declarations\n\u251c\u2500\u2500 protocols.py           # Protocol management (get_protocol_from_env, check_protocol_dependency)\n\u251c\u2500\u2500 a2a/                   # A2A Protocol Server implementation\n\u2502   \u251c\u2500\u2500 __init__.py        # A2A module exports\n\u2502   \u251c\u2500\u2500 server.py          # A2A server creation\n\u2502   \u251c\u2500\u2500 agent_executor.py  # A2A agent executor\n\u2502   \u251c\u2500\u2500 custom_starlette_app.py  # Custom A2A Starlette application\n\u2502   \u251c\u2500\u2500 event_queue_bridge.py    # Event queue bridge\n\u2502   \u2514\u2500\u2500 task_routes_adapter.py   # TaskRoutes adapter for A2A\n\u251c\u2500\u2500 docs/                  # API documentation\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 openapi.py         # OpenAPI schema generation\n\u2502   \u2514\u2500\u2500 swagger_ui.py      # Swagger UI endpoint\n\u251c\u2500\u2500 mcp/                   # MCP (Model Context Protocol) Server implementation\n\u2502   \u251c\u2500\u2500 __init__.py        # MCP module exports\n\u2502   \u251c\u2500\u2500 server.py          # MCP server creation\n\u2502   \u251c\u2500\u2500 adapter.py         # TaskRoutes adapter for MCP\n\u2502   \u251c\u2500\u2500 tools.py           # MCP tools registry\n\u2502   \u251c\u2500\u2500 resources.py       # MCP resources registry\n\u2502   \u251c\u2500\u2500 transport_http.py  # HTTP transport\n\u2502   \u2514\u2500\u2500 transport_stdio.py # Stdio transport\n\u251c\u2500\u2500 middleware/            # Middleware components\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 db_session.py     # Database session middleware\n\u2514\u2500\u2500 routes/                # Protocol-agnostic route handlers\n    \u251c\u2500\u2500 __init__.py        # Route handlers exports\n    \u251c\u2500\u2500 base.py            # BaseRouteHandler - shared functionality\n    \u251c\u2500\u2500 docs.py            # Documentation route handlers\n    \u251c\u2500\u2500 tasks.py           # TaskRoutes - task management handlers\n    \u2514\u2500\u2500 system.py          # SystemRoutes - system operation handlers\n</code></pre> <p>Route Handlers Architecture:</p> <p>The <code>api/routes/</code> directory contains protocol-agnostic route handlers that can be used by any protocol implementation (A2A, REST, GraphQL, etc.):</p> <ul> <li><code>base.py</code>: Provides <code>BaseRouteHandler</code> class with shared functionality for permission checking, user information extraction, and common utilities</li> <li><code>tasks.py</code>: Contains <code>TaskRoutes</code> class with handlers for task CRUD operations, execution, and monitoring</li> <li><code>system.py</code>: Contains <code>SystemRoutes</code> class with handlers for system operations like health checks, LLM key configuration, and examples management</li> </ul> <p>These handlers are designed to be protocol-agnostic, allowing them to be reused across different protocol implementations.</p>"},{"location":"architecture/directory-structure/#cli-tools-cli","title":"CLI Tools (<code>cli/</code>)","text":"<p>Command-line interface for task management.</p> <p>Installation: <code>pip install apflow[cli]</code></p> <pre><code>cli/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 main.py                # CLI entry point\n\u251c\u2500\u2500 api_client.py          # API client for remote operations\n\u251c\u2500\u2500 api_gateway_helper.py  # API vs direct DB mode helper\n\u251c\u2500\u2500 cli_config.py          # CLI configuration management\n\u251c\u2500\u2500 jwt_token.py           # JWT token generation\n\u251c\u2500\u2500 extension.py           # CLI extension management\n\u251c\u2500\u2500 decorators.py          # CLI decorators\n\u2514\u2500\u2500 commands/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py          # Configuration commands\n    \u251c\u2500\u2500 daemon.py          # Daemon management commands\n    \u251c\u2500\u2500 executors.py       # Executor management commands\n    \u251c\u2500\u2500 generate.py        # Task tree generation commands\n    \u251c\u2500\u2500 run.py             # Task execution commands\n    \u251c\u2500\u2500 scheduler.py       # Scheduler commands (start, list, export-ical)\n    \u251c\u2500\u2500 serve.py           # API server commands\n    \u2514\u2500\u2500 tasks.py           # Task management commands\n</code></pre>"},{"location":"architecture/directory-structure/#test-suite-tests","title":"Test Suite (<code>tests/</code>)","text":"<p>Test suite organized to mirror the source code structure.</p> <pre><code>tests/\n\u251c\u2500\u2500 conftest.py                     # Shared fixtures and configuration\n\u251c\u2500\u2500 core/                           # Core framework tests\n\u2502   \u251c\u2500\u2500 execution/                  # Task orchestration tests\n\u2502   \u2502   \u251c\u2500\u2500 test_task_manager.py\n\u2502   \u2502   \u251c\u2500\u2500 test_task_creator.py\n\u2502   \u2502   \u251c\u2500\u2500 test_task_executor_additional.py\n\u2502   \u2502   \u251c\u2500\u2500 test_task_executor_concurrent.py\n\u2502   \u2502   \u251c\u2500\u2500 test_task_executor_tools_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_task_failure_handling.py\n\u2502   \u2502   \u2514\u2500\u2500 test_task_reexecution.py\n\u2502   \u251c\u2500\u2500 storage/                    # Storage tests\n\u2502   \u2502   \u251c\u2500\u2500 sqlalchemy/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_task_repository.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_task_repository_additional.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_custom_task_model.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_flag_modified.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 test_scheduling.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 test_session_pool.py\n\u2502   \u2502   \u251c\u2500\u2500 test_context.py\n\u2502   \u2502   \u251c\u2500\u2500 test_database_path_priority.py\n\u2502   \u2502   \u251c\u2500\u2500 test_hook_context.py\n\u2502   \u2502   \u251c\u2500\u2500 test_hook_modify_task_with_context.py\n\u2502   \u2502   \u2514\u2500\u2500 test_migration.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 test_llm_key_context.py\n\u2502   \u2502   \u2514\u2500\u2500 test_project_detection.py\n\u2502   \u251c\u2500\u2500 validator/\n\u2502   \u2502   \u2514\u2500\u2500 test_dependency_validator.py\n\u2502   \u251c\u2500\u2500 test_config_manager.py\n\u2502   \u251c\u2500\u2500 test_decorators.py\n\u2502   \u251c\u2500\u2500 test_demo_mode.py\n\u2502   \u251c\u2500\u2500 test_executor_hooks.py\n\u2502   \u251c\u2500\u2500 test_extension_scanner.py\n\u2502   \u251c\u2500\u2500 test_task_builder.py\n\u2502   \u251c\u2500\u2500 test_task_tree_hooks.py\n\u2502   \u2514\u2500\u2500 test_user_id_extraction.py\n\u251c\u2500\u2500 extensions/                     # Extension tests\n\u2502   \u251c\u2500\u2500 apflow/\n\u2502   \u2502   \u2514\u2500\u2500 test_api_executor.py\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u2502   \u2514\u2500\u2500 test_aggregate_results_executor.py\n\u2502   \u251c\u2500\u2500 crewai/\n\u2502   \u2502   \u251c\u2500\u2500 test_crewai_executor.py\n\u2502   \u2502   \u2514\u2500\u2500 test_batch_crewai_executor.py\n\u2502   \u251c\u2500\u2500 docker/\n\u2502   \u2502   \u2514\u2500\u2500 test_docker_executor.py\n\u2502   \u251c\u2500\u2500 email/\n\u2502   \u2502   \u2514\u2500\u2500 test_send_email_executor.py\n\u2502   \u251c\u2500\u2500 generate/\n\u2502   \u2502   \u251c\u2500\u2500 test_crewai_dependency_fix.py\n\u2502   \u2502   \u251c\u2500\u2500 test_crewai_schema_mapping.py\n\u2502   \u2502   \u251c\u2500\u2500 test_docs_loader.py\n\u2502   \u2502   \u251c\u2500\u2500 test_executor_info.py\n\u2502   \u2502   \u251c\u2500\u2500 test_generate_crewai.py\n\u2502   \u2502   \u251c\u2500\u2500 test_generate_executor.py\n\u2502   \u2502   \u251c\u2500\u2500 test_generate_executor_enhanced.py\n\u2502   \u2502   \u251c\u2500\u2500 test_generate_real_scenario.py\n\u2502   \u2502   \u251c\u2500\u2500 test_integration.py\n\u2502   \u2502   \u251c\u2500\u2500 test_llm_client.py\n\u2502   \u2502   \u251c\u2500\u2500 test_llm_scrape_executor.py\n\u2502   \u2502   \u251c\u2500\u2500 test_multi_phase_crew.py\n\u2502   \u2502   \u251c\u2500\u2500 test_principles_extractor.py\n\u2502   \u2502   \u2514\u2500\u2500 test_schema_formatter.py\n\u2502   \u251c\u2500\u2500 grpc/\n\u2502   \u2502   \u2514\u2500\u2500 test_grpc_executor.py\n\u2502   \u251c\u2500\u2500 http/\n\u2502   \u2502   \u2514\u2500\u2500 test_rest_executor.py\n\u2502   \u251c\u2500\u2500 llm/\n\u2502   \u2502   \u2514\u2500\u2500 test_llm_executor.py\n\u2502   \u251c\u2500\u2500 mcp/\n\u2502   \u2502   \u2514\u2500\u2500 test_mcp_executor.py\n\u2502   \u251c\u2500\u2500 ssh/\n\u2502   \u2502   \u2514\u2500\u2500 test_ssh_executor.py\n\u2502   \u251c\u2500\u2500 stdio/\n\u2502   \u2502   \u251c\u2500\u2500 test_command_executor.py\n\u2502   \u2502   \u2514\u2500\u2500 test_system_info_executor.py\n\u2502   \u251c\u2500\u2500 tools/\n\u2502   \u2502   \u251c\u2500\u2500 test_tools_decorator.py\n\u2502   \u2502   \u2514\u2500\u2500 test_limited_scrape_tools.py\n\u2502   \u2514\u2500\u2500 websocket/\n\u2502       \u2514\u2500\u2500 test_websocket_executor.py\n\u251c\u2500\u2500 api/                            # API service tests\n\u2502   \u251c\u2500\u2500 a2a/\n\u2502   \u2502   \u251c\u2500\u2500 test_a2a_client.py\n\u2502   \u2502   \u251c\u2500\u2500 test_agent_executor.py\n\u2502   \u2502   \u251c\u2500\u2500 test_docs_routes.py\n\u2502   \u2502   \u2514\u2500\u2500 test_http_json_rpc.py\n\u2502   \u251c\u2500\u2500 mcp/\n\u2502   \u2502   \u251c\u2500\u2500 test_adapter.py\n\u2502   \u2502   \u251c\u2500\u2500 test_resources.py\n\u2502   \u2502   \u251c\u2500\u2500 test_server.py\n\u2502   \u2502   \u251c\u2500\u2500 test_tools.py\n\u2502   \u2502   \u2514\u2500\u2500 test_transport_http.py\n\u2502   \u251c\u2500\u2500 test_executor_permissions.py\n\u2502   \u251c\u2500\u2500 test_executors_api.py\n\u2502   \u251c\u2500\u2500 test_main.py\n\u2502   \u251c\u2500\u2500 test_main_env_loading.py\n\u2502   \u251c\u2500\u2500 test_scheduler_routes.py\n\u2502   \u251c\u2500\u2500 test_system_executors_endpoint.py\n\u2502   \u251c\u2500\u2500 test_task_routes_extension.py\n\u2502   \u251c\u2500\u2500 test_task_update_validation.py\n\u2502   \u2514\u2500\u2500 test_tasks_routes.py\n\u251c\u2500\u2500 cli/                            # CLI tests\n\u2502   \u251c\u2500\u2500 test_api_gateway.py\n\u2502   \u251c\u2500\u2500 test_api_server_fallback.py\n\u2502   \u251c\u2500\u2500 test_cli_api_gateway_integration.py\n\u2502   \u251c\u2500\u2500 test_cli_config.py\n\u2502   \u251c\u2500\u2500 test_cli_env_loading.py\n\u2502   \u251c\u2500\u2500 test_cli_extension.py\n\u2502   \u251c\u2500\u2500 test_config_command.py\n\u2502   \u251c\u2500\u2500 test_daemon_command.py\n\u2502   \u251c\u2500\u2500 test_decorators.py\n\u2502   \u251c\u2500\u2500 test_entry_point_function.py\n\u2502   \u251c\u2500\u2500 test_executors_command.py\n\u2502   \u251c\u2500\u2500 test_generate_command.py\n\u2502   \u251c\u2500\u2500 test_run_command.py\n\u2502   \u251c\u2500\u2500 test_scheduler_command.py\n\u2502   \u251c\u2500\u2500 test_serve_command.py\n\u2502   \u251c\u2500\u2500 test_tasks_command.py\n\u2502   \u2514\u2500\u2500 test_tasks_command_api_mode.py\n\u251c\u2500\u2500 scheduler/                      # Scheduler tests\n\u2502   \u251c\u2500\u2500 test_scheduler.py\n\u2502   \u2514\u2500\u2500 test_webhook_api.py\n\u2514\u2500\u2500 integration/                    # Integration tests\n    \u251c\u2500\u2500 test_aggregate_results_integration.py\n    \u251c\u2500\u2500 test_config_manager_integration.py\n    \u2514\u2500\u2500 test_schema_based_execution.py\n</code></pre> <p>Note: Test structure mirrors source code structure for easy navigation and maintenance.</p>"},{"location":"architecture/exception-handling/","title":"Exception Handling Standards","text":""},{"location":"architecture/exception-handling/#overview","title":"Overview","text":"<p>This document defines the exception handling standards for AiPartnerUpFlow, based on best practices from FastAPI, CrewAI, and production frameworks.</p>"},{"location":"architecture/exception-handling/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>ApflowError (RuntimeError)\n\u251c\u2500\u2500 BusinessError (expected/user errors)\n\u2502   \u251c\u2500\u2500 ValidationError\n\u2502   \u2514\u2500\u2500 ConfigurationError\n\u2514\u2500\u2500 SystemError (unexpected errors)\n    \u251c\u2500\u2500 ExecutorError  \n    \u2514\u2500\u2500 StorageError\n</code></pre>"},{"location":"architecture/exception-handling/#exception-types","title":"Exception Types","text":""},{"location":"architecture/exception-handling/#businesserror","title":"BusinessError","text":"<p>Purpose: Expected failures caused by user input, missing configuration, or business logic constraints.</p> <p>Logging: WITHOUT stack trace (<code>exc_info=False</code>)</p> <p>Use cases: - Invalid input parameters - Missing required configuration - Permission/quota violations - Resource not found (user-specified)</p> <p>Examples: <pre><code># Validation\nif not inputs.get(\"model\"):\n    raise ValidationError(\"model is required in inputs\")\n\n# Configuration  \nif not LIBRARY_AVAILABLE:\n    raise ConfigurationError(\"library X is not installed. Install with: pip install X\")\n\n# Business logic\nif user.quota_exceeded():\n    raise BusinessError(\"API quota exceeded for this user\")\n</code></pre></p>"},{"location":"architecture/exception-handling/#systemerror","title":"SystemError","text":"<p>Purpose: Unexpected system-level failures requiring investigation.</p> <p>Logging: WITH stack trace (<code>exc_info=True</code>)</p> <p>Use cases: - Unexpected internal errors - Database corruption - Resource exhaustion</p> <p>Note: Most system errors should propagate naturally (TimeoutError, ConnectionError, etc.) rather than being wrapped.</p>"},{"location":"architecture/exception-handling/#executor-implementation-guidelines","title":"Executor Implementation Guidelines","text":""},{"location":"architecture/exception-handling/#do-let-technical-exceptions-propagate","title":"DO: Let Technical Exceptions Propagate","text":"<pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    # Validate inputs\n    if not inputs.get(\"url\"):\n        raise ValidationError(\"url is required\")\n\n    # Let httpx exceptions propagate naturally\n    # (TimeoutException, ConnectError, etc.)\n    response = await httpx.get(inputs[\"url\"])\n\n    return {\"status\": response.status_code}\n</code></pre>"},{"location":"architecture/exception-handling/#dont-catch-and-return-error-dicts","title":"DON'T: Catch and Return Error Dicts","text":"<pre><code># \u274c BAD - Don't do this\nasync def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    try:\n        response = await httpx.get(inputs[\"url\"])\n    except httpx.TimeoutException:\n        return {\"success\": False, \"error\": \"Timeout\"}  # Wrong!\n</code></pre>"},{"location":"architecture/exception-handling/#do-distinguish-business-vs-system-errors","title":"DO: Distinguish Business vs System Errors","text":"<pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    # Business error - missing config\n    if not os.getenv(\"API_KEY\"):\n        raise ConfigurationError(\"API_KEY environment variable not set\")\n\n    # Technical error - let propagate\n    response = await service.call()  # May raise TimeoutError, ConnectionError\n\n    return {\"result\": response}\n</code></pre>"},{"location":"architecture/exception-handling/#taskmanager-logging","title":"TaskManager Logging","text":"<p>TaskManager handles exceptions with context-aware logging:</p> <pre><code>try:\n    result = await executor.execute(inputs)\nexcept BusinessError as e:\n    # Expected error - log message only\n    logger.error(f\"Business error: {str(e)}\")\n    # Still mark task as failed\nexcept Exception as e:\n    # Unexpected error - log with stack trace\n    logger.error(f\"Unexpected error: {str(e)}\", exc_info=True)\n    # Mark task as failed\n</code></pre>"},{"location":"architecture/exception-handling/#comparison-with-other-frameworks","title":"Comparison with Other Frameworks","text":""},{"location":"architecture/exception-handling/#fastapi-pattern","title":"FastAPI Pattern","text":"<ul> <li><code>HTTPException</code> for client errors (400-499)</li> <li><code>RequestValidationError</code> for input validation</li> <li>Generic exceptions for server errors (500+)</li> <li>Custom exception handlers for different error types</li> </ul>"},{"location":"architecture/exception-handling/#our-pattern","title":"Our Pattern","text":"<ul> <li><code>BusinessError</code> for user/config errors (like FastAPI's validation errors)</li> <li><code>SystemError</code> for unexpected errors (rare, most propagate naturally)</li> <li><code>ApflowError</code> as base for framework-specific exceptions</li> <li>TaskManager acts as central exception handler</li> </ul>"},{"location":"architecture/exception-handling/#migration-notes","title":"Migration Notes","text":"<p>For existing code: 1. Replace <code>return {\"success\": False, \"error\": \"...\"}</code> with <code>raise BusinessError(\"...\")</code> 2. Remove try/except blocks around technical operations (httpx, docker, ssh) 3. Keep business validation and raise <code>ValidationError</code> or <code>ConfigurationError</code> 4. Let timeout/connection/service errors propagate naturally</p>"},{"location":"architecture/exception-handling/#testing","title":"Testing","text":"<p>Tests should verify: <pre><code>@pytest.mark.asyncio\nasync def test_validation_error_marks_failed():\n    task = await create_task_with_missing_input()\n    await task_manager.execute(task)\n\n    # Verify task marked as failed\n    assert task.status == \"failed\"\n    assert \"required\" in task.error\n\n@pytest.mark.asyncio  \nasync def test_timeout_marks_failed():\n    task = await create_task_with_slow_service()\n    await task_manager.execute(task)\n\n    # Timeout propagated and task marked failed\n    assert task.status == \"failed\"\n    assert \"timeout\" in task.error.lower()\n</code></pre></p>"},{"location":"architecture/exception-handling/#summary","title":"Summary","text":"<ol> <li>Use <code>BusinessError</code> subclasses for expected failures (validation, config)</li> <li>Let technical exceptions propagate (timeout, connection, service errors)</li> <li>TaskManager handles all exceptions and marks tasks as failed appropriately</li> <li>Log with context: <code>BusinessError</code> without stack trace, others with stack trace</li> <li>Never return error dicts - always raise exceptions for failures</li> </ol>"},{"location":"architecture/extension-registry-design/","title":"Extension Registry Design - Protocol-Based Architecture","text":""},{"location":"architecture/extension-registry-design/#overview","title":"Overview","text":"<p>The extension registry uses Protocol-based design to avoid circular dependencies while maintaining type safety. This follows the Dependency Inversion Principle (SOLID).</p>"},{"location":"architecture/extension-registry-design/#architecture","title":"Architecture","text":""},{"location":"architecture/extension-registry-design/#core-components","title":"Core Components","text":"<ol> <li><code>Extension</code> (base interface)</li> <li>All extensions must implement this</li> <li> <p>Defines <code>id</code>, <code>category</code>, <code>name</code>, etc.</p> </li> <li> <p><code>ExecutableTask</code> (executor interface)</p> </li> <li>Extends <code>Extension</code></li> <li>Defines <code>execute()</code>, <code>get_input_schema()</code></li> <li> <p>Located in <code>core/interfaces/executable_task.py</code></p> </li> <li> <p><code>ExecutorLike</code> Protocol (structural typing)</p> </li> <li>Defines the \"shape\" of executors</li> <li>Located in <code>core/extensions/protocol.py</code></li> <li> <p>No dependency on ExecutableTask</p> </li> <li> <p><code>ExtensionRegistry</code></p> </li> <li>Uses <code>ExecutorLike</code> protocol for type checking</li> <li>No direct import of ExecutableTask</li> <li>Located in <code>core/extensions/registry.py</code></li> </ol>"},{"location":"architecture/extension-registry-design/#import-flow-no-circular-dependencies","title":"Import Flow (No Circular Dependencies)","text":"<pre><code>Extension (base.py)\n    \u2191\nExecutableTask (interfaces/executable_task.py)\n    \u2191\nExtensionRegistry (extensions/registry.py)\n    \u2191 (uses ExecutorLike Protocol, not ExecutableTask)\nExecutorLike Protocol (extensions/protocol.py)\n    \u2191 (no dependencies)\n</code></pre>"},{"location":"architecture/extension-registry-design/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/extension-registry-design/#1-protocol-based-structural-typing","title":"1. Protocol-Based Structural Typing","text":"<p>Instead of importing <code>ExecutableTask</code> for type checks, we use <code>ExecutorLike</code> protocol:</p> <pre><code># \u2705 Good: No circular dependency\nfrom apflow.core.extensions.protocol import ExecutorLike\n\nif hasattr(extension, 'execute') and hasattr(extension, 'get_input_schema'):\n    # It's an executor-like object\n    ...\n\n# \u274c Bad: Would cause circular import\nfrom apflow.core.interfaces.executable_task import ExecutableTask\nif isinstance(extension, ExecutableTask):  # Circular!\n    ...\n</code></pre>"},{"location":"architecture/extension-registry-design/#2-runtime-structural-checks","title":"2. Runtime Structural Checks","text":"<p>We use <code>hasattr()</code> checks instead of <code>isinstance()</code> with Protocol:</p> <pre><code># Check if object has required methods (duck typing)\nif hasattr(extension, 'execute') and hasattr(extension, 'get_input_schema'):\n    # It implements ExecutorLike protocol\n    ...\n</code></pre> <p>This works because: - Python's duck typing allows any object with the right methods - Protocol defines the interface without requiring inheritance - No runtime dependency on ExecutableTask</p>"},{"location":"architecture/extension-registry-design/#3-type-hints-with-protocol","title":"3. Type Hints with Protocol","text":"<p>Type hints use Protocol for better IDE support:</p> <pre><code>from apflow.core.extensions.protocol import ExecutorFactory\n\n_factory_functions: Dict[str, ExecutorFactory] = {}\n</code></pre>"},{"location":"architecture/extension-registry-design/#benefits","title":"Benefits","text":"<ol> <li>No Circular Dependencies</li> <li><code>ExtensionRegistry</code> doesn't import <code>ExecutableTask</code></li> <li><code>ExecutableTask</code> can safely import <code>Extension</code></li> <li> <p>Clean import hierarchy</p> </li> <li> <p>Type Safety</p> </li> <li>Protocol provides type hints</li> <li>IDE autocomplete works</li> <li> <p>Static type checkers understand the structure</p> </li> <li> <p>Flexibility</p> </li> <li>Any class with <code>execute()</code> and <code>get_input_schema()</code> can be used</li> <li>Not limited to <code>ExecutableTask</code> inheritance</li> <li> <p>Supports duck typing</p> </li> <li> <p>Maintainability</p> </li> <li>Clear separation of concerns</li> <li>Easy to understand and modify</li> <li>Follows SOLID principles</li> </ol>"},{"location":"architecture/extension-registry-design/#example-usage","title":"Example Usage","text":"<pre><code># 1. Define executor (implements ExecutableTask which extends Extension)\nfrom apflow.core.base import BaseTask\n\nclass MyExecutor(BaseTask):\n    id = \"my_executor\"\n    name = \"My Executor\"\n\n    async def execute(self, inputs):\n        return {\"result\": \"done\"}\n\n    def get_input_schema(self):\n        return {}\n\n# 2. Register (automatically via decorator)\nfrom apflow.core.extensions.decorators import extension_register\n\n@extension_register()\nclass MyExecutor(BaseTask):\n    ...\n\n# 3. Use in registry (no ExecutableTask import needed)\nfrom apflow.core.extensions import get_registry\n\nregistry = get_registry()\nexecutor = registry.create_executor_instance(\"my_executor\", inputs={})\n# executor has execute() and get_input_schema() - that's all we need!\n</code></pre>"},{"location":"architecture/extension-registry-design/#migration-notes","title":"Migration Notes","text":""},{"location":"architecture/extension-registry-design/#before-circular-import-problem","title":"Before (Circular Import Problem)","text":"<pre><code># registry.py\nfrom apflow.core.interfaces.executable_task import ExecutableTask  # \u274c\n\nif isinstance(extension, ExecutableTask):  # Circular!\n    ...\n</code></pre>"},{"location":"architecture/extension-registry-design/#after-protocol-based-solution","title":"After (Protocol-Based Solution)","text":"<pre><code># protocol.py\nfrom typing import Protocol\n\nclass ExecutorLike(Protocol):\n    async def execute(self, inputs): ...\n    def get_input_schema(self): ...\n\n# registry.py\nfrom apflow.core.extensions.protocol import ExecutorLike  # \u2705\n\nif hasattr(extension, 'execute') and hasattr(extension, 'get_input_schema'):\n    # No circular dependency!\n    ...\n</code></pre>"},{"location":"architecture/extension-registry-design/#testing","title":"Testing","text":"<p>All imports work without circular dependencies:</p> <pre><code># \u2705 All these imports work simultaneously\nfrom apflow.core.extensions import ExtensionRegistry\nfrom apflow.core.interfaces import ExecutableTask\nfrom apflow.core.extensions.protocol import ExecutorLike\nfrom apflow.extensions.stdio import StdioExecutor\n</code></pre>"},{"location":"architecture/extension-registry-design/#future-extensions","title":"Future Extensions","text":"<p>This pattern can be extended for other extension types:</p> <pre><code>class StorageLike(Protocol):\n    def connect(self): ...\n    def disconnect(self): ...\n\nclass HookLike(Protocol):\n    def before_execute(self): ...\n    def after_execute(self): ...\n</code></pre>"},{"location":"architecture/overview/","title":"apflow Architecture Design Document","text":""},{"location":"architecture/overview/#core-positioning","title":"Core Positioning","text":"<p>The core of apflow is task orchestration and execution specifications</p> <p>This is a task orchestration framework library that provides: 1. Task Orchestration Specifications: Task tree construction, dependency management, priority scheduling 2. Task Execution Specifications: Unified execution interface supporting multiple task types 3. Supporting Features: Storage, unified external API interfaces</p> <p>apflow is a reusable framework library.</p>"},{"location":"architecture/overview/#architecture-layers","title":"Architecture Layers","text":"<p>The apflow architecture consists of four main layers:</p> <pre><code>flowchart TD\n    subgraph APILayer[\"Unified External API Interface Layer\"]\n        NativeAPI[\"Native API&lt;br/&gt;POST /tasks (JSON-RPC)\"]\n        A2AServer[\"A2A Protocol Adapter&lt;br/&gt;(execute/generate/cancel)\"]\n        MCPServer[\"MCP Protocol Adapter&lt;br/&gt;(15 tools)\"]\n        CLI[\"CLI Tools\"]\n    end\n\n    subgraph OrchestrationLayer[\"Task Orchestration Specification Layer (CORE)\"]\n        TaskManager[\"TaskManager&lt;br/&gt;- Task tree orchestration&lt;br/&gt;- Dependency management&lt;br/&gt;- Priority scheduling\"]\n        ExecutableTask[\"ExecutableTask&lt;br/&gt;- Task execution interface\"]\n    end\n\n    subgraph ExecutionLayer[\"Task Execution Layer\"]\n        CrewaiExecutor[\"CrewaiExecutor [crewai]&lt;br/&gt;- LLM task execution\"]\n        BatchCrewaiExecutor[\"BatchCrewaiExecutor [crewai]&lt;br/&gt;- Batch orchestration\"]\n        CustomTasks[\"Custom Tasks&lt;br/&gt;- External services&lt;br/&gt;- Automated tasks\"]\n    end\n\n    subgraph SupportLayer[\"Supporting Features Layer\"]\n        Storage[\"Storage&lt;br/&gt;- DuckDB/PostgreSQL&lt;br/&gt;- Task state persistence\"]\n        Streaming[\"Streaming&lt;br/&gt;- Real-time updates\"]\n    end\n\n    APILayer --&gt; OrchestrationLayer\n    OrchestrationLayer --&gt; ExecutionLayer\n    ExecutionLayer --&gt; SupportLayer\n\n    style APILayer fill:#e1f5ff\n    style OrchestrationLayer fill:#fff4e1\n    style ExecutionLayer fill:#e8f5e9\n    style SupportLayer fill:#f3e5f5</code></pre>"},{"location":"architecture/overview/#layer-details","title":"Layer Details","text":"<p>Unified External API Interface Layer - Native API (<code>POST /tasks</code>): Primary JSON-RPC interface for all 15 task operations - A2A Protocol Adapter (<code>POST /</code>): Agent-level actions (execute, generate, cancel) with streaming - MCP Protocol Adapter: All 15 operations as MCP tools for LLM integration - CLI Tools [cli]</p>"},{"location":"architecture/overview/#protocol-standard","title":"Protocol Standard","text":"<p>A2A (Agent-to-Agent) Protocol is the standard protocol adopted by apflow for agent communication.</p> <p>Why A2A Protocol? - Mature Standard: A2A Protocol is a mature, production-ready specification designed specifically for AI Agent systems - Complete Features: Provides streaming execution, task management, agent capability descriptions, and multiple transport methods - Well-Designed Abstraction: <code>RequestContext</code> encapsulates all request information, <code>EventQueue</code> unifies streaming updates - Protocol-Agnostic: Can be implemented over different transport layers (HTTP REST, SSE, WebSocket)</p> <p>A2A Protocol Components: - <code>AgentExecutor</code>: Interface for executing agent tasks - <code>RequestContext</code>: Encapsulates method, parameters, metadata, and message content - <code>EventQueue</code>: Unified interface for streaming updates and real-time progress notifications - <code>AgentCard</code> / <code>AgentSkill</code>: Agent capability description mechanism</p> <p>Task Orchestration Specification Layer (CORE) - TaskManager: Task tree orchestration, dependency management, priority scheduling - ExecutableTask: Task execution specification interface</p> <p>Task Execution Layer - CrewaiExecutor [crewai]: CrewAI (LLM) task execution - BatchCrewaiExecutor [crewai]: Batch task orchestration - Custom Tasks: Traditional external service calls, automated task services, etc.</p> <p>Supporting Features Layer - Storage: Task state persistence (DuckDB/PostgreSQL) - Streaming: Real-time progress updates</p>"},{"location":"architecture/overview/#module-organization","title":"Module Organization","text":""},{"location":"architecture/overview/#core-pip-install-apflow","title":"Core (pip install apflow)","text":"<p>Pure task orchestration framework - NO CrewAI dependency</p> <pre><code>execution/          # Task orchestration specifications (CORE)\n\u251c\u2500\u2500 task_manager.py      # TaskManager - core orchestration engine\n\u2514\u2500\u2500 streaming_callbacks.py  # Streaming support\n\ninterfaces/        # Core interfaces\n\u251c\u2500\u2500 executable_task.py  # ExecutableTask interface\n\u251c\u2500\u2500 base/               # Base class implementations\n\u2502   \u2514\u2500\u2500 base_task.py   # BaseTask base class\n\u2514\u2500\u2500 storage.py          # TaskStorage interface\n\nstorage/           # Storage implementation\n\u251c\u2500\u2500 factory.py         # create_storage() function\n\u251c\u2500\u2500 sqlalchemy/        # SQLAlchemy implementation\n\u2514\u2500\u2500 dialects/          # Database dialects (DuckDB/PostgreSQL)\n\nutils/             # Utility functions\n</code></pre>"},{"location":"architecture/overview/#optional-features","title":"Optional Features","text":""},{"location":"architecture/overview/#crewai-crewai-llm-task-support","title":"[crewai] - CrewAI LLM Task Support","text":"<pre><code>extensions/crewai/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 crewai_executor.py     # CrewaiExecutor - CrewAI wrapper\n\u251c\u2500\u2500 batch_crewai_executor.py    # BatchCrewaiExecutor - batch execution of multiple crews\n\u2514\u2500\u2500 types.py            # CrewaiExecutorState, BatchState\n</code></pre> <p>Installation: <code>pip install apflow[crewai]</code></p> <p>Includes: - CrewaiExecutor for LLM-based agent crews - BatchCrewaiExecutor for atomic batch execution of multiple crews - Related type definitions</p> <p>Note: BatchCrewaiExecutor is part of [crewai] because it's specifically designed for batching multiple CrewAI crews together.</p>"},{"location":"architecture/overview/#http_executor-httpremote-api-task-execution","title":"[http_executor] - HTTP/Remote API Task Execution","text":"<pre><code>extensions/http_executor/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 http_executor.py    # HTTPExecutor - ExecutableTask implementation for HTTP calls\n\u251c\u2500\u2500 client.py           # HTTP client with retry, timeout, auth support\n\u2514\u2500\u2500 types.py            # HTTPExecutorState, RequestConfig, ResponseConfig\n</code></pre> <p>Installation: <code>pip install apflow[http]</code></p> <p>Purpose: Execute tasks by calling remote HTTP/HTTPS APIs.</p> <p>Features: - HTTP/HTTPS request execution - Retry logic with exponential backoff - Authentication support (API keys, OAuth, etc.) - Request/response validation - Timeout handling</p> <p>Use Case: Tasks that need to call external REST APIs, webhooks, or remote services.</p> <p>Note: For examples and learning templates, see the test cases in <code>tests/integration/</code> and <code>tests/extensions/</code>. Test cases serve as comprehensive examples demonstrating real-world usage patterns.</p>"},{"location":"architecture/overview/#a2a-a2a-protocol-server","title":"[a2a] - A2A Protocol Server","text":"<pre><code>api/                   # Unified API service layer (supports multiple protocols)\n\u251c\u2500\u2500 __init__.py        # Unified entry point, backward compatible\n\u251c\u2500\u2500 main.py            # CLI entry point (main() function and uvicorn server)\n\u251c\u2500\u2500 capabilities.py    # Unified operations registry (single source of truth for all 15 operations)\n\u251c\u2500\u2500 extensions.py      # Extension management (initialize_extensions, EXTENSION_CONFIG)\n\u251c\u2500\u2500 protocols.py       # Protocol management (get_protocol_from_env, check_protocol_dependency)\n\u251c\u2500\u2500 app.py             # Application creation (create_app_by_protocol, create_a2a_server, create_mcp_server)\n\u251c\u2500\u2500 a2a/               # A2A Protocol Adapter (agent-level actions only)\n\u2502   \u251c\u2500\u2500 __init__.py    # A2A module exports\n\u2502   \u251c\u2500\u2500 server.py      # A2A server creation\n\u2502   \u251c\u2500\u2500 agent_executor.py      # A2A agent executor (execute/generate/cancel)\n\u2502   \u251c\u2500\u2500 task_routes_adapter.py # Lightweight adapter (method extraction, A2A Task conversion)\n\u2502   \u251c\u2500\u2500 custom_starlette_app.py # Custom A2A Starlette application\n\u2502   \u2514\u2500\u2500 event_queue_bridge.py   # Event queue bridge\n\u251c\u2500\u2500 mcp/               # MCP Protocol Adapter (all 15 operations as tools)\n\u2502   \u251c\u2500\u2500 __init__.py    # MCP module exports\n\u2502   \u251c\u2500\u2500 server.py      # MCP server creation\n\u2502   \u251c\u2500\u2500 adapter.py     # TaskRoutes adapter for MCP (registry-driven routing)\n\u2502   \u251c\u2500\u2500 tools.py       # MCP tools registry (auto-generated from capabilities)\n\u2502   \u2514\u2500\u2500 ...            # Other MCP components\n\u251c\u2500\u2500 routes/            # Protocol-agnostic route handlers (shared core)\n\u2502   \u251c\u2500\u2500 __init__.py    # Route handlers exports\n\u2502   \u251c\u2500\u2500 base.py        # BaseRouteHandler - shared functionality\n\u2502   \u251c\u2500\u2500 tasks.py       # TaskRoutes - all 15 task handler methods\n\u2502   \u2514\u2500\u2500 system.py      # SystemRoutes - system operation handlers\n</code></pre>"},{"location":"architecture/overview/#cli-cli-tools","title":"[cli] - CLI Tools","text":"<pre><code>cli/                   # Command-line interface\n\u251c\u2500\u2500 main.py            # CLI entry point\n\u2514\u2500\u2500 commands/          # CLI commands\n</code></pre>"},{"location":"architecture/overview/#installation-strategy","title":"Installation Strategy","text":""},{"location":"architecture/overview/#core-only","title":"Core Only","text":"<pre><code>pip install apflow\n</code></pre> <p>Includes: - Task orchestration specifications (TaskManager) - Core interfaces (ExecutableTask, BaseTask, TaskStorage) - Storage (DuckDB default) - NO CrewAI dependency</p> <p>Use case: Users who want pure orchestration framework with custom task implementations.</p>"},{"location":"architecture/overview/#with-crewai-support","title":"With CrewAI Support","text":"<pre><code>pip install apflow[crewai]\n</code></pre> <p>Adds: - CrewaiExecutor for LLM-based tasks via CrewAI - BatchCrewaiExecutor for atomic batch execution of multiple crews</p> <p>Use case: Users who want LLM agent capabilities and/or batch execution of multiple crews.</p>"},{"location":"architecture/overview/#full-installation","title":"Full Installation","text":"<pre><code>pip install apflow[all]\n</code></pre> <p>Includes: Core + crewai + batch + api + cli + examples + postgres</p>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-task-orchestration-specifications-core","title":"1. Task Orchestration Specifications (Core)","text":""},{"location":"architecture/overview/#taskmanager-executiontask_managerpy","title":"TaskManager (<code>execution/task_manager.py</code>)","text":"<ul> <li>Core Responsibility: Task orchestration and execution specifications</li> <li>Features:</li> <li>Task tree construction and management (TaskTreeNode)</li> <li>Dependency resolution and execution order control</li> <li>Priority scheduling</li> <li>Task state management (pending, in_progress, completed, failed)</li> <li>Task lifecycle management (create, execute, complete, failure handling)</li> </ul>"},{"location":"architecture/overview/#executabletask-interfacesexecutable_taskpy","title":"ExecutableTask (<code>interfaces/executable_task.py</code>)","text":"<ul> <li>Responsibility: Define task execution specification interface</li> <li>Implementations:</li> <li><code>CrewaiExecutor</code> [crewai]: LLM-based tasks (via CrewAI)</li> <li>Custom tasks: Traditional external service calls, automated task services, etc.</li> </ul>"},{"location":"architecture/overview/#2-task-execution-layer","title":"2. Task Execution Layer","text":""},{"location":"architecture/overview/#crewaiexecutor-extensionscrewaicrewai_executorpy-crewai","title":"CrewaiExecutor (<code>extensions/crewai/crewai_executor.py</code>) [crewai]","text":"<ul> <li>Responsibility: CrewAI (LLM) task execution - ExecutableTask implementation</li> <li>Features:</li> <li>Wraps CrewAI Crew functionality</li> <li>Supports LLM-based agent collaboration</li> <li>Implements ExecutableTask interface</li> <li>Type: Task executor (one of many possible implementations)</li> </ul>"},{"location":"architecture/overview/#batchcrewaiexecutor-extensionscrewaibatch_crewai_executorpy-crewai","title":"BatchCrewaiExecutor (<code>extensions/crewai/batch_crewai_executor.py</code>) [crewai]","text":"<ul> <li>Responsibility: Batch task orchestration for multiple crews (simple merging)</li> <li>Features:</li> <li>Atomic execution of multiple crews</li> <li>Result merging</li> <li>Not an ExecutableTask (it's a container, not a task)</li> <li>Location: Part of [crewai] because it's specifically for batching CrewAI crews together</li> </ul>"},{"location":"architecture/overview/#httpexecutor-extensionshttp_executorhttp_executorpy-http","title":"HTTPExecutor (<code>extensions/http_executor/http_executor.py</code>) [http]","text":"<ul> <li>Responsibility: Remote HTTP/API call task execution - ExecutableTask implementation</li> <li>Features:</li> <li>HTTP/HTTPS request execution</li> <li>Retry logic, timeout handling</li> <li>Authentication support</li> <li>Type: Task executor</li> <li>Use Case: Tasks that call external REST APIs or remote services</li> </ul>"},{"location":"architecture/overview/#custom-tasks-core","title":"Custom Tasks (Core)","text":"<ul> <li>Types:</li> <li>User-defined implementations of ExecutableTask</li> <li>Can be any task type (database operations, file processing, etc.)</li> <li>Implementation: Inherit from <code>ExecutableTask</code> or <code>BaseTask</code></li> <li>Location: Users create these in their own codebase</li> </ul>"},{"location":"architecture/overview/#built-in-executors-features","title":"Built-in Executors (Features)","text":"<ul> <li>CrewaiExecutor [crewai]: LLM-based tasks via CrewAI</li> <li>HTTPExecutor [http]: Remote API calls via HTTP</li> <li>Future executors: database executor, queue executor, etc.</li> <li>Location: <code>extensions/</code> directory</li> </ul>"},{"location":"architecture/overview/#3-supporting-features","title":"3. Supporting Features","text":""},{"location":"architecture/overview/#storage-storage","title":"Storage (<code>storage/</code>)","text":"<ul> <li>Responsibility: Task state persistence</li> <li>Implementations:</li> <li>DuckDB (default, embedded, zero-config)</li> <li>PostgreSQL (optional, production environment)</li> <li>Features:</li> <li>Task creation, querying, updating</li> <li>Task tree state management</li> </ul> <p>TaskModel Design and A2A Protocol Integration:</p> <p>The storage layer uses <code>TaskModel</code> (SQLAlchemy model) to persist task definitions and execution results. A key design decision is the separation between <code>TaskModel</code> (task definition) and A2A Protocol's <code>Task</code> (execution instance).</p> <p>Conceptual Separation:</p> Component Nature Purpose Lifecycle TaskModel Task definition (static) Task orchestration, dependency management, task tree structure Persistent (can have multiple executions) A2A Protocol Task Execution instance (dynamic) LLM message context management, execution tracking Single execution lifecycle <p>Key Understanding:</p> <p>A2A Protocol's <code>Task</code> is primarily designed for LLM message context management. It represents an execution instance with: - <code>Task.history</code>: LLM conversation history - <code>Task.artifacts</code>: LLM-generated results - <code>Task.status.message</code>: LLM response messages</p> <p>TaskModel, on the other hand, focuses on task orchestration: - Task definition metadata (name, dependencies, priority, schemas) - Task tree structure (parent_id, children) - Latest execution results (extracted from A2A Task.artifacts) - Execution status and progress</p> <p>Mapping Relationship:</p> <pre><code>TaskModel (Task Definition)          A2A Protocol Task (Execution Instance)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nTaskModel.id          \u2192  Task.context_id    (task definition ID = context ID)\nTaskModel.status      \u2192  TaskStatus.state   (status mapping)\nTaskModel.result      \u2192  Task.artifacts     (execution result)\nTaskModel.error       \u2192  TaskStatus.message (error message)\nTaskModel.user_id     \u2192  Task.metadata[\"user_id\"] (optional user identifier)\n\nTask.id               \u2192  Execution instance ID (A2A Protocol internal, auto-generated)\nTask.history           \u2192  Not stored in TaskModel (LLM conversation history, execution-level)\n</code></pre> <p>Design Decisions:</p> <ol> <li>TaskModel does NOT store execution-level fields:</li> <li>\u274c <code>context_id</code>: Execution-level concept (TaskModel.id serves this purpose)</li> <li>\u274c <code>artifacts</code>: Execution instance field (extracted to TaskModel.result)</li> <li>\u274c <code>history</code>: Execution instance field (LLM conversation history, managed by A2A Protocol)</li> <li>\u274c <code>metadata</code>: Execution instance field (orchestration info stored in TaskModel fields)</li> <li> <p>\u274c <code>kind</code>: A2A Protocol-specific field</p> </li> <li> <p>One-to-Many Relationship:</p> </li> <li>One <code>TaskModel</code> can have multiple <code>Task</code> execution instances</li> <li>Each execution creates a new A2A Protocol <code>Task</code> with a unique execution instance ID</li> <li> <p><code>TaskModel.id</code> (task definition ID) maps to <code>Task.context_id</code></p> </li> <li> <p>Table Name:</p> </li> <li>Default table name: <code>apflow_tasks</code> (prefixed to distinguish from A2A Protocol's <code>tasks</code> table)</li> <li>Configurable via <code>APFLOW_TASK_TABLE_NAME</code> environment variable</li> <li> <p>See Configuration for details</p> </li> <li> <p>Storage Bridge:</p> </li> <li>Uses A2A SDK's <code>InMemoryTaskStore</code> for A2A Protocol task execution instances</li> <li>TaskModel persistence is handled by <code>TaskRepository</code> (separate from A2A TaskStore)</li> <li>Converts between <code>TaskModel</code> (task definition) and A2A Protocol <code>Task</code> (execution instance)</li> <li>Updates <code>TaskModel</code> with execution results from A2A Protocol <code>Task</code></li> </ol>"},{"location":"architecture/overview/#api-api-a2a","title":"API (<code>api/</code>) [a2a]","text":"<ul> <li>Responsibility: Unified external API service layer supporting multiple network protocols</li> <li>Three-Layer Architecture:</li> <li>TaskRoutes (<code>api/routes/tasks.py</code>): Protocol-agnostic core. All task operations are handler methods on TaskRoutes, shared by all protocols.</li> <li>Native API (<code>POST /tasks</code>): Primary programmatic interface using JSON-RPC. Supports all 15 task methods. Discovery available at <code>GET /tasks/methods</code>.</li> <li>Protocol Adapters: A2A and MCP as interoperability layers.<ul> <li>A2A (<code>api/a2a/</code>): Agent-level actions only (<code>tasks.execute</code>, <code>tasks.generate</code>, <code>tasks.cancel</code>) via <code>POST /</code>. CRUD operations return an error directing to <code>POST /tasks</code>.</li> <li>MCP (<code>api/mcp/</code>): All 15 operations exposed as MCP tools for LLM tool-use.</li> </ul> </li> <li>Capabilities Registry (<code>api/capabilities.py</code>): Single source of truth defining all 15 operations. Generates A2A AgentSkills and MCP tool definitions from the same registry.</li> <li>Method Discovery: <code>GET /tasks/methods</code> returns all available methods grouped by category with input schemas.</li> </ul>"},{"location":"architecture/overview/#streaming-executionstreaming_callbackspy","title":"Streaming (<code>execution/streaming_callbacks.py</code>)","text":"<ul> <li>Responsibility: Real-time progress updates</li> <li>Features: Real-time task execution state updates</li> </ul>"},{"location":"architecture/overview/#task-type-support","title":"Task Type Support","text":""},{"location":"architecture/overview/#1-llm-tasks-crewai-crewai","title":"1. LLM Tasks (CrewAI) [crewai]","text":"<pre><code># Requires: pip install apflow[crewai]\nfrom apflow.extensions.crewai import CrewaiExecutor\n\ncrew = CrewaiExecutor(\n    agents=[{\"role\": \"Analyst\", \"goal\": \"Analyze data\"}],\n    tasks=[{\"description\": \"Analyze input\", \"agent\": \"Analyst\"}]\n)\nresult = await crew.execute(inputs={...})\n</code></pre>"},{"location":"architecture/overview/#2-traditional-external-service-calls","title":"2. Traditional External Service Calls","text":"<pre><code>from apflow.core.interfaces.plugin import ExecutableTask\n\nclass APICallTask(ExecutableTask):\n    async def execute(self, inputs):\n        # Call external API\n        response = await http_client.post(url, data=inputs)\n        return response.json()\n</code></pre>"},{"location":"architecture/overview/#3-automated-task-services","title":"3. Automated Task Services","text":"<pre><code>class ScheduledTask(ExecutableTask):\n    async def execute(self, inputs):\n        # Execute scheduled task logic\n        return {\"status\": \"completed\"}\n</code></pre>"},{"location":"architecture/overview/#task-orchestration-patterns","title":"Task Orchestration Patterns","text":""},{"location":"architecture/overview/#simple-batch-orchestration-batchcrewaiexecutor-crewai","title":"Simple Batch Orchestration (BatchCrewaiExecutor) [crewai]","text":"<ul> <li>Multiple crews execute sequentially, results are merged</li> <li>Atomic operation: failure of any crew causes entire batch to fail</li> <li>Part of [crewai] extra (designed for batching CrewAI crews)</li> </ul>"},{"location":"architecture/overview/#complex-task-tree-orchestration-taskmanager-core","title":"Complex Task Tree Orchestration (TaskManager) [core]","text":"<ul> <li>Supports dependencies</li> <li>Supports priorities</li> <li>Supports hierarchical task tree structure</li> <li>Automatic scheduling and execution</li> <li>No external dependencies</li> </ul>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":"<ol> <li>Clear Core: Task orchestration and execution specifications are the core</li> <li>Pure Core: Core has no external LLM/AI dependencies (CrewAI optional)</li> <li>Unified Interface: All task types unified through ExecutableTask interface</li> <li>Executor Pattern: Different executors (CrewAI, HTTP, etc.) are separate features</li> <li>Flexible Orchestration: Supports simple batch to complex task tree</li> <li>Optional Storage: Provides persistence but configurable</li> <li>Unified API: Provides unified external interface using A2A Protocol (standard protocol)</li> <li>Modular Installation: Users install only what they need</li> <li>Learning Resources: Test cases in <code>tests/integration/</code> and <code>tests/extensions/</code> serve as comprehensive examples</li> </ol>"},{"location":"architecture/task-manager/","title":"Architecture and Development Design Document (Focus on TaskManager)","text":"<p>This document is a focused summary. For the full architecture reference, see <code>docs/architecture/</code>.</p>"},{"location":"architecture/task-manager/#1-project-positioning-and-overall-architecture","title":"1. Project Positioning and Overall Architecture","text":"<p>Core Positioning: apflow is a framework library centered on the Task Orchestration &amp; Execution Specification. Its core capabilities focus on task tree orchestration, dependency management, priority scheduling, and unified execution interfaces. All other components\u2014API, CLI, storage, and extensions\u2014serve as peripheral support layers.</p>"},{"location":"architecture/task-manager/#layered-architecture-simplified-view","title":"Layered Architecture (Simplified View)","text":"<ul> <li>API / External Interface Layer</li> <li>Native API (JSON-RPC <code>POST /tasks</code>)</li> <li>A2A Adapter (execute / generate / cancel)</li> <li>MCP Adapter (15 auto-generated tools)</li> <li> <p>CLI</p> </li> <li> <p>Orchestration Core Layer (Core)</p> </li> <li><code>TaskManager</code>: task tree orchestration, dependency checking, priority scheduling, lifecycle management</li> <li> <p><code>TaskExecutor</code>: execution entrypoint, session management, concurrency protection, task tree distribution</p> </li> <li> <p>Execution Layer (Extension Ecosystem)</p> </li> <li>Various Executors (REST / WebSocket / gRPC / SSH / Docker / CrewAI / MCP / LLM, etc.)</li> <li> <p>Unified discovery and instantiation via Extension Registry</p> </li> <li> <p>Support Layer</p> </li> <li>Storage (DuckDB / PostgreSQL)</li> <li>Streaming (real-time progress and result push)</li> <li>Config / Hooks / Scheduler</li> </ul> <p>For detailed architecture, refer to: <code>docs/architecture/overview.md</code>.</p>"},{"location":"architecture/task-manager/#2-key-data-models-and-core-concepts","title":"2. Key Data Models and Core Concepts","text":""},{"location":"architecture/task-manager/#21-taskmodel-orchestration-definition","title":"2.1 TaskModel (Orchestration Definition)","text":"<ul> <li>Task definition, dependencies, priority, task tree structure, latest execution result</li> <li>Stored in <code>apflow_tasks</code> (configurable via <code>APFLOW_TASK_TABLE_NAME</code>)</li> </ul>"},{"location":"architecture/task-manager/#22-a2a-task-execution-instance","title":"2.2 A2A Task (Execution Instance)","text":"<ul> <li>LLM-oriented execution context (history / artifacts / status)</li> <li>Separated from TaskModel; TaskModel only retains the latest execution result</li> </ul> <p>For details, see: TaskModel and A2A Task mapping in <code>docs/architecture/overview.md</code>.</p>"},{"location":"architecture/task-manager/#3-core-of-task-orchestration-taskmanager-focus","title":"3. Core of Task Orchestration: TaskManager (Focus)","text":"<p>File: <code>src/apflow/core/execution/task_manager.py</code></p>"},{"location":"architecture/task-manager/#31-core-responsibilities","title":"3.1 Core Responsibilities","text":"<ul> <li>Recursive scheduling of task trees</li> <li>Dependency checking and input merging</li> <li>Task execution state transitions (pending \u2192 in_progress \u2192 completed/failed/cancelled)</li> <li>Pre/post hook execution and context management</li> <li>Streaming status push</li> <li>Task cancellation and executor cleanup</li> <li>Failed task retry and dependency re-execution</li> </ul>"},{"location":"architecture/task-manager/#32-task-tree-execution-flow-summary","title":"3.2 Task Tree Execution Flow (Summary)","text":"<ol> <li>Set Hook Context (<code>set_hook_context</code>)</li> <li>Execute tree hooks: <code>on_tree_created</code> / <code>on_tree_started</code></li> <li>Recursively execute task tree:</li> <li>Group by priority</li> <li>Parallel execution of nodes with satisfied dependencies at the same priority</li> <li>Task execution:</li> <li>Dependency resolution \u2192 pre-hooks \u2192 executor \u2192 post-hooks</li> <li>Aggregate tree status and trigger <code>on_tree_completed</code> / <code>on_tree_failed</code></li> <li>Clean up Hook Context</li> </ol> <p>For detailed lifecycle, refer to: <code>docs/architecture/task-tree-lifecycle.md</code>.</p>"},{"location":"architecture/task-manager/#33-re-execution-and-failure-recovery-strategy","title":"3.3 Re-execution and Failure Recovery Strategy","text":"<ul> <li><code>failed</code> tasks are always re-executed</li> <li><code>completed</code> tasks are re-executed only if marked for re-execution</li> <li><code>pending</code> tasks executed normally</li> <li><code>in_progress</code> tasks skipped by default</li> </ul> <p><code>TaskExecutor</code> marks the \"set of tasks requiring re-execution\" at the tree level; <code>TaskManager</code> filters executable nodes based on this set during distribution.</p>"},{"location":"architecture/task-manager/#34-dependency-resolution-and-input-merging","title":"3.4 Dependency Resolution and Input Merging","text":"<ul> <li>Dependency checking handled by <code>are_dependencies_satisfied()</code></li> <li>Dependency result merging:</li> <li>Supports schema-based mapping (input/output schema)</li> <li>Supports aggregate tasks (<code>aggregate_results_executor</code>)</li> <li>Falls back to <code>dep_id</code> namespace by default for complex outputs</li> </ul>"},{"location":"architecture/task-manager/#35-executor-loading-and-permission-control","title":"3.5 Executor Loading and Permission Control","text":"<ul> <li>Executors retrieved by executor_id via <code>ExtensionRegistry</code></li> <li>Supports restricting available executors via <code>APFLOW_EXTENSIONS</code> / <code>APFLOW_EXTENSIONS_IDS</code></li> <li>Returns clear errors for unauthorized or missing executors</li> </ul>"},{"location":"architecture/task-manager/#36-hook-system","title":"3.6 Hook System","text":"<ul> <li>Pre hooks: Before execution; can modify <code>task.inputs</code> (auto-persisted)</li> <li>Post hooks: After execution; receive inputs + result</li> <li>Task-tree hooks: Tree lifecycle (created/started/completed/failed)</li> <li>Hooks share the same DB session at runtime (ContextVar)</li> </ul>"},{"location":"architecture/task-manager/#37-streaming-cancellation","title":"3.7 Streaming &amp; Cancellation","text":"<ul> <li>Streaming: Progress and final status pushed via <code>StreamingCallbacks</code></li> <li>Cancellation: Supports <code>executor.cancel()</code>, with merged <code>token_usage</code> / <code>partial_result</code></li> <li>Executor instances stored in <code>_executor_instances</code> for cancellation and cleanup</li> </ul>"},{"location":"architecture/task-manager/#38-demo-mode","title":"3.8 Demo Mode","text":"<ul> <li>Returns demo result directly when <code>use_demo=True</code>; supports simulated sleep (<code>APFLOW_DEMO_SLEEP_SCALE</code>)</li> </ul>"},{"location":"architecture/task-manager/#4-task-execution-entry-taskexecutor","title":"4. Task Execution Entry: TaskExecutor","text":"<p>File: <code>src/apflow/core/execution/task_executor.py</code></p> <p>Main Responsibilities: - Entrypoint for task tree execution (API/CLI invocation) - Thread-safe \"concurrency protection for same-root task trees\" (TaskTracker) - Distribution to TaskManager - Task creation and task tree construction (TaskCreator) - Task subtree execution (dependencies auto-resolved when task_id is specified)</p>"},{"location":"architecture/task-manager/#5-storage-layer-and-repository","title":"5. Storage Layer and Repository","text":"<p>File: <code>src/apflow/core/storage/sqlalchemy/task_repository.py</code></p> <p>Key Features: - Single access entry (TaskRepository) - Supports custom TaskModel (<code>APFLOW_TASK_MODEL_CLASS</code>) - Supports fast query of full tree by <code>task_tree_id</code> - JSON field change detection (<code>flag_modified</code>) - Support for scheduling-related fields (cron + next_run_at)</p>"},{"location":"architecture/task-manager/#6-extension-mechanism-extension-registry","title":"6. Extension Mechanism (Extension Registry)","text":"<p>Files: <code>src/apflow/core/extensions/registry.py</code> + <code>manager.py</code></p> <p>Core Capabilities: - Extension registration and lookup (ID / category / type) - Lazy loading of Executors (avoids slowing down CLI startup) - Uses Protocol to avoid circular dependencies - Supports override</p> <p>Security Control: - <code>APFLOW_EXTENSIONS</code> / <code>APFLOW_EXTENSIONS_IDS</code> allow restricting executor set</p>"},{"location":"architecture/task-manager/#7-dependencies-and-optional-features","title":"7. Dependencies and Optional Features","text":"<p>Basic Dependencies (Core) - <code>sqlalchemy</code> / <code>duckdb-engine</code> / <code>alembic</code> / <code>pydantic</code></p> <p>Optional Extensions (Excerpt) - <code>a2a</code>: FastAPI / Uvicorn / a2a-sdk / httpx / websockets - <code>cli</code>: click / typer / rich / python-dotenv - <code>postgres</code>: asyncpg / psycopg2-binary - <code>crewai</code>: crewai / litellm / anthropic - <code>docker</code> / <code>grpc</code> / <code>ssh</code> / <code>mcp</code> / <code>llm</code> / <code>email</code></p> <p>For details, see <code>pyproject.toml</code>.</p>"},{"location":"architecture/task-manager/#8-functional-features-summary","title":"8. Functional Features (Summary)","text":"<ul> <li>\u2705 Task tree orchestration (parent-child structure + dependency control)</li> <li>\u2705 Priority scheduling (parallel at same priority)</li> <li>\u2705 Failure re-execution and dependency propagation</li> <li>\u2705 Schema-based dependency result mapping</li> <li>\u2705 Hooks + ContextVar (unified DB session)</li> <li>\u2705 Streaming execution progress</li> <li>\u2705 Cancelable tasks</li> <li>\u2705 Extension mechanism (Executor / Hook / Storage)</li> <li>\u2705 Multi-protocol API (A2A / MCP / JSON-RPC)</li> <li>\u2705 Built-in scheduling and external scheduler support</li> </ul>"},{"location":"architecture/task-manager/#9-development-recommendations-and-extension-points","title":"9. Development Recommendations and Extension Points","text":"<ul> <li>New executor: Register via extension decorator; ensure <code>execute()</code> and schema methods are available</li> <li>Custom TaskModel: Set <code>APFLOW_TASK_MODEL_CLASS</code></li> <li>Extend hooks: Register pre/post/tree hooks; avoid long-running logic</li> <li>Distributed extension (Future): Based on Distributed Core design in roadmap</li> </ul>"},{"location":"architecture/task-manager/#references","title":"References","text":"<ul> <li><code>docs/architecture/overview.md</code></li> <li><code>docs/architecture/task-tree-lifecycle.md</code></li> <li><code>docs/architecture/directory-structure.md</code></li> <li><code>docs/architecture/extension-registry-design.md</code></li> <li><code>docs/guides/task-orchestration.md</code></li> <li><code>pyproject.toml</code></li> </ul>"},{"location":"architecture/task-tree-lifecycle/","title":"Task Tree Execution Lifecycle","text":"<p>This document describes the complete lifecycle of task tree execution, including the database session context and hook execution lifecycle.</p>"},{"location":"architecture/task-tree-lifecycle/#overview","title":"Overview","text":"<p>Task tree execution involves multiple layers of coordination between TaskExecutor, TaskManager, and hooks. Understanding these lifecycles is crucial for: - Implementing hooks that access the database - Debugging execution issues - Ensuring proper resource cleanup - Maintaining data consistency</p>"},{"location":"architecture/task-tree-lifecycle/#1-task-tree-execution-lifecycle","title":"1. Task Tree Execution Lifecycle","text":""},{"location":"architecture/task-tree-lifecycle/#11-entry-point-taskexecutorexecute_task_tree","title":"1.1 Entry Point: TaskExecutor.execute_task_tree()","text":"<p>File: <code>src/apflow/core/execution/task_executor.py</code> (lines 151-250)</p> <pre><code>async def execute_task_tree(\n    self,\n    task_tree: TaskTreeNode,\n    root_task_id: str,\n    use_streaming: bool = False,\n    streaming_callbacks_context: Optional[Any] = None,\n    use_demo: bool = False,\n    db_session: Optional[Union[Session, AsyncSession]] = None\n) -&gt; Dict[str, Any]:\n</code></pre> <p>Key Steps:</p> <ol> <li>Session Creation (if not provided)    <pre><code>if db_session is None:\n    async with create_pooled_session() as session:\n        return await self.execute_task_tree(...)  # Recursive call with session\n</code></pre></li> <li>Uses <code>create_pooled_session()</code> for automatic session lifecycle management</li> <li>Session is automatically committed/rolled back by the context manager</li> <li> <p>Ensures all operations within the task tree share the same session</p> </li> <li> <p>Concurrent Execution Protection <pre><code>if self.is_task_running(root_task_id):\n    return {\"status\": \"already_running\", ...}\n</code></pre></p> </li> <li>Prevents multiple concurrent executions of the same task tree</li> <li>Uses singleton <code>TaskTracker</code> for execution state tracking</li> <li> <p>Returns early without starting execution</p> </li> <li> <p>Execution Tracking <pre><code>await self.start_task_tracking(root_task_id)\ntry:\n    # ... execution ...\nfinally:\n    await self.end_task_tracking(root_task_id)\n</code></pre></p> </li> <li>Registers task tree as \"running\" in TaskTracker</li> <li> <p>Guarantees cleanup in finally block even on exceptions</p> </li> <li> <p>TaskManager Creation <pre><code>task_manager = TaskManager(\n    db_session,\n    root_task_id=root_task_id,\n    pre_hooks=self.pre_hooks,\n    post_hooks=self.post_hooks,\n    executor_instances=self._executor_instances,\n    use_demo=use_demo\n)\n</code></pre></p> </li> <li>Passes shared session to TaskManager</li> <li>Passes hook lists (registered at application startup)</li> <li> <p>Passes shared executor_instances dict for cancellation support</p> </li> <li> <p>Task Tree Distribution <pre><code>if use_streaming and streaming_callbacks_context:\n    await task_manager.distribute_task_tree_with_streaming(task_tree, use_callback=True)\nelse:\n    await task_manager.distribute_task_tree(task_tree, use_callback=True)\n</code></pre></p> </li> </ol>"},{"location":"architecture/task-tree-lifecycle/#12-task-manager-distribute_task_tree","title":"1.2 Task Manager: distribute_task_tree()","text":"<p>File: <code>src/apflow/core/execution/task_manager.py</code> (lines 268-318)</p> <p>Key Steps:</p> <ol> <li>Hook Context Setup (lines 283-285)    <pre><code>set_hook_context(self.task_repository)\n</code></pre></li> <li>CRITICAL: Sets up ContextVar for all hooks to access DB</li> <li>Must be called BEFORE any hooks are executed</li> <li> <p>Thread-safe context isolation (ContextVar not thread-local)</p> </li> <li> <p>Try Block - Tree Lifecycle Hooks (lines 286-311)    <pre><code>try:\n    # 1. on_tree_created hook\n    await self._call_task_tree_hooks(\"on_tree_created\", root_task, task_tree)\n\n    # 2. on_tree_started hook\n    await self._call_task_tree_hooks(\"on_tree_started\", root_task)\n\n    try:\n        # 3. Execute task tree recursively\n        await self._execute_task_tree_recursive(task_tree, use_callback)\n\n        # 4. Check final status and call completion hooks\n        final_status = task_tree.calculate_status()\n        if final_status == \"completed\":\n            await self._call_task_tree_hooks(\"on_tree_completed\", root_task, \"completed\")\n        else:\n            await self._call_task_tree_hooks(\"on_tree_failed\", root_task, f\"Tree finished with status: {final_status}\")\n\n    except Exception as e:\n        # 5. on_tree_failed hook on exception\n        await self._call_task_tree_hooks(\"on_tree_failed\", root_task, str(e))\n        raise\n</code></pre></p> </li> <li> <p>Finally Block - Hook Context Cleanup (lines 312-313)    <pre><code>finally:\n    clear_hook_context()\n</code></pre></p> </li> <li>GUARANTEED: Always executes even on exceptions</li> <li>Cleans up ContextVar to prevent context leakage</li> <li>Critical for proper resource cleanup</li> </ol>"},{"location":"architecture/task-tree-lifecycle/#13-recursive-task-tree-execution","title":"1.3 Recursive Task Tree Execution","text":"<p>File: <code>src/apflow/core/execution/task_manager.py</code> (lines 415-605)</p> <p>Key Steps:</p> <ol> <li> <p>Priority-Based Execution <pre><code># Group children by priority\npriority_groups: Dict[int, List[TaskTreeNode]] = {}\nself._add_children_to_priority_groups(node, priority_groups)\n\n# Execute by priority (ascending order)\nsorted_priorities = sorted(priority_groups.keys())\nfor priority in sorted_priorities:\n    children_with_same_priority = priority_groups[priority]\n    # ... execute children in parallel ...\n</code></pre></p> </li> <li> <p>Parallel Execution Within Same Priority <pre><code>async def execute_child_and_children(child_node):\n    await self._execute_single_task(child_node.task, use_callback)\n    await self._execute_task_tree_recursive(child_node, use_callback)\n\nparallel_tasks = [execute_child_and_children(child_node) for child_node in ready_tasks]\nawait asyncio.gather(*parallel_tasks)\n</code></pre></p> </li> <li> <p>Dependency Resolution <pre><code>deps_satisfied = await are_dependencies_satisfied(\n    child_task, self.task_repository, self._tasks_to_reexecute\n)\nif deps_satisfied:\n    ready_tasks.append(child_node)\nelse:\n    waiting_tasks.append(child_node)\n</code></pre></p> </li> </ol>"},{"location":"architecture/task-tree-lifecycle/#14-single-task-execution","title":"1.4 Single Task Execution","text":"<p>File: <code>src/apflow/core/execution/task_manager.py</code> (lines 627-897)</p> <p>Key Steps:</p> <ol> <li> <p>Status Checks and Updates (lines 627-735)    <pre><code># Refresh task from DB to get latest status\ntask = await self.task_repository.get_task_by_id(task_id)\n\n# Update to in_progress\nawait self.task_repository.update_task(\n    task_id=task_id,\n    status=\"in_progress\",\n    started_at=datetime.now(timezone.utc)\n)\n</code></pre></p> </li> <li> <p>Dependency Resolution (lines 738-759)    <pre><code># Merge dependency results into inputs\nresolved_inputs = await resolve_task_dependencies(task, self.task_repository)\n\nif resolved_inputs != (task.inputs or {}):\n    await self.task_repository.update_task(task_id, inputs=resolved_inputs)\n</code></pre></p> </li> <li> <p>Pre-Hook Execution (lines 761-794)    <pre><code># Store inputs before pre-hooks for change detection\ninputs_before_pre_hooks = copy.deepcopy(task.inputs) if task.inputs else {}\n\n# Execute pre-hooks (can modify task.inputs)\nawait self._execute_pre_hooks(task)\n\n# Auto-persist if inputs changed\nif inputs_after_pre_hooks != inputs_before_pre_hooks:\n    await self.task_repository.update_task(task_id, inputs=inputs_to_save)\n    task = await self.task_repository.get_task_by_id(task_id)  # Refresh\n</code></pre></p> </li> <li> <p>Task Execution (lines 807-815)    <pre><code># Execute task using agent executor\ntask_result = await self._execute_task_with_schemas(task, final_inputs)\n</code></pre></p> </li> <li> <p>Status Update and Cleanup (lines 832-856)    <pre><code># Update task status\nawait self.task_repository.update_task(\n    task_id=task_id,\n    status=\"completed\",\n    progress=1.0,\n    result=task_result,\n    error=None,\n    completed_at=datetime.now(timezone.utc)\n)\n\n# Clear executor reference\nexecutor = self._executor_instances.pop(task_id, None)\nif executor and hasattr(executor, 'clear_task_context'):\n    executor.clear_task_context()\n</code></pre></p> </li> <li> <p>Post-Hook Execution (lines 858-897)    <pre><code># Execute post-hooks\nawait self._execute_post_hooks(task, final_inputs, task_result)\n\n# Trigger dependent tasks via callbacks\nif use_callback:\n    await self.execute_after_task(task)\n</code></pre></p> </li> </ol>"},{"location":"architecture/task-tree-lifecycle/#2-database-session-lifecycle","title":"2. Database Session Lifecycle","text":""},{"location":"architecture/task-tree-lifecycle/#21-session-scope","title":"2.1 Session Scope","text":"<p>Session Creation: - Created at <code>TaskExecutor.execute_task_tree()</code> entry point - Shared by ALL operations within the task tree execution - Managed by <code>create_pooled_session()</code> async context manager</p> <p>Session Lifetime &amp; Hook Context: <pre><code>TaskExecutor.execute_task_tree (session created)\n\u251c\u2500\u2500 TaskManager.distribute_task_tree\n\u2502   \u251c\u2500\u2500 set_hook_context(self.task_repository)  [ContextVar active, shares session]\n\u2502   \u251c\u2500\u2500 on_tree_created hooks\n\u2502   \u251c\u2500\u2500 on_tree_started hooks\n\u2502   \u251c\u2500\u2500 _execute_task_tree_recursive\n\u2502   \u2502   \u251c\u2500\u2500 _execute_single_task (task 1)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 update_task\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 resolve_task_dependencies\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pre-hooks (can modify task.inputs)\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 execute task\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 update_task\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 post-hooks\n\u2502   \u2502   \u251c\u2500\u2500 _execute_single_task (task 2)\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 on_tree_completed/failed hooks\n\u2502   \u2514\u2500\u2500 clear_hook_context()  [ContextVar cleared]\n\u2514\u2500\u2500 (session automatically committed/rolled back)\n</code></pre> All hooks and task executions within a tree share the same session and ContextVar context. See also: Hook Context Lifecycle Timeline below.</p>"},{"location":"architecture/task-tree-lifecycle/#22-commit-strategy","title":"2.2 Commit Strategy","text":"<p>Per-Operation Commits: - Each <code>TaskRepository</code> method commits its own transaction</p> <p>Benefits: - No cascading rollbacks across the entire task tree - Each task's status update is persisted immediately - Failed tasks don't affect already-completed tasks - Better error isolation and recovery</p> <p>Trade-offs: - Cannot roll back multiple tasks atomically - Must handle partial failures explicitly - Requires careful error handling in business logic</p>"},{"location":"architecture/task-tree-lifecycle/#23-session-safety-features","title":"2.3 Session Safety Features","text":"<p>1. flag_modified for JSON Fields (lines 633, 675, 717, 833, 869 in task_repository.py) <pre><code>from sqlalchemy.orm.attributes import flag_modified\n\n# After modifying JSON field\ntask.result = new_result\nflag_modified(task, \"result\")  # Tell SQLAlchemy about the change\nawait self.db.commit()\n</code></pre></p> <p>2. db.refresh After Commits (lines 645-647 in task_repository.py) <pre><code>await self.db.commit()\n# Refresh to get latest data from database\nawait self.db.refresh(task)\n</code></pre></p> <p>3. Concurrent Execution Protection (lines 188-197 in task_executor.py) <pre><code>if self.is_task_running(root_task_id):\n    return {\"status\": \"already_running\", ...}\n</code></pre></p>"},{"location":"architecture/task-tree-lifecycle/#3-hook-context-lifecycle","title":"3. Hook Context Lifecycle","text":""},{"location":"architecture/task-tree-lifecycle/#31-contextvar-based-context-management","title":"3.1 ContextVar-Based Context Management","text":"<p>File: <code>src/apflow/core/storage/context.py</code> (lines 10-108)</p> <p>Design Pattern: - Uses Python 3.7+ ContextVar (not thread-local) - Inspired by Flask's request context and Celery's task context - Thread-safe and async-compatible</p> <p>Implementation: <pre><code>from contextvars import ContextVar\n\n_hook_context: ContextVar[Optional[Dict[str, Any]]] = ContextVar('hook_context', default=None)\n\ndef set_hook_context(task_repository: TaskRepository) -&gt; None:\n    \"\"\"Set hook context with task repository\"\"\"\n    _hook_context.set({\"task_repository\": task_repository})\n\ndef clear_hook_context() -&gt; None:\n    \"\"\"Clear hook context\"\"\"\n    _hook_context.set(None)\n\ndef get_hook_session() -&gt; Session:\n    \"\"\"Get database session from hook context\"\"\"\n    context = _hook_context.get()\n    if context is None:\n        raise RuntimeError(\"Hook context not set. Hooks can only access DB during task tree execution.\")\n    return context[\"task_repository\"].db\n\ndef get_hook_repository() -&gt; TaskRepository:\n    \"\"\"Get task repository from hook context\"\"\"\n    context = _hook_context.get()\n    if context is None:\n        raise RuntimeError(\"Hook context not set. Hooks can only access DB during task tree execution.\")\n    return context[\"task_repository\"]\n</code></pre></p>"},{"location":"architecture/task-tree-lifecycle/#32-hook-context-lifecycle-timeline","title":"3.2 Hook Context Lifecycle Timeline","text":"<p>See the unified diagram above under Session Lifetime &amp; Hook Context. All hooks (on_tree_created, on_tree_started, pre-hooks, post-hooks, on_tree_completed/failed) share the same session and ContextVar context, which is set at the start of distribute_task_tree and cleared in the finally block.</p>"},{"location":"architecture/task-tree-lifecycle/#33-hook-context-guarantees","title":"3.3 Hook Context Guarantees","text":"<p>1. Exception Safety: <pre><code>try:\n    set_hook_context(self.task_repository)\n    # ... all hook executions ...\nfinally:\n    clear_hook_context()  # ALWAYS executes\n</code></pre></p> <p>2. Context Isolation: - Each task tree execution has its own ContextVar context - Concurrent task tree executions have isolated contexts - No context leakage between executions</p> <p>3. Session Sharing: - All hooks in the same task tree execution share the same session - Same transaction context for all hooks - Modifications in pre-hooks are visible to post-hooks</p>"},{"location":"architecture/task-tree-lifecycle/#34-hook-types-and-db-access","title":"3.4 Hook Types and DB Access","text":"<p>Pre-Hooks (<code>register_pre_hook</code>): - When: After dependency resolution, before task execution - Can Modify: <code>task.inputs</code> (auto-persisted if changed) - DB Access: Via <code>get_hook_repository()</code> or <code>get_hook_session()</code> - Use Cases: Input validation, data transformation, pre-processing</p> <p>Post-Hooks (<code>register_post_hook</code>): - When: After task execution, before dependent tasks - Can Modify: Other task fields (requires explicit repository calls) - DB Access: Via <code>get_hook_repository()</code> or <code>get_hook_session()</code> - Use Cases: Notifications, logging, result processing, side effects</p> <p>Task Tree Hooks (<code>register_task_tree_hook</code>): - When: Tree lifecycle events (created, started, completed, failed) - Can Modify: Task fields (requires explicit repository calls) - DB Access: Via <code>get_hook_repository()</code> or <code>get_hook_session()</code> - Use Cases: Tree-level monitoring, cleanup, aggregation</p>"},{"location":"architecture/task-tree-lifecycle/#35-hook-modification-patterns","title":"3.5 Hook Modification Patterns","text":"<p>Pattern 1: Auto-Persisted task.inputs (pre-hooks only): <pre><code>from apflow import register_pre_hook\n\n@register_pre_hook\nasync def validate_inputs(task):\n    # Modify task.inputs directly\n    if task.inputs and \"url\" in task.inputs:\n        task.inputs[\"url\"] = task.inputs[\"url\"].strip()\n\n    # Changes are auto-detected and persisted\n    # No explicit repository call needed\n</code></pre></p> <p>Pattern 2: Explicit Field Updates (all hooks): <pre><code>from apflow import register_post_hook, get_hook_repository\n\n@register_post_hook\nasync def update_metadata(task, inputs, result):\n    repo = get_hook_repository()\n\n    # Explicit repository call required for non-inputs fields\n    await repo.update_task(\n        task_id=task.id,\n        params={\"processed_at\": datetime.now().isoformat()}\n    )\n</code></pre></p> <p>Pattern 3: Query Other Tasks: <pre><code>from apflow import register_task_tree_hook, get_hook_repository\n\n@register_task_tree_hook(\"on_tree_completed\")\nasync def aggregate_results(root_task, status):\n    repo = get_hook_repository()\n\n    # Query dependent tasks\n    all_tasks = await repo.list_tasks_by_root_task_id(root_task.id)\n\n    # Aggregate and update\n    total_tokens = sum(t.result.get(\"token_usage\", 0) for t in all_tasks if t.result)\n    await repo.update_task(\n        task_id=root_task.id,\n        result={\"total_tokens\": total_tokens}\n    )\n</code></pre></p>"},{"location":"architecture/task-tree-lifecycle/#4-error-handling-and-cleanup","title":"4. Error Handling and Cleanup","text":""},{"location":"architecture/task-tree-lifecycle/#41-exception-propagation","title":"4.1 Exception Propagation","text":"<p>Task Execution Errors: <pre><code>try:\n    task_result = await self._execute_task_with_schemas(task, final_inputs)\nexcept Exception as e:\n    # Update task status to failed\n    await self.task_repository.update_task(\n        task_id=task_id,\n        status=\"failed\",\n        error=str(e),\n        completed_at=datetime.now(timezone.utc)\n    )\n    raise  # Re-raise to propagate to tree level\n</code></pre></p> <p>Hook Errors (lines 924-929 in task_manager.py): <pre><code>try:\n    if iscoroutinefunction(hook):\n        await hook(task)\n    else:\n        await asyncio.to_thread(hook, task)\nexcept Exception as e:\n    # Log error but don't fail the task execution\n    logger.warning(f\"Pre-hook {hook.__name__} failed for task {task.id}: {str(e)}. Continuing with task execution.\")\n</code></pre></p> <p>Design Decision: - Hook errors are logged but don't fail task execution - Ensures robustness: one broken hook doesn't break entire system - Critical hooks should implement their own error handling</p>"},{"location":"architecture/task-tree-lifecycle/#42-resource-cleanup","title":"4.2 Resource Cleanup","text":"<p>Executor Cleanup (lines 831-837, 849-856 in task_manager.py): <pre><code># Clear executor reference after execution\nexecutor = self._executor_instances.pop(task_id, None)\nif executor and hasattr(executor, 'clear_task_context'):\n    executor.clear_task_context()\n    logger.debug(f\"Cleared task context for task {task_id}\")\n</code></pre></p> <p>Hook Context Cleanup (guaranteed by finally): <pre><code>finally:\n    clear_hook_context()  # Always executes\n</code></pre></p> <p>Execution Tracking Cleanup (in TaskExecutor): <pre><code>try:\n    # ... execution ...\nfinally:\n    await self.end_task_tracking(root_task_id)  # Always executes\n</code></pre></p>"},{"location":"architecture/task-tree-lifecycle/#5-key-lifecycle-relationships","title":"5. Key Lifecycle Relationships","text":""},{"location":"architecture/task-tree-lifecycle/#51-session-hook-context","title":"5.1 Session \u2194 Hook Context","text":"<pre><code>TaskExecutor creates session\n    \u2193\nTaskManager receives session\n    \u2193\nset_hook_context(task_repository)  \u2190 Hook context references same session\n    \u2193\nAll hooks share this session\n    \u2193\nclear_hook_context()  \u2190 Context cleared\n    \u2193\nSession committed/rolled back by context manager\n</code></pre> <p>Important: Hook context lifetime is WITHIN session lifetime</p>"},{"location":"architecture/task-tree-lifecycle/#52-concurrent-execution-isolation","title":"5.2 Concurrent Execution Isolation","text":"<pre><code>Execution A (root_task_id=task-1)\n\u251c\u2500\u2500 Session A (thread/async context 1)\n\u251c\u2500\u2500 Hook Context A (ContextVar context 1)\n\u2514\u2500\u2500 TaskTracker entry A\n\nExecution B (root_task_id=task-2)  [concurrent]\n\u251c\u2500\u2500 Session B (thread/async context 2)\n\u251c\u2500\u2500 Hook Context B (ContextVar context 2)\n\u2514\u2500\u2500 TaskTracker entry B\n\nExecution C (root_task_id=task-1)  [rejected]\n\u2514\u2500\u2500 Rejected by TaskTracker (already running)\n</code></pre> <p>Guarantees: - Different task trees execute independently - Same task tree cannot execute concurrently - ContextVar provides context isolation</p>"},{"location":"architecture/task-tree-lifecycle/#53-hook-execution-order","title":"5.3 Hook Execution Order","text":"<p>Within Single Task: <pre><code>1. Update to in_progress\n2. Resolve dependencies (merge into inputs)\n3. Pre-hooks (can modify task.inputs)\n4. Auto-persist task.inputs if changed\n5. Execute task\n6. Update to completed/failed\n7. Post-hooks (can read result, modify other fields)\n8. Trigger dependent tasks (callbacks)\n</code></pre></p> <p>Within Task Tree: <pre><code>1. on_tree_created (tree structure created)\n2. on_tree_started (execution begins)\n3. Tasks execute (priority-based, parallel within priority)\n   \u2514\u2500\u2500 For each task: pre-hooks \u2192 execute \u2192 post-hooks\n4. on_tree_completed/failed (all tasks finished)\n</code></pre></p>"},{"location":"architecture/task-tree-lifecycle/#6-best-practices","title":"6. Best Practices","text":""},{"location":"architecture/task-tree-lifecycle/#61-for-hook-developers","title":"6.1 For Hook Developers","text":"<p>DO: - Use <code>get_hook_repository()</code> for DB access in hooks - Handle exceptions in your hooks (don't rely on framework catching them) - Keep hooks fast (they block task execution) - Modify <code>task.inputs</code> directly in pre-hooks (auto-persists) - Use explicit repository calls for other fields</p> <p>DON'T: - Create new sessions in hooks (use provided session) - Perform long-running operations in hooks - Assume hook execution order across different tasks - Modify task attributes without repository methods (except task.inputs in pre-hooks)</p>"},{"location":"architecture/task-tree-lifecycle/#62-for-extension-developers","title":"6.2 For Extension Developers","text":"<p>DO: - Register hooks at application startup (before TaskExecutor creation) - Test hooks with concurrent task tree executions - Document hook requirements and side effects - Implement proper error handling in hooks</p> <p>DON'T: - Register hooks dynamically during execution - Share mutable state between hook invocations - Rely on hook execution order guarantees</p>"},{"location":"architecture/task-tree-lifecycle/#63-for-framework-users","title":"6.3 For Framework Users","text":"<p>DO: - Understand session lifecycle when debugging - Use concurrent execution protection (already built-in) - Monitor hook errors in logs - Test task trees with various failure scenarios</p> <p>DON'T: - Execute same task tree concurrently (automatically prevented) - Assume atomic rollback across multiple tasks (per-operation commits) - Rely on hook execution for critical business logic (they can fail silently)</p>"},{"location":"architecture/task-tree-lifecycle/#7-summary","title":"7. Summary","text":""},{"location":"architecture/task-tree-lifecycle/#session-lifecycle","title":"Session Lifecycle","text":"<ul> <li>Scope: Entire task tree execution</li> <li>Creation: TaskExecutor.execute_task_tree()</li> <li>Cleanup: Automatic via async context manager</li> <li>Strategy: Per-operation commits (no cascading rollbacks)</li> </ul>"},{"location":"architecture/task-tree-lifecycle/#hook-context-lifecycle","title":"Hook Context Lifecycle","text":"<ul> <li>Scope: Entire task tree execution (same as session)</li> <li>Setup: set_hook_context() at distribute_task_tree entry</li> <li>Cleanup: clear_hook_context() in finally block (guaranteed)</li> <li>Isolation: ContextVar provides per-execution context</li> </ul>"},{"location":"architecture/task-tree-lifecycle/#execution-lifecycle","title":"Execution Lifecycle","text":"<ul> <li>Entry: TaskExecutor.execute_task_tree()</li> <li>Distribution: TaskManager.distribute_task_tree()</li> <li>Execution: Priority-based, parallel within priority</li> <li>Cleanup: Guaranteed via try/finally blocks</li> </ul>"},{"location":"architecture/task-tree-lifecycle/#key-guarantees","title":"Key Guarantees","text":"<ol> <li>Hook context is always cleared (finally block)</li> <li>Execution tracking is always cleaned up (finally block)</li> <li>Each task's status updates are persisted immediately (per-operation commits)</li> <li>Concurrent execution of same task tree is prevented (TaskTracker)</li> <li>Hooks share same session as task execution (ContextVar)</li> <li>Context isolation between concurrent task trees (ContextVar)</li> </ol>"},{"location":"cli/","title":"CLI Documentation","text":"<p>This directory contains comprehensive guides for using the apflow CLI.</p>"},{"location":"cli/#overview","title":"Overview","text":"<p>For a high-level overview and quick start, see CLI Usage Guide in the guides directory.</p>"},{"location":"cli/#structure","title":"Structure","text":"<ul> <li>configuration.md - Configuration management</li> <li>Multi-location system (project-local, user-global, environment override)</li> <li>File structure (config.cli.yaml)</li> <li>Configuration commands (init-server, set, gen-token)</li> <li>File permissions and security</li> <li> <p>Best practices</p> </li> <li> <p>commands.md - Complete command reference</p> </li> <li>Task execution (run, list, status, watch)</li> <li>Task management (create, update, delete, copy)</li> <li>Flow management</li> <li>Executor methods</li> <li> <p>Input/output formats</p> </li> <li> <p>api-gateway.md - CLI with API Server integration</p> </li> <li>Direct access vs API gateway modes</li> <li>Why API gateway (DuckDB concurrency)</li> <li>Configuration</li> <li>Error handling and retry strategy</li> <li>Deployment patterns</li> <li> <p>Testing and validation</p> </li> <li> <p>examples.md - Practical usage examples</p> </li> <li>Quick start examples</li> <li>Real-world scenarios</li> <li>Advanced usage patterns</li> <li>Common workflow examples</li> </ul>"},{"location":"cli/#quick-navigation","title":"Quick Navigation","text":""},{"location":"cli/#for-first-time-users","title":"For First-Time Users","text":"<p>Start with the CLI Usage Guide to understand CLI basics, then commands.md for practical examples.</p>"},{"location":"cli/#for-configuration","title":"For Configuration","text":"<p>See configuration.md for multi-location setup, file permissions, and management commands.</p>"},{"location":"cli/#for-production-deployment","title":"For Production Deployment","text":"<p>See api-gateway.md for API server integration and distributed deployment patterns.</p>"},{"location":"cli/#for-advanced-usage","title":"For Advanced Usage","text":"<p>See examples.md for complex workflows, parallel execution, and scripting.</p>"},{"location":"cli/#key-concepts","title":"Key Concepts","text":""},{"location":"cli/#cli-modes","title":"CLI Modes","text":"<p>Direct Access (Default): - CLI directly accesses database - No server required - Fast but single-process only</p> <p>API Gateway (Production): - CLI communicates with API server - Multiple clients safe - Distributed deployment support</p>"},{"location":"cli/#configuration-priority","title":"Configuration Priority","text":"<ol> <li>Environment variable: <code>APFLOW_CONFIG_DIR</code> (highest)</li> <li>Project-local: <code>.data/</code> directory</li> <li>User-global: <code>~/.aipartnerup/apflow/</code></li> <li>Default: Current directory (lowest)</li> </ol>"},{"location":"cli/#file-structure","title":"File Structure","text":"<ul> <li><code>config.cli.yaml</code> (600 permissions) - All CLI settings (sensitive and non-sensitive)</li> </ul>"},{"location":"cli/#common-tasks","title":"Common Tasks","text":""},{"location":"cli/#initialize-configuration","title":"Initialize Configuration","text":"<pre><code>apflow config init-server\n</code></pre>"},{"location":"cli/#execute-tasks","title":"Execute Tasks","text":"<pre><code>apflow run batch-id --tasks '[...]'\n</code></pre>"},{"location":"cli/#query-task-status","title":"Query Task Status","text":"<pre><code>apflow tasks list\napflow tasks status task-id\napflow tasks watch --all\n</code></pre>"},{"location":"cli/#manage-configuration","title":"Manage Configuration","text":"<pre><code>apflow config path              # Show all locations\napflow config set key value     # Set value\napflow config gen-token --save  # Generate JWT token\n</code></pre>"},{"location":"cli/#external-links","title":"External Links","text":"<ul> <li>Main Documentation</li> <li>Getting Started</li> <li>API Server Guide</li> <li>Best Practices</li> <li>Contributing Guide</li> </ul>"},{"location":"cli/#support","title":"Support","text":"<p>For issues or questions: 1. Check FAQ 2. See Troubleshooting sections 3. Review Best Practices 4. Open an issue on GitHub</p>"},{"location":"cli/api-gateway/","title":"CLI with API Gateway","text":""},{"location":"cli/api-gateway/#overview","title":"Overview","text":"<p>The CLI can be configured to route commands through an API server, enabling:</p> <ul> <li>Concurrent access: Multiple clients (CLI, API, external tools) safely accessing the same database</li> <li>Distributed deployment: CLI on one machine, API server on another</li> <li>Unified data: All clients see the same data through shared database</li> <li>Security: API authentication, request validation, audit logging</li> </ul>"},{"location":"cli/api-gateway/#problem-statement","title":"Problem Statement","text":""},{"location":"cli/api-gateway/#duckdb-concurrency-issue","title":"DuckDB Concurrency Issue","text":"<p>DuckDB (default database) is optimized for single-process access:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CLI    \u2502   API    \u2502 External \u2502\n\u2502          \u2502 Server   \u2502 Tools    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502         \u2502          \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2193\n        DuckDB (Single-process optimized)\n</code></pre> <p>Problem: Multiple processes accessing DuckDB simultaneously can cause: - Lock contention - Transaction conflicts - Potential data corruption</p>"},{"location":"cli/api-gateway/#solution-api-gateway-pattern","title":"Solution: API Gateway Pattern","text":"<p>Route all database access through a single API server:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CLI    \u2502 External \u2502 Browser  \u2502\n\u2502          \u2502 Tools    \u2502 Client   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502         \u2502          \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2193\n          API Server (Single-process)\n                \u2193\n            DuckDB (Safe)\n</code></pre> <p>Benefits: - Single process accesses database (safe) - Multiple clients use safe HTTP API - All writes serialized through API - Supports concurrent reads - Enables distributed deployment</p>"},{"location":"cli/api-gateway/#architecture","title":"Architecture","text":""},{"location":"cli/api-gateway/#two-operating-modes","title":"Two Operating Modes","text":""},{"location":"cli/api-gateway/#mode-1-direct-database-access-default","title":"Mode 1: Direct Database Access (Default)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CLI Process               \u2502\n\u2502  (cli_config.py)            \u2502\n\u2502    \u2193                         \u2502\n\u2502  TaskRepository             \u2502\n\u2502    \u2193                         \u2502\n\u2502  DuckDB Instance            \u2502\n\u2502  (~/.aipartnerup/data/)    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>When to use: Single user, single machine, development</p> <p>Commands: <pre><code># No config needed\napflow run batch --tasks '[...]'\napflow tasks list\napflow tasks status task-001\n</code></pre></p> <p>Pros: - No server required - Fast (direct DB access) - Simple setup - Good for development</p> <p>Cons: - Only works locally - Single-process only - Cannot share database with API</p>"},{"location":"cli/api-gateway/#mode-2-api-gateway-configured","title":"Mode 2: API Gateway (Configured)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CLI Process   \u2502\n\u2502  (api_client)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 HTTP/A2A\n         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   API Server                \u2502\n\u2502  (api/app.py)               \u2502\n\u2502    \u2193                         \u2502\n\u2502  TaskRepository             \u2502\n\u2502    \u2193                         \u2502\n\u2502  DuckDB Instance            \u2502\n\u2502  (Shared)                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>When to use: Multi-client, production, distributed systems</p> <p>Commands: <pre><code># Configure API server\napflow config init-server\n\n# CLI automatically uses API\napflow run batch --tasks '[...]'\napflow tasks list\napflow tasks status task-001\n</code></pre></p> <p>Pros: - Multiple clients safe - Supports distributed deployment - Can run API and CLI on different machines - Production-ready concurrency</p> <p>Cons: - Requires API server running - Slightly higher latency (HTTP overhead) - More complex setup</p>"},{"location":"cli/api-gateway/#configuration","title":"Configuration","text":""},{"location":"cli/api-gateway/#quick-setup","title":"Quick Setup","text":"<pre><code># Initialize API server configuration (recommended)\napflow config init-server\n\n# This creates:\n# - .data/config.cli.yaml with api_server_url and generated JWT secret\n</code></pre>"},{"location":"cli/api-gateway/#manual-configuration","title":"Manual Configuration","text":"<pre><code># Set API server URL\napflow config set api_server_url http://localhost:8000\n\n# Generate authentication token\napflow config gen-token --role admin --save\n</code></pre>"},{"location":"cli/api-gateway/#environment-override","title":"Environment Override","text":"<pre><code># Override for specific command\nexport APFLOW_CONFIG_DIR=/custom/config\napflow run batch --tasks '[...]'\n\n# Or use environment variable directly\nexport APFLOW_API_SERVER=http://api.example.com\nexport APFLOW_API_AUTH_TOKEN=your-token\n</code></pre>"},{"location":"cli/api-gateway/#configuration-file","title":"Configuration File","text":"<p>config.cli.yaml: <pre><code>api_server_url: http://localhost:8000\napi_timeout: 30\napi_retry_count: 3\nauto_use_api_if_configured: true\nadmin_auth_token: your-jwt-token\njwt_secret: server-jwt-secret\n</code></pre></p>"},{"location":"cli/api-gateway/#cli-usage-with-api-gateway","title":"CLI Usage with API Gateway","text":""},{"location":"cli/api-gateway/#enable-api-gateway","title":"Enable API Gateway","text":"<p>Once API server is configured, CLI automatically uses it:</p> <pre><code># No changes needed - CLI detects config and uses API\napflow run batch --tasks '[...]'\n</code></pre>"},{"location":"cli/api-gateway/#run-without-api-force-direct-access","title":"Run Without API (Force Direct Access)","text":"<p>If you want to skip API and use direct database access:</p> <pre><code># Temporarily disable API\nexport APFLOW_API_SERVER=\napflow run batch --tasks '[...]'\n</code></pre> <p>Or remove configuration: <pre><code>rm -rf .data/config.cli.yaml\n</code></pre></p>"},{"location":"cli/api-gateway/#error-handling","title":"Error Handling","text":""},{"location":"cli/api-gateway/#common-errors","title":"Common Errors","text":"<p>Error: \"Cannot connect to API server\"</p> <pre><code># Check if server is running\ncurl http://localhost:8000/health\n\n# If not, start it\napflow serve\n# Or for daemon mode\napflow daemon start --port 8000\n</code></pre> <p>Error: \"Unauthorized (401)\"</p> <pre><code># Check token\napflow config set api_auth_token &lt;new-token&gt; --sensitive\n\n# Or regenerate\napflow config gen-token --role admin --save\n</code></pre> <p>Error: \"Connection timeout\"</p> <pre><code># Increase timeout\napflow config set api_timeout 60\n\n# Or check network\nping localhost:8000\n</code></pre> <p>Error: \"Database locked\"</p> <pre><code># Check if multiple processes accessing DuckDB\nps aux | grep apflow\n\n# Use API server instead to avoid conflicts\napflow config init-server\napflow serve\n</code></pre>"},{"location":"cli/api-gateway/#error-recovery","title":"Error Recovery","text":"<p>Automatic retry with exponential backoff:</p> <pre><code>Request \u2192 Fail (retried up to 3 times)\n  \u2193 Wait 1s, retry\n  \u2193 Fail, Wait 2s, retry\n  \u2193 Fail, Wait 4s, retry\n  \u2193 Final error\n</code></pre> <p>Configure retry behavior:</p> <pre><code>apflow config set api_retry_count 5\napflow config set api_timeout 60\n</code></pre>"},{"location":"cli/api-gateway/#retry-strategy","title":"Retry Strategy","text":""},{"location":"cli/api-gateway/#default-behavior","title":"Default Behavior","text":"<ul> <li>Retries: 3 attempts</li> <li>Initial delay: 1 second</li> <li>Backoff: Exponential (1s \u2192 2s \u2192 4s)</li> <li>Max timeout: 30 seconds per request</li> </ul>"},{"location":"cli/api-gateway/#configuration_1","title":"Configuration","text":"<pre><code># Increase retries for unreliable networks\napflow config set api_retry_count 5\n\n# Increase timeout for slow servers\napflow config set api_timeout 60\n</code></pre>"},{"location":"cli/api-gateway/#idempotent-operations","title":"Idempotent Operations","text":"<p>All CLI operations are idempotent: - Running same task twice creates duplicate (OK, different ID) - Querying same task returns same result (safe to retry) - Cancelling same task twice is safe (already cancelled)</p>"},{"location":"cli/api-gateway/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"cli/api-gateway/#pattern-1-single-machine-development","title":"Pattern 1: Single Machine (Development)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        Same Machine             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502  \u2502   CLI       \u2502 (Manual)       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502         \u2502 HTTP                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502   API Server (daemon)  \u2502    \u2502\n\u2502  \u2502   (HTTP port 8000)     \u2502    \u2502\n\u2502  \u2502        \u2193                \u2502    \u2502\n\u2502  \u2502   DuckDB (shared)      \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Setup: <pre><code># Initialize configuration\napflow config init-server\n\n# Start API server in background\napflow daemon start --port 8000\n\n# Use CLI (automatically uses API)\napflow run batch --tasks '[...]'\napflow tasks list\n</code></pre></p>"},{"location":"cli/api-gateway/#pattern-2-distributed-production","title":"Pattern 2: Distributed (Production)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client Machine A   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  CLI Instance  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u2502 HTTP\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   API Server        \u2502\n\u2502   (machine B)       \u2502\n\u2502   (HTTP port 8000)  \u2502\n\u2502        \u2193            \u2502\n\u2502   DuckDB (NFS)      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u25b2\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client Machine C   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  CLI Instance  \u2502  \u2502\n\u2502  \u2502 or Web Browser \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Setup: <pre><code># On API Server Machine\nexport DATABASE_URL=\"postgresql+asyncpg://user:pass@db-server/apflow\"\napflow daemon start --port 8000\n\n# On Client Machine\napflow config set api_server http://api-server:8000\napflow config set api_auth_token &lt;token&gt; --sensitive\napflow run batch --tasks '[...]'\n</code></pre></p>"},{"location":"cli/api-gateway/#pattern-3-hybrid-api-cli-mixed","title":"Pattern 3: Hybrid (API + CLI Mixed)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   API Server        \u2502\n\u2502   (HTTP clients)    \u2502\n\u2502        \u2193            \u2502\n\u2502   DuckDB            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u25b2\n           \u2502 Direct Access\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   CLI Instance      \u2502\n\u2502   (Local tasks)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Use case: CLI tasks locally, API for remote access</p> <p>Setup: <pre><code># API and CLI on same machine\n# - CLI uses direct DB access (no config)\n# - API uses same DuckDB\n# - They share data automatically\n\n# Start API server\napflow serve --reload\n\n# In another terminal, use CLI\napflow run batch --tasks '[...]'\n\n# Both see same data\napflow tasks list  # Shows tasks from CLI and API\n</code></pre></p>"},{"location":"cli/api-gateway/#migration-guide","title":"Migration Guide","text":""},{"location":"cli/api-gateway/#from-direct-access-to-api-gateway","title":"From Direct Access to API Gateway","text":"<p>Step 1: Start API server <pre><code>apflow daemon start --port 8000\n</code></pre></p> <p>Step 2: Configure CLI <pre><code>apflow config init-server\n</code></pre></p> <p>Step 3: Switch to API CLI automatically detects and uses API. No code changes needed.</p> <p>Step 4: Verify <pre><code># This now goes through API instead of direct DB\napflow tasks list\napflow run batch --tasks '[...]'\n</code></pre></p> <p>Step 5: Stop direct access Once API is stable, can stop direct CLI database access:</p> <pre><code># Leave config file for automatic API routing\n# All CLI commands now use API\n</code></pre>"},{"location":"cli/api-gateway/#testing-validation","title":"Testing &amp; Validation","text":""},{"location":"cli/api-gateway/#health-check","title":"Health Check","text":"<pre><code># Check if API server is responding\ncurl http://localhost:8000/health\n\n# Check authentication\ncurl -H \"Authorization: Bearer &lt;token&gt;\" \\\n  http://localhost:8000/api/tasks\n</code></pre>"},{"location":"cli/api-gateway/#load-testing","title":"Load Testing","text":"<pre><code># Test concurrent CLI access through API\nfor i in {1..10}; do\n  (apflow run batch$i --tasks '[...]' &amp;)\ndone\nwait\n\n# All requests handled safely\napflow tasks list\n</code></pre>"},{"location":"cli/api-gateway/#data-consistency","title":"Data Consistency","text":"<pre><code># Via API\ncurl -H \"Authorization: Bearer &lt;token&gt;\" \\\n  http://localhost:8000/api/tasks\n\n# Via CLI\napflow tasks list\n\n# Data should be identical\n</code></pre>"},{"location":"cli/api-gateway/#faq","title":"FAQ","text":""},{"location":"cli/api-gateway/#q-do-i-need-api-server-for-cli","title":"Q: Do I need API server for CLI?","text":"<p>A: No. CLI works independently. API server is optional for multi-client scenarios.</p>"},{"location":"cli/api-gateway/#q-can-cli-and-api-run-at-the-same-time","title":"Q: Can CLI and API run at the same time?","text":"<p>A: Yes. When API is configured, CLI automatically routes through it. Both read/write to same database.</p>"},{"location":"cli/api-gateway/#q-what-if-api-server-goes-down","title":"Q: What if API server goes down?","text":"<p>A: CLI falls back to direct database access (if configured), or fails with connection error.</p>"},{"location":"cli/api-gateway/#q-is-api-gateway-required-for-production","title":"Q: Is API gateway required for production?","text":"<p>A: Recommended for production because it: - Handles concurrent access safely - Supports distributed deployment - Provides audit logging - Enables request validation</p>"},{"location":"cli/api-gateway/#q-can-i-use-postgresql-instead-of-duckdb","title":"Q: Can I use PostgreSQL instead of DuckDB?","text":"<p>A: Yes. Set <code>DATABASE_URL</code> environment variable: <pre><code>export DATABASE_URL=\"postgresql+asyncpg://user:pass@localhost/apflow\"\napflow daemon start  # Uses PostgreSQL\n</code></pre></p> <p>Both CLI and API will use PostgreSQL automatically.</p>"},{"location":"cli/api-gateway/#q-how-do-i-know-if-cli-is-using-api-or-direct-db","title":"Q: How do I know if CLI is using API or direct DB?","text":"<p>A: Check configuration: <pre><code>apflow config path\n# Shows active config location\n\n# If api_server is set, CLI uses API\n# Otherwise, CLI uses direct DB\n</code></pre></p>"},{"location":"cli/api-gateway/#summary","title":"Summary","text":"<ul> <li>\u2705 CLI can work with or without API server</li> <li>\u2705 API gateway solves DuckDB concurrency</li> <li>\u2705 All data shared between CLI and API</li> <li>\u2705 Easy to enable: <code>apflow config init-server</code></li> <li>\u2705 Automatic routing: no code changes needed</li> <li>\u2705 Supports distributed deployment</li> </ul> <p>Use direct access for development, API gateway for production.</p>"},{"location":"cli/commands/","title":"CLI Commands Reference","text":"<p>Complete reference of all CLI commands for executing and managing tasks.</p>"},{"location":"cli/commands/#task-execution","title":"Task Execution","text":""},{"location":"cli/commands/#apflow-run","title":"apflow run","text":"<p>Execute a task or batch of tasks:</p> <pre><code>apflow run &lt;batch_id&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--tasks &lt;json&gt;</code> - Task definition in JSON format (required) - <code>--inputs &lt;json&gt;</code> - Task inputs as JSON (optional) - <code>--tags &lt;tag1,tag2&gt;</code> - Task tags (optional) - <code>--priority &lt;priority&gt;</code> - Task priority: low, normal, high (default: normal) - <code>--user-id &lt;id&gt;</code> - User ID (optional) - <code>--timeout &lt;seconds&gt;</code> - Execution timeout (default: 300) - <code>--retry-count &lt;count&gt;</code> - Retry failed tasks (default: 0) - <code>--retry-delay &lt;seconds&gt;</code> - Delay between retries (default: 1) - <code>--parallel-count &lt;count&gt;</code> - Run multiple tasks in parallel (default: 1) - <code>--demo-mode</code> - Run in demo mode (for testing)</p> <p>Examples:</p> <p>Basic execution: <pre><code>apflow run batch-001 --tasks '[\n  {\n    \"id\": \"task1\",\n    \"name\": \"Check CPU\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"cpu\"}\n  }\n]'\n</code></pre></p> <p>With inputs and tags: <pre><code>apflow run batch-002 --tasks '[{...}]' \\\n  --inputs '{\"config\": \"value\"}' \\\n  --tags production,monitoring \\\n  --priority high\n</code></pre></p> <p>Parallel execution: <pre><code>apflow run batch-003 --tasks '[{...}, {...}, {...}]' \\\n  --parallel-count 3\n</code></pre></p> <p>With retry: <pre><code>apflow run batch-004 --tasks '[{...}]' \\\n  --retry-count 3 \\\n  --retry-delay 5\n</code></pre></p>"},{"location":"cli/commands/#task-querying","title":"Task Querying","text":""},{"location":"cli/commands/#apflow-tasks-list","title":"apflow tasks list","text":"<p>List all tasks in database:</p> <pre><code>apflow tasks list [OPTIONS]\n</code></pre> <p>Options: - <code>--user-id &lt;id&gt;</code> - Filter by user ID - <code>--status &lt;status&gt;</code> - Filter by status: pending, running, completed, failed, cancelled - <code>--batch-id &lt;id&gt;</code> - Filter by batch ID - <code>--limit &lt;count&gt;</code> - Limit results (default: 100) - <code>--offset &lt;count&gt;</code> - Skip N results (default: 0) - <code>--sort &lt;field&gt;</code> - Sort by field: created_at, updated_at, status - <code>--reverse</code> - Reverse sort order</p> <p>Examples:</p> <p>List all tasks: <pre><code>apflow tasks list\n</code></pre></p> <p>List by user: <pre><code>apflow tasks list --user-id user123\n</code></pre></p> <p>List failed tasks: <pre><code>apflow tasks list --status failed\n</code></pre></p> <p>List with pagination: <pre><code>apflow tasks list --limit 50 --offset 0\n</code></pre></p> <p>List and sort by date: <pre><code>apflow tasks list --sort created_at --reverse\n</code></pre></p>"},{"location":"cli/commands/#apflow-tasks-status","title":"apflow tasks status","text":"<p>Get status of a specific task:</p> <pre><code>apflow tasks status &lt;task_id&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--include-details</code> - Include full task details - <code>--watch</code> - Watch for changes (exit with Ctrl+C) - <code>--watch-interval &lt;seconds&gt;</code> - Polling interval when watching (default: 1)</p> <p>Examples:</p> <p>Check status: <pre><code>apflow tasks status task-001\n</code></pre></p> <p>With full details: <pre><code>apflow tasks status task-001 --include-details\n</code></pre></p> <p>Watch for completion: <pre><code>apflow tasks status task-001 --watch\n# Press Ctrl+C to stop watching\n</code></pre></p>"},{"location":"cli/commands/#apflow-tasks-watch","title":"apflow tasks watch","text":"<p>Monitor task execution in real-time:</p> <pre><code>apflow tasks watch [OPTIONS]\n</code></pre> <p>Options: - <code>--task-id &lt;id&gt;</code> - Watch specific task - <code>--all</code> - Watch all running tasks - <code>--batch-id &lt;id&gt;</code> - Watch tasks in batch - <code>--user-id &lt;id&gt;</code> - Watch user's tasks - <code>--interval &lt;seconds&gt;</code> - Polling interval (default: 1)</p> <p>Examples:</p> <p>Watch single task: <pre><code>apflow tasks watch --task-id task-001\n</code></pre></p> <p>Watch all running tasks: <pre><code>apflow tasks watch --all\n</code></pre></p> <p>Watch batch: <pre><code>apflow tasks watch --batch-id batch-001\n</code></pre></p> <p>Watch with slower polling: <pre><code>apflow tasks watch --all --interval 5\n</code></pre></p>"},{"location":"cli/commands/#task-cancellation","title":"Task Cancellation","text":""},{"location":"cli/commands/#apflow-tasks-cancel","title":"apflow tasks cancel","text":"<p>Cancel one or more running tasks:</p> <pre><code>apflow tasks cancel &lt;task_ids&gt;... [OPTIONS]\n</code></pre> <p>Options: - <code>--force</code> / <code>-f</code> - Force immediate cancellation - <code>--error-message</code> / <code>-m</code> - Custom error message for cancellation (default: \"Cancelled by user\" or \"Force cancelled by user\")</p> <p>Examples:</p> <p>Cancel task: <pre><code>apflow tasks cancel task-001\n</code></pre></p> <p>Cancel multiple tasks: <pre><code>apflow tasks cancel task-001 task-002 task-003\n</code></pre></p> <p>Force cancel: <pre><code>apflow tasks cancel task-001 --force\n</code></pre></p> <p>Cancel with custom message: <pre><code>apflow tasks cancel task-001 --error-message \"Incorrect parameters\"\n</code></pre></p>"},{"location":"cli/commands/#task-management","title":"Task Management","text":""},{"location":"cli/commands/#apflow-tasks-create","title":"apflow tasks create","text":"<p>Create a task without executing:</p> <pre><code>apflow tasks create [OPTIONS]\n</code></pre> <p>Options: - <code>--name &lt;name&gt;</code> - Task name (required) - <code>--method &lt;method&gt;</code> - Executor method (required) - <code>--inputs &lt;json&gt;</code> - Task inputs as JSON - <code>--tags &lt;tags&gt;</code> - Task tags - <code>--description &lt;text&gt;</code> - Task description - <code>--batch-id &lt;id&gt;</code> - Batch ID</p> <p>Examples:</p> <p>Create task: <pre><code>apflow tasks create --name \"CPU Check\" --method system_info_executor \\\n  --inputs '{\"resource\": \"cpu\"}'\n</code></pre></p> <p>With batch and tags: <pre><code>apflow tasks create --name \"Memory Check\" --method system_info_executor \\\n  --batch-id batch-001 --tags monitoring,system\n</code></pre></p>"},{"location":"cli/commands/#apflow-tasks-update","title":"apflow tasks update","text":"<p>Update task configuration:</p> <pre><code>apflow tasks update &lt;task_id&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--name &lt;name&gt;</code> - Update task name - <code>--inputs &lt;json&gt;</code> - Update task inputs - <code>--tags &lt;tags&gt;</code> - Update task tags - <code>--status &lt;status&gt;</code> - Update task status - <code>--description &lt;text&gt;</code> - Update description - <code>--validate</code> - Validate changes before applying</p> <p>Examples:</p> <p>Update task name: <pre><code>apflow tasks update task-001 --name \"New Name\"\n</code></pre></p> <p>Update inputs: <pre><code>apflow tasks update task-001 --inputs '{\"resource\": \"memory\"}'\n</code></pre></p> <p>Update with validation: <pre><code>apflow tasks update task-001 --inputs '{\"resource\": \"disk\"}' --validate\n</code></pre></p>"},{"location":"cli/commands/#apflow-tasks-delete","title":"apflow tasks delete","text":"<p>Delete a task:</p> <pre><code>apflow tasks delete &lt;task_id&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--force</code> - Delete without confirmation - <code>--reason &lt;text&gt;</code> - Deletion reason - <code>--keep-logs</code> - Keep execution logs after deletion</p> <p>Examples:</p> <p>Delete task: <pre><code>apflow tasks delete task-001\n</code></pre></p> <p>Force delete: <pre><code>apflow tasks delete task-001 --force\n</code></pre></p> <p>Delete and keep logs: <pre><code>apflow tasks delete task-001 --force --keep-logs\n</code></pre></p>"},{"location":"cli/commands/#apflow-tasks-clone","title":"apflow tasks clone","text":"<p>Clone (copy/link/archive) a task tree. <code>tasks copy</code> is a backward-compatible alias.</p> <pre><code>apflow tasks clone &lt;task_id&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--origin-type &lt;type&gt;</code> - Origin type: <code>copy</code> (default), <code>link</code>, <code>archive</code>, or <code>mixed</code> - <code>--recursive/--no-recursive</code> - Clone/link entire subtree (default: True) - <code>--link-task-ids &lt;ids&gt;</code> - Comma-separated task IDs to link (for mixed mode) - <code>--reset-fields &lt;fields&gt;</code> - Field overrides as key=value pairs (e.g., 'user_id=new_user,priority=1') - <code>--dry-run</code> - Preview clone without saving to database</p> <p>Examples:</p> <p>Clone task (default copy mode): <pre><code>apflow tasks clone task-001\n</code></pre></p> <p>Clone to new batch (with field override): <pre><code>apflow tasks clone task-001 --reset-fields batch_id=batch-002\n</code></pre></p> <p>Clone with incremented name: <pre><code>apflow tasks clone task-001 --reset-fields name=\"Task (copy)\"\n</code></pre></p> <p>Clone and reset status: <pre><code>apflow tasks clone task-001 --reset-fields status=pending\n</code></pre></p> <p>Note: <code>apflow tasks copy</code> is a supported alias for backward compatibility and accepts the same options.</p>"},{"location":"cli/commands/#flow-management","title":"Flow Management","text":""},{"location":"cli/commands/#apflow-flow-run","title":"apflow flow run","text":"<p>Execute a flow (batch of tasks):</p> <pre><code>apflow flow run &lt;flow_id&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--tasks &lt;json&gt;</code> - Task definitions (required) - <code>--inputs &lt;json&gt;</code> - Flow inputs - <code>--parallel-count &lt;count&gt;</code> - Tasks to run in parallel - <code>--skip-failed</code> - Continue even if task fails - <code>--timeout &lt;seconds&gt;</code> - Flow timeout</p> <p>Examples:</p> <p>Run flow: <pre><code>apflow flow run flow-001 --tasks '[{...}, {...}]'\n</code></pre></p> <p>Run with parallelism: <pre><code>apflow flow run flow-002 --tasks '[...]' --parallel-count 3\n</code></pre></p> <p>Skip failed tasks: <pre><code>apflow flow run flow-003 --tasks '[...]' --skip-failed\n</code></pre></p>"},{"location":"cli/commands/#apflow-flow-status","title":"apflow flow status","text":"<p>Get flow execution status:</p> <pre><code>apflow flow status &lt;flow_id&gt;\n</code></pre> <p>Examples:</p> <p>Check flow status: <pre><code>apflow flow status flow-001\n</code></pre></p>"},{"location":"cli/commands/#apflow-flow-cancel","title":"apflow flow cancel","text":"<p>Cancel a flow:</p> <pre><code>apflow flow cancel &lt;flow_id&gt; [OPTIONS]\n</code></pre> <p>Options: - <code>--force</code> - Force cancellation - <code>--cancel-tasks</code> - Cancel remaining tasks in flow</p> <p>Examples:</p> <p>Cancel flow: <pre><code>apflow flow cancel flow-001\n</code></pre></p> <p>Cancel with tasks: <pre><code>apflow flow cancel flow-001 --cancel-tasks\n</code></pre></p>"},{"location":"cli/commands/#query-and-filtering","title":"Query and Filtering","text":""},{"location":"cli/commands/#common-filter-patterns","title":"Common Filter Patterns","text":"<p>Filter by status: <pre><code>apflow tasks list --status running\napflow tasks list --status completed\napflow tasks list --status failed\napflow tasks list --status cancelled\n</code></pre></p> <p>Filter by user: <pre><code>apflow tasks list --user-id alice\napflow tasks list --user-id bob\n</code></pre></p> <p>Filter by batch: <pre><code>apflow tasks list --batch-id batch-001\napflow tasks list --batch-id batch-002\n</code></pre></p> <p>Combine filters: <pre><code>apflow tasks list --batch-id batch-001 --status failed\napflow tasks list --user-id alice --status running\n</code></pre></p>"},{"location":"cli/commands/#executor-discovery","title":"Executor Discovery","text":""},{"location":"cli/commands/#apflow-executors-list","title":"apflow executors list","text":"<p>List all available executors:</p> <pre><code>apflow executors list [OPTIONS]\n</code></pre> <p>Description: Shows all executors that are currently accessible based on APFLOW_EXTENSIONS environment variable configuration. If APFLOW_EXTENSIONS is set, only executors from those extensions are shown (security restriction).</p> <p>Options: - <code>--format &lt;format&gt;</code> - Output format: table (default), json, ids - <code>--verbose</code> - Show detailed executor information (with descriptions)</p> <p>Examples:</p> <p>Default table format: <pre><code>apflow executors list\n</code></pre></p> <p>Output: <pre><code>Executor ID            Name                      Extension\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsystem_info_executor   System Info Executor      stdio\ncommand_executor       Command Executor          stdio\nrest_executor          REST Executor             http\napflow_api_executor    ApFlow API Executor       apflow\n</code></pre></p> <p>JSON format (useful for scripts and tools): <pre><code>apflow executors list --format json\n</code></pre></p> <p>Output: <pre><code>{\n  \"executors\": [\n    {\n      \"id\": \"system_info_executor\",\n      \"name\": \"System Info Executor\",\n      \"extension\": \"stdio\",\n      \"description\": \"Retrieve system information like CPU, memory, disk usage\"\n    },\n    {\n      \"id\": \"command_executor\",\n      \"name\": \"Command Executor\",\n      \"extension\": \"stdio\",\n      \"description\": \"Execute shell commands on the local system\"\n    },\n    {\n      \"id\": \"rest_executor\",\n      \"name\": \"REST Executor\",\n      \"extension\": \"http\",\n      \"description\": \"Make HTTP REST API calls\"\n    }\n  ],\n  \"count\": 3,\n  \"restricted\": false\n}\n</code></pre></p> <p>IDs only format (simple list of executor IDs): <pre><code>apflow executors list --format ids\n</code></pre></p> <p>Output: <pre><code>system_info_executor\ncommand_executor\nrest_executor\napflow_api_executor\n</code></pre></p> <p>Verbose output with descriptions: <pre><code>apflow executors list --verbose\n</code></pre></p> <p>Output: <pre><code>Executor ID            Name                      Extension  Description\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsystem_info_executor   System Info Executor      stdio      Retrieve system information...\ncommand_executor       Command Executor          stdio      Execute shell commands on...\nrest_executor          REST Executor             http       Make HTTP REST API calls\napflow_api_executor    ApFlow API Executor       apflow     Execute tasks through ApFlow API\n</code></pre></p> <p>Restricted Access Example:</p> <p>When APFLOW_EXTENSIONS environment variable is set: <pre><code>export APFLOW_EXTENSIONS=stdio,http\napflow executors list\n</code></pre></p> <p>Output shows only executors from stdio and http extensions: <pre><code>Executor ID            Name                      Extension\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsystem_info_executor   System Info Executor      stdio\ncommand_executor       Command Executor          stdio\nrest_executor          REST Executor             http\n</code></pre></p> <p>Notes: - Executor availability depends on installed optional dependencies - Use <code>--format json</code> for programmatic access in scripts - The executor IDs are used in task schemas when executing tasks - Check installed extensions with <code>apflow config list</code></p>"},{"location":"cli/commands/#executor-methods","title":"Executor Methods","text":"<p>Common executor methods available:</p>"},{"location":"cli/commands/#system_info_executor","title":"system_info_executor","text":"<p>Get system information:</p> <pre><code>apflow run batch --tasks '[{\n  \"id\": \"t1\",\n  \"name\": \"CPU Info\",\n  \"schemas\": {\"method\": \"system_info_executor\"},\n  \"inputs\": {\"resource\": \"cpu\"}\n}]'\n</code></pre> <p>Inputs: - <code>resource</code>: cpu, memory, disk, network</p>"},{"location":"cli/commands/#http_executor","title":"http_executor","text":"<p>Make HTTP requests:</p> <pre><code>apflow run batch --tasks '[{\n  \"id\": \"t1\",\n  \"name\": \"API Call\",\n  \"schemas\": {\"method\": \"http_executor\"},\n  \"inputs\": {\n    \"url\": \"https://api.example.com/data\",\n    \"method\": \"GET\"\n  }\n}]'\n</code></pre> <p>Inputs: - <code>url</code>: Target URL - <code>method</code>: GET, POST, PUT, DELETE - <code>headers</code>: HTTP headers (optional) - <code>body</code>: Request body (optional)</p>"},{"location":"cli/commands/#command_executor","title":"command_executor","text":"<p>Execute shell commands:</p> <pre><code>apflow run batch --tasks '[{\n  \"id\": \"t1\",\n  \"name\": \"Run Script\",\n  \"schemas\": {\"method\": \"command_executor\"},\n  \"inputs\": {\n    \"command\": \"ls -la\",\n    \"timeout\": 30\n  }\n}]'\n</code></pre> <p>Inputs: - <code>command</code>: Shell command to execute - <code>timeout</code>: Timeout in seconds</p>"},{"location":"cli/commands/#custom_executor","title":"custom_executor","text":"<p>Custom business logic:</p> <p>Implement custom executors by extending the executor framework. See Custom Tasks Guide for details.</p>"},{"location":"cli/commands/#task-input-format","title":"Task Input Format","text":"<p>All task inputs are JSON:</p> <pre><code>{\n  \"id\": \"unique-task-id\",\n  \"name\": \"Human Readable Name\",\n  \"schemas\": {\n    \"method\": \"executor_method_name\"\n  },\n  \"inputs\": {\n    \"param1\": \"value1\",\n    \"param2\": \"value2\"\n  },\n  \"tags\": [\"tag1\", \"tag2\"],\n  \"priority\": \"high\"\n}\n</code></pre> <p>Fields: - <code>id</code>: Unique task identifier - <code>name</code>: Human-readable task name - <code>schemas.method</code>: Executor method to use - <code>inputs</code>: Method-specific parameters (object) - <code>tags</code>: Optional tags for organization - <code>priority</code>: low, normal, high (optional)</p>"},{"location":"cli/commands/#output-format","title":"Output Format","text":"<p>Task output depends on the executor method. Examples:</p> <p>system_info_executor output: <pre><code>{\n  \"cpu\": 45.2,\n  \"memory\": 8192,\n  \"disk\": 102400,\n  \"timestamp\": \"2024-01-15T10:30:00Z\"\n}\n</code></pre></p> <p>http_executor output: <pre><code>{\n  \"status_code\": 200,\n  \"headers\": {...},\n  \"body\": {...},\n  \"elapsed_time\": 0.234\n}\n</code></pre></p> <p>command_executor output: <pre><code>{\n  \"stdout\": \"...\",\n  \"stderr\": \"\",\n  \"return_code\": 0,\n  \"elapsed_time\": 1.234\n}\n</code></pre></p>"},{"location":"cli/commands/#error-handling","title":"Error Handling","text":""},{"location":"cli/commands/#common-errors","title":"Common Errors","text":"<p>Error: \"Task not found\" <pre><code># Check if task ID is correct\napflow tasks list | grep task-id\n</code></pre></p> <p>Error: \"Database connection error\" <pre><code># Check database configuration\necho $DATABASE_URL\n# Or check DuckDB file\nls ~/.aipartnerup/data/apflow.duckdb\n</code></pre></p> <p>Error: \"Invalid task format\" <pre><code># Validate JSON\necho '[{\"id\":\"t1\",\"name\":\"Task\",\"schemas\":{\"method\":\"system_info_executor\"},\"inputs\":{}}]' | python -m json.tool\n</code></pre></p> <p>Error: \"Executor method not found\" <pre><code># Check available methods\napflow executor list\n</code></pre></p>"},{"location":"cli/commands/#debugging","title":"Debugging","text":""},{"location":"cli/commands/#debug-mode","title":"Debug mode:","text":"<pre><code># Enable verbose logging (preferred with APFLOW_ prefix)\nexport APFLOW_LOG_LEVEL=DEBUG\napflow run batch --tasks '[...]'\n\n# Or use generic LOG_LEVEL (fallback)\nexport LOG_LEVEL=DEBUG\napflow run batch --tasks '[...]'\n</code></pre>"},{"location":"cli/commands/#check-task-details","title":"Check task details:","text":"<pre><code># Get full task information\napflow tasks status task-001 --include-details\n</code></pre>"},{"location":"cli/commands/#monitor-execution","title":"Monitor execution:","text":"<pre><code># Watch task execution in real-time\napflow tasks watch --task-id task-001\n</code></pre>"},{"location":"cli/commands/#summary","title":"Summary","text":"<ul> <li>\u2705 Execute tasks: <code>apflow run</code> with JSON task definitions</li> <li>\u2705 Query tasks: <code>apflow tasks list</code>, <code>status</code>, <code>watch</code></li> <li>\u2705 Manage tasks: <code>create</code>, <code>update</code>, <code>delete</code>, <code>copy</code></li> <li>\u2705 Cancel tasks: <code>apflow tasks cancel</code> with force option</li> <li>\u2705 Monitor flows: <code>apflow flow</code> commands</li> <li>\u2705 Debug issues: <code>apflow config set log-level DEBUG</code> Enable debug mode and check logs</li> </ul> <p>All commands support JSON input/output for integration with other tools.</p>"},{"location":"cli/configuration/","title":"CLI Configuration Management","text":""},{"location":"cli/configuration/#overview","title":"Overview","text":"<p>The CLI configuration system supports both project-local and user-global configuration. Configuration is stored in a unified YAML file:</p> <ul> <li><code>config.cli.yaml</code> (600 permissions, owner-only) - All CLI settings (sensitive and non-sensitive)</li> </ul>"},{"location":"cli/configuration/#multi-location-priority-system","title":"Multi-Location Priority System","text":"<p>Configuration can be stored in multiple locations with the following priority (highest to lowest):</p> <ol> <li>Environment Variable: <code>APFLOW_CONFIG_DIR</code> (highest priority)</li> <li>Project-Local: <code>.data/</code> directory in project root</li> <li>User-Global: <code>~/.aipartnerup/apflow/</code> directory</li> <li>Default: Current working directory (if no other location found)</li> </ol> <p>When you save configuration, it goes to the active location determined by this priority system.</p>"},{"location":"cli/configuration/#file-structure","title":"File Structure","text":""},{"location":"cli/configuration/#configcliyaml-all-settings","title":"config.cli.yaml (All Settings)","text":"<p>Unified configuration file with both settings and sensitive credentials:</p> <pre><code>api_server_url: http://localhost:8000\napi_timeout: 30\napi_retry_count: 3\nauto_use_api_if_configured: true\nadmin_auth_token: your-jwt-token-here\njwt_secret: your-jwt-secret-here\n</code></pre> <p>Permissions: <code>600</code> (owner: read/write, group: none, others: none)</p> <p>Who can read: Owner only Who can write: Owner only</p> <p>Why unified? - All configuration in one place for easier management - Consistent permissions (600) for all settings - Still supports multi-location system (project-local and user-global)</p>"},{"location":"cli/configuration/#configuration-commands","title":"Configuration Commands","text":""},{"location":"cli/configuration/#view-configuration-locations","title":"View Configuration Locations","text":"<pre><code>apflow config path\n</code></pre> <p>Example output: <pre><code>Configuration Locations (in priority order):\n1. APFLOW_CONFIG_DIR env var:  Not set\n2. Project-local (.data/):     /home/user/project/.data/\n3. User-global (~/.aipartnerup/apflow/):  ~/.aipartnerup/apflow/\n4. Current directory:          /home/user/project\n\nActive location: /home/user/project/.data/\n</code></pre></p>"},{"location":"cli/configuration/#initialize-configuration-quick-setup","title":"Initialize Configuration (Quick Setup)","text":"<pre><code>apflow config init-server\n</code></pre> <p>This command: 1. Creates <code>.data/config.cli.yaml</code> in project (if not exists) 2. Sets <code>api_server_url</code> to <code>http://localhost:8000</code> 3. Generates and saves JWT secret 4. Shows all file paths for verification</p> <p>Example output: <pre><code>Configuration initialized successfully:\n  Config: .data/config.cli.yaml (600)\n  API Server: http://localhost:8000\n  JWT Secret: Generated and saved\n</code></pre></p>"},{"location":"cli/configuration/#set-configuration-value","title":"Set Configuration Value","text":"<pre><code># Set configuration value\napflow config set api_server_url http://api.example.com\n\n# Set sensitive value (stored in same file with 600 permissions)\napflow config set admin_auth_token my-secret-token\n</code></pre>"},{"location":"cli/configuration/#generate-jwt-token","title":"Generate JWT Token","text":"<pre><code>apflow config gen-token --role admin --save\n</code></pre> <p>This command: 1. Generates a new JWT token 2. Saves to <code>config.cli.yaml</code> (with 600 permissions) 3. Shows token value for reference</p> <p>Options: - <code>--role &lt;role&gt;</code> - User role (default: admin) - <code>--user-id &lt;id&gt;</code> - User ID (optional) - <code>--save</code> - Save to config.cli.yaml</p>"},{"location":"cli/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"cli/configuration/#apflow_config_dir","title":"APFLOW_CONFIG_DIR","text":"<p>Override default configuration directory:</p> <pre><code># Save configuration to a specific directory\nexport APFLOW_CONFIG_DIR=/custom/config/path\napflow config init-server\n\n# Unset to use default priority system\nunset APFLOW_CONFIG_DIR\n</code></pre> <p>This is useful for: - Docker/container deployments (use mounted volumes) - Testing (isolated config directories) - CI/CD pipelines (centralized config)</p>"},{"location":"cli/configuration/#database_url","title":"DATABASE_URL","text":"<p>Specify database connection (optional):</p> <pre><code># Use default DuckDB\n# (no DATABASE_URL needed)\n\n# Use PostgreSQL\nexport DATABASE_URL=\"postgresql+asyncpg://user:password@localhost/apflow\"\napflow run flow --tasks '[...]'\n</code></pre>"},{"location":"cli/configuration/#configuration-loading","title":"Configuration Loading","text":""},{"location":"cli/configuration/#how-cli-loads-configuration","title":"How CLI Loads Configuration","text":"<p>When you run CLI commands, configuration is loaded from:</p> <ol> <li>Check APFLOW_CONFIG_DIR - If set, load from here first</li> <li>Check project-local - If <code>.data/config.cli.yaml</code> exists, load from here</li> <li>Check user-global - If <code>~/.aipartnerup/apflow/config.cli.yaml</code> exists, load from here</li> <li>Use defaults - If nothing found, use built-in defaults</li> </ol>"},{"location":"cli/configuration/#how-api-server-loads-configuration","title":"How API Server Loads Configuration","text":"<p>The API server uses the same multi-location system via <code>core/config_manager.py</code>:</p> <pre><code>from apflow.cli.cli_config import load_cli_config, load_secrets_config\n\n# API loads from multi-location system\nconfig = load_cli_config()\nsecrets = load_secrets_config()\n</code></pre>"},{"location":"cli/configuration/#configuration-scenarios","title":"Configuration Scenarios","text":""},{"location":"cli/configuration/#scenario-1-quick-local-development","title":"Scenario 1: Quick Local Development","text":"<p>No configuration needed - just use CLI:</p> <pre><code>cd ~/my-project\napflow run flow --tasks '[{\"id\": \"t1\", \"name\": \"Task 1\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}}]'\n</code></pre> <p>Database is created automatically in <code>~/.aipartnerup/data/apflow.duckdb</code></p>"},{"location":"cli/configuration/#scenario-2-multi-project-setup","title":"Scenario 2: Multi-Project Setup","text":"<p>Use project-local configuration to keep settings separate:</p> <pre><code># Project A\ncd ~/project-a\napflow config init-server\n# Creates .data/config.cli.yaml in project-a\n\n# Project B\ncd ~/project-b\napflow config init-server\n# Creates .data/config.cli.yaml in project-b\n</code></pre> <p>Each project has its own isolated configuration.</p>"},{"location":"cli/configuration/#scenario-3-team-shared-configuration","title":"Scenario 3: Team Shared Configuration","text":"<p>Use user-global configuration for shared settings:</p> <pre><code># Set up once in home directory\napflow config set api_server_url http://team-api.example.com\n# Saves to ~/.aipartnerup/apflow/config.cli.yaml\n\n# Now use from any project\ncd ~/project-a\napflow run flow --tasks '[...]'\n# Automatically uses ~/.aipartnerup/apflow/config.cli.yaml\n</code></pre> <p>All team members share the same API server.</p>"},{"location":"cli/configuration/#scenario-4-dockercontainer-deployment","title":"Scenario 4: Docker/Container Deployment","text":"<p>Use environment variable for isolated configuration:</p> <pre><code># In Dockerfile or docker-compose.yml\nENV APFLOW_CONFIG_DIR=/etc/apflow/config\n\n# Run container\ndocker run -v /etc/apflow/config:/etc/apflow/config my-app\n\n# Or in docker-compose.yml\nenvironment:\n  - APFLOW_CONFIG_DIR=/config\nvolumes:\n  - ./config:/config\n</code></pre> <p>Configuration comes from mounted volume.</p>"},{"location":"cli/configuration/#scenario-5-cicd-pipeline","title":"Scenario 5: CI/CD Pipeline","text":"<p>Use environment variables for pipeline-specific configuration:</p> <pre><code># In GitHub Actions workflow\nenv:\n  APFLOW_CONFIG_DIR: ./.github/config\n  API_AUTH_TOKEN: ${{ secrets.API_AUTH_TOKEN }}\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - run: apflow config set api_auth_token $API_AUTH_TOKEN --sensitive\n      - run: apflow run flow --tasks '[...]'\n</code></pre> <p>Configuration is isolated per pipeline run.</p>"},{"location":"cli/configuration/#command-reference","title":"Command Reference","text":""},{"location":"cli/configuration/#apflow-config-path","title":"apflow config path","text":"<p>Show all configuration locations:</p> <pre><code>apflow config path\n</code></pre>"},{"location":"cli/configuration/#apflow-config-init-server","title":"apflow config init-server","text":"<p>Quick setup for API server:</p> <pre><code>apflow config init-server [--api-server &lt;url&gt;]\n</code></pre>"},{"location":"cli/configuration/#apflow-config-set","title":"apflow config set","text":"<p>Set a configuration value:</p> <pre><code>apflow config set &lt;key&gt; &lt;value&gt; [--sensitive]\n</code></pre> <p>Examples: <pre><code>apflow config set api_server http://api.example.com\napflow config set api_timeout 60\napflow config set api_auth_token my-secret --sensitive\n</code></pre></p>"},{"location":"cli/configuration/#apflow-config-gen-token","title":"apflow config gen-token","text":"<p>Generate JWT token:</p> <pre><code>apflow config gen-token [--role &lt;role&gt;] [--user-id &lt;id&gt;] [--save]\n</code></pre> <p>Examples: <pre><code># Generate and show token\napflow config gen-token --role admin\n\n# Generate and save to config.cli.yaml\napflow config gen-token --role admin --user-id user123 --save\n</code></pre></p>"},{"location":"cli/configuration/#file-permissions-explanation","title":"File Permissions Explanation","text":""},{"location":"cli/configuration/#why-600-for-configcliyaml","title":"Why 600 for config.cli.yaml?","text":"<ul> <li>Owner only: All configuration (including tokens and secrets) must not be readable by other users</li> <li>No compromise: If a service is compromised, it cannot read other users' credentials</li> <li>Security principle: Both settings and secrets are isolated per user</li> <li>Unified approach: Single secure file is simpler and safer than splitting across files</li> </ul>"},{"location":"cli/configuration/#on-macoslinux","title":"On macOS/Linux","text":"<p>Check permissions: <pre><code>ls -la ~/.aipartnerup/apflow/\n# Output:\n# -rw------- 1 user group config.cli.yaml\n</code></pre></p> <p>Manually fix permissions (if needed): <pre><code>chmod 600 ~/.aipartnerup/apflow/config.cli.yaml\n</code></pre></p>"},{"location":"cli/configuration/#best-practices","title":"Best Practices","text":""},{"location":"cli/configuration/#1-use-multi-location-system","title":"1. Use Multi-Location System","text":"<p>Let CLI manage configuration locations:</p> <pre><code># \u2705 Good: Use default system\ncd ~/my-project\napflow config init-server\n\n# \u274c Avoid: Hardcoding paths\nexport APFLOW_CONFIG_DIR=/tmp/config\n</code></pre>"},{"location":"cli/configuration/#2-all-configuration-in-one-file","title":"2. All Configuration in One File","text":"<p>Use the same <code>config.cli.yaml</code> for all settings (both settings and secrets):</p> <pre><code># \u2705 Good: Set API URL\napflow config set api_server_url http://api.example.com\n\n# \u2705 Good: Set auth token (stored securely in same file)\napflow config set admin_auth_token my-secret-token\n</code></pre>"},{"location":"cli/configuration/#3-use-project-local-for-team-settings","title":"3. Use Project-Local for Team Settings","text":"<p>Each project has its own configuration:</p> <pre><code># \u2705 Good: Project-specific config\ncd ~/project-a\napflow config set api_server http://project-a-api.com\n\ncd ~/project-b\napflow config set api_server http://project-b-api.com\n</code></pre>"},{"location":"cli/configuration/#4-use-user-global-for-common-settings","title":"4. Use User-Global for Common Settings","text":"<p>Shared settings across all projects:</p> <pre><code># \u2705 Good: Shared config in ~/.aipartnerup/apflow/\napflow config set api_retry_count 5\n# Now all projects use this value\n</code></pre>"},{"location":"cli/configuration/#5-verify-configuration-before-using","title":"5. Verify Configuration Before Using","text":"<p>Check active configuration:</p> <pre><code>apflow config path\n# Shows which location is being used\n</code></pre>"},{"location":"cli/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/configuration/#problem-configuration-file-not-found","title":"Problem: \"Configuration file not found\"","text":"<p>Solution: Initialize configuration: <pre><code>apflow config init-server\n</code></pre></p>"},{"location":"cli/configuration/#problem-permission-denied-on-configcliyaml","title":"Problem: \"Permission denied on config.cli.yaml\"","text":"<p>Solution: Check permissions: <pre><code>ls -la ~/.aipartnerup/apflow/config.cli.yaml\n# Should show: -rw------- (600)\n\n# Fix if needed:\nchmod 600 ~/.aipartnerup/apflow/config.cli.yaml\n</code></pre></p>"},{"location":"cli/configuration/#problem-api-server-url-not-working","title":"Problem: \"API server URL not working\"","text":"<p>Solution: Verify configuration: <pre><code>apflow config path\napflow config set api_server http://correct-url.com\n</code></pre></p>"},{"location":"cli/configuration/#problem-multiple-configuration-files-exist","title":"Problem: \"Multiple configuration files exist\"","text":"<p>Solution: Check priority order: <pre><code>apflow config path\n# Shows all locations and which one is active\n</code></pre></p> <p>Suggestion: Keep configuration in one location to avoid confusion.</p>"},{"location":"cli/configuration/#problem-different-config-per-machine","title":"Problem: \"Different config per machine\"","text":"<p>Solution: Use environment variable: <pre><code># On development machine\nexport APFLOW_CONFIG_DIR=~/.apflow-dev\n\n# On production machine\nexport APFLOW_CONFIG_DIR=/etc/apflow\n</code></pre></p>"},{"location":"cli/configuration/#summary","title":"Summary","text":"<ul> <li>\u2705 Configuration stored in 1 unified file (config.cli.yaml)</li> <li>\u2705 Multi-location support (project-local + user-global)</li> <li>\u2705 Environment variable override (APFLOW_CONFIG_DIR)</li> <li>\u2705 Proper file permissions (600 - owner-only)</li> <li>\u2705 Easy commands to initialize and manage</li> <li>\u2705 Shared between CLI and API server</li> </ul> <p>Configuration is flexible enough for both local development and production deployment.</p>"},{"location":"cli/examples/","title":"CLI Usage Examples &amp; Scenarios","text":"<p>Complete examples and common scenarios for using the CLI.</p>"},{"location":"cli/examples/#quick-start-examples","title":"Quick Start Examples","text":""},{"location":"cli/examples/#example-1-monitor-system-resources","title":"Example 1: Monitor System Resources","text":"<p>Execute a single task to check CPU usage:</p> <pre><code># Execute task\napflow run batch-001 --tasks '[\n  {\n    \"id\": \"check-cpu\",\n    \"name\": \"Check CPU Usage\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"cpu\"}\n  }\n]'\n\n# Output: {\"cpu\": 45.2, \"timestamp\": \"2024-01-15T10:30:00Z\"}\n\n# Check task status\napflow tasks status check-cpu\n</code></pre>"},{"location":"cli/examples/#example-2-batch-system-check","title":"Example 2: Batch System Check","text":"<p>Check multiple system resources in one batch:</p> <pre><code>apflow run batch-002 --tasks '[\n  {\n    \"id\": \"cpu-check\",\n    \"name\": \"CPU Usage\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"cpu\"}\n  },\n  {\n    \"id\": \"mem-check\",\n    \"name\": \"Memory Usage\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"memory\"}\n  },\n  {\n    \"id\": \"disk-check\",\n    \"name\": \"Disk Usage\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"disk\"}\n  }\n]'\n\n# Run all 3 tasks in parallel (default)\n# Check status\napflow tasks list --batch-id batch-002\n</code></pre>"},{"location":"cli/examples/#example-3-api-request-with-error-handling","title":"Example 3: API Request with Error Handling","text":"<p>Make HTTP request with error handling:</p> <pre><code># Execute HTTP request\napflow run batch-003 --tasks '[\n  {\n    \"id\": \"api-call\",\n    \"name\": \"Fetch User Data\",\n    \"schemas\": {\"method\": \"http_executor\"},\n    \"inputs\": {\n      \"url\": \"https://api.example.com/users/123\",\n      \"method\": \"GET\",\n      \"headers\": {\"Authorization\": \"Bearer token-123\"}\n    }\n  }\n]' --retry-count 3\n\n# If fails, CLI retries up to 3 times\napflow tasks status api-call --watch\n</code></pre>"},{"location":"cli/examples/#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"cli/examples/#scenario-1-daily-health-check-cron-job","title":"Scenario 1: Daily Health Check (Cron Job)","text":"<p>Setup a cron job to monitor system health daily:</p> <p>File: <code>check_health.sh</code> <pre><code>#!/bin/bash\n\nTIMESTAMP=$(date -u +%Y%m%d_%H%M%S)\nBATCH_ID=\"health-check-$TIMESTAMP\"\n\napflow run \"$BATCH_ID\" --tasks '[\n  {\n    \"id\": \"cpu\",\n    \"name\": \"Check CPU\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"cpu\"}\n  },\n  {\n    \"id\": \"memory\",\n    \"name\": \"Check Memory\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"memory\"}\n  },\n  {\n    \"id\": \"disk\",\n    \"name\": \"Check Disk\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"disk\"}\n  }\n]' --tags health-check,automated\n\n# Log results\nRESULTS=$(apflow tasks list --batch-id \"$BATCH_ID\")\necho \"Health check results: $RESULTS\" &gt;&gt; /var/log/health-check.log\n\n# Alert if any failed\nFAILED=$(echo \"$RESULTS\" | grep -c '\"status\": \"failed\"')\nif [ \"$FAILED\" -gt 0 ]; then\n  echo \"WARNING: $FAILED checks failed\" | mail -s \"Health Check Alert\" admin@example.com\nfi\n</code></pre></p> <p>Setup cron job: <pre><code># Edit crontab\ncrontab -e\n\n# Add entry to run daily at 2 AM\n0 2 * * * /path/to/check_health.sh\n</code></pre></p>"},{"location":"cli/examples/#scenario-2-data-processing-pipeline","title":"Scenario 2: Data Processing Pipeline","text":"<p>Process data in multiple stages:</p> <pre><code># Stage 1: Fetch data\nFETCH_BATCH=\"data-pipeline-fetch-$(date +%s)\"\n\napflow run \"$FETCH_BATCH\" --tasks '[\n  {\n    \"id\": \"fetch-data\",\n    \"name\": \"Fetch Data from API\",\n    \"schemas\": {\"method\": \"http_executor\"},\n    \"inputs\": {\n      \"url\": \"https://api.example.com/data\",\n      \"method\": \"GET\"\n    }\n  }\n]'\n\n# Wait for completion\napflow tasks watch --task-id fetch-data\n\n# Stage 2: Process fetched data\nPROCESS_BATCH=\"data-pipeline-process-$(date +%s)\"\n\napflow run \"$PROCESS_BATCH\" --tasks '[\n  {\n    \"id\": \"process-data\",\n    \"name\": \"Process Data\",\n    \"schemas\": {\"method\": \"command_executor\"},\n    \"inputs\": {\n      \"command\": \"python process_data.py --input /tmp/data.json --output /tmp/processed.json\",\n      \"timeout\": 300\n    }\n  }\n]'\n\n# Stage 3: Archive results\nARCHIVE_BATCH=\"data-pipeline-archive-$(date +%s)\"\n\napflow run \"$ARCHIVE_BATCH\" --tasks '[\n  {\n    \"id\": \"archive-data\",\n    \"name\": \"Archive Results\",\n    \"schemas\": {\"method\": \"command_executor\"},\n    \"inputs\": {\n      \"command\": \"tar -czf /archive/data-$(date +%Y%m%d).tar.gz /tmp/processed.json\",\n      \"timeout\": 60\n    }\n  }\n]'\n\necho \"Pipeline complete\"\napflow tasks list --batch-id \"$ARCHIVE_BATCH\" --include-details\n</code></pre>"},{"location":"cli/examples/#scenario-3-monitoring-with-api-server","title":"Scenario 3: Monitoring with API Server","text":"<p>Setup CLI with API server for continuous monitoring:</p> <p>Step 1: Initialize configuration <pre><code>apflow config init-server\n\n# Creates:\n# - .data/config.cli.yaml with api_server_url and JWT token\n</code></pre></p> <p>Step 2: Start API server <pre><code># Terminal 1: Start API server\napflow daemon start --port 8000\n</code></pre></p> <p>Step 3: Create monitoring script <pre><code># Terminal 2: Monitoring script\n#!/bin/bash\n\nwhile true; do\n  # Execute monitoring task\n  BATCH_ID=\"monitor-$(date +%s)\"\n\n  apflow run \"$BATCH_ID\" --tasks '[\n    {\n      \"id\": \"monitor\",\n      \"name\": \"System Monitor\",\n      \"schemas\": {\"method\": \"system_info_executor\"},\n      \"inputs\": {\"resource\": \"cpu\"}\n    }\n  ]'\n\n  # Display current status\n  echo \"=== Status at $(date) ===\"\n  apflow tasks list --limit 5 --sort created_at --reverse\n\n  # Wait 30 seconds\n  sleep 30\ndone\n</code></pre></p> <p>Key point: CLI automatically routes through API server!</p>"},{"location":"cli/examples/#scenario-4-batch-job-with-retries","title":"Scenario 4: Batch Job with Retries","text":"<p>Process batch with automatic retry on failure:</p> <pre><code># Create batch of jobs\nBATCH_ID=\"batch-jobs-$(date +%Y%m%d_%H%M%S)\"\n\n# Array of tasks to process\nTASKS='[\n  {\n    \"id\": \"job-1\",\n    \"name\": \"Job 1\",\n    \"schemas\": {\"method\": \"http_executor\"},\n    \"inputs\": {\n      \"url\": \"https://api.example.com/job1\",\n      \"method\": \"POST\"\n    }\n  },\n  {\n    \"id\": \"job-2\",\n    \"name\": \"Job 2\",\n    \"schemas\": {\"method\": \"http_executor\"},\n    \"inputs\": {\n      \"url\": \"https://api.example.com/job2\",\n      \"method\": \"POST\"\n    }\n  },\n  {\n    \"id\": \"job-3\",\n    \"name\": \"Job 3\",\n    \"schemas\": {\"method\": \"http_executor\"},\n    \"inputs\": {\n      \"url\": \"https://api.example.com/job3\",\n      \"method\": \"POST\"\n    }\n  }\n]'\n\n# Execute with retry\napflow run \"$BATCH_ID\" --tasks \"$TASKS\" \\\n  --parallel-count 3 \\\n  --retry-count 5 \\\n  --retry-delay 10 \\\n  --timeout 3600\n\n# Monitor progress\necho \"Monitoring batch $BATCH_ID...\"\napflow tasks watch --batch-id \"$BATCH_ID\"\n\n# Show final results\necho \"Final Results:\"\napflow tasks list --batch-id \"$BATCH_ID\"\n</code></pre>"},{"location":"cli/examples/#scenario-5-task-dependency-chain","title":"Scenario 5: Task Dependency Chain","text":"<p>Execute tasks in sequence (one task per stage):</p> <pre><code>#!/bin/bash\n\n# Task 1: Prepare\necho \"Stage 1: Prepare...\"\nSTAGE1=$(apflow run stage-1 --tasks '[\n  {\n    \"id\": \"prepare\",\n    \"name\": \"Prepare Environment\",\n    \"schemas\": {\"method\": \"command_executor\"},\n    \"inputs\": {\n      \"command\": \"mkdir -p /tmp/workflow &amp;&amp; cd /tmp/workflow &amp;&amp; echo \\\"ready\\\" &gt; status.txt\"\n    }\n  }\n]')\n\n# Wait for completion\napflow tasks watch --task-id prepare\nSTATUS=$(apflow tasks status prepare --include-details)\nif echo \"$STATUS\" | grep -q '\"status\": \"failed\"'; then\n  echo \"Preparation failed!\"\n  exit 1\nfi\n\n# Task 2: Process\necho \"Stage 2: Process...\"\napflow run stage-2 --tasks '[\n  {\n    \"id\": \"process\",\n    \"name\": \"Process Data\",\n    \"schemas\": {\"method\": \"command_executor\"},\n    \"inputs\": {\n      \"command\": \"python /path/to/process.py\"\n    }\n  }\n]'\n\napflow tasks watch --task-id process\nSTATUS=$(apflow tasks status process --include-details)\nif echo \"$STATUS\" | grep -q '\"status\": \"failed\"'; then\n  echo \"Processing failed!\"\n  exit 1\nfi\n\n# Task 3: Finalize\necho \"Stage 3: Finalize...\"\napflow run stage-3 --tasks '[\n  {\n    \"id\": \"finalize\",\n    \"name\": \"Generate Report\",\n    \"schemas\": {\"method\": \"command_executor\"},\n    \"inputs\": {\n      \"command\": \"python /path/to/report.py\"\n    }\n  }\n]'\n\napflow tasks watch --task-id finalize\n\necho \"Workflow complete!\"\n</code></pre>"},{"location":"cli/examples/#advanced-usage-patterns","title":"Advanced Usage Patterns","text":""},{"location":"cli/examples/#pattern-1-conditional-execution","title":"Pattern 1: Conditional Execution","text":"<p>Execute next task based on previous task result:</p> <pre><code># Task 1\napflow run batch --tasks '[\n  {\n    \"id\": \"check-env\",\n    \"name\": \"Check Environment\",\n    \"schemas\": {\"method\": \"command_executor\"},\n    \"inputs\": {\n      \"command\": \"test -f /etc/important-file &amp;&amp; echo ok || echo missing\"\n    }\n  }\n]'\n\n# Get result\nRESULT=$(apflow tasks status check-env --include-details)\n\n# Conditional next task\nif echo \"$RESULT\" | grep -q \"ok\"; then\n  echo \"Environment OK, proceeding...\"\n  apflow run batch --tasks '[\n    {\n      \"id\": \"deploy\",\n      \"name\": \"Deploy\",\n      \"schemas\": {\"method\": \"command_executor\"},\n      \"inputs\": {\"command\": \"bash deploy.sh\"}\n    }\n  ]'\nelse\n  echo \"Environment missing required files\"\n  exit 1\nfi\n</code></pre>"},{"location":"cli/examples/#pattern-2-parallel-task-execution","title":"Pattern 2: Parallel Task Execution","text":"<p>Run multiple independent tasks in parallel:</p> <pre><code># Define 10 independent tasks\nTASKS=$(python3 &lt;&lt; 'EOF'\nimport json\n\ntasks = []\nfor i in range(10):\n    tasks.append({\n        \"id\": f\"task-{i:02d}\",\n        \"name\": f\"Process Item {i}\",\n        \"schemas\": {\"method\": \"http_executor\"},\n        \"inputs\": {\n            \"url\": f\"https://api.example.com/items/{i}\",\n            \"method\": \"POST\"\n        }\n    })\n\nprint(json.dumps(tasks))\nEOF\n)\n\n# Run with parallelism\napflow run batch --tasks \"$TASKS\" --parallel-count 5\n\n# Monitor all\napflow tasks watch --all\n</code></pre>"},{"location":"cli/examples/#pattern-3-dynamic-task-generation","title":"Pattern 3: Dynamic Task Generation","text":"<p>Generate tasks dynamically from external source:</p> <pre><code>#!/bin/bash\n\n# Fetch task list from API\nTASK_CONFIG=$(curl -s https://internal-api.example.com/tasks)\n\n# Transform to apflow format\nAPFLOW_TASKS=$(python3 &lt;&lt; EOF\nimport json\nimport sys\n\nconfig = json.loads('$TASK_CONFIG')\ntasks = []\n\nfor item in config['items']:\n    tasks.append({\n        \"id\": item['id'],\n        \"name\": item['name'],\n        \"schemas\": {\"method\": \"http_executor\"},\n        \"inputs\": {\n            \"url\": item['endpoint'],\n            \"method\": item.get('method', 'GET')\n        }\n    })\n\nprint(json.dumps(tasks))\nEOF\n)\n\n# Execute generated tasks\napflow run dynamic-batch --tasks \"$APFLOW_TASKS\" --parallel-count 3\n</code></pre>"},{"location":"cli/examples/#common-workflow-examples","title":"Common Workflow Examples","text":""},{"location":"cli/examples/#workflow-web-service-health-check","title":"Workflow: Web Service Health Check","text":"<pre><code>#!/bin/bash\n\n# Configuration\nSERVICES=(\n  \"https://api.example.com/health\"\n  \"https://db.example.com/status\"\n  \"https://cache.example.com/ping\"\n)\n\nTIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)\nBATCH_ID=\"health-check-$TIMESTAMP\"\n\n# Generate tasks\nTASKS=$(python3 &lt;&lt; 'EOF'\nimport json\nimport os\n\nservices = os.environ['SERVICES'].split(',')\ntasks = []\n\nfor i, service in enumerate(services):\n    tasks.append({\n        \"id\": f\"health-check-{i}\",\n        \"name\": f\"Check {service}\",\n        \"schemas\": {\"method\": \"http_executor\"},\n        \"inputs\": {\n            \"url\": service,\n            \"method\": \"GET\",\n            \"timeout\": 5\n        }\n    })\n\nprint(json.dumps(tasks))\nEOF\n)\n\nexport SERVICES=\"${SERVICES[*]}\"\n\n# Execute\napflow run \"$BATCH_ID\" --tasks \"$TASKS\" --parallel-count 3 --timeout 30\n\n# Report\necho \"=== Health Check Report ===\"\necho \"Batch ID: $BATCH_ID\"\necho \"Timestamp: $TIMESTAMP\"\necho \"\"\napflow tasks list --batch-id \"$BATCH_ID\"\n</code></pre>"},{"location":"cli/examples/#conclusion","title":"Conclusion","text":"<p>The CLI is flexible and powerful: - \u2705 Simple commands for common tasks - \u2705 Complex workflows through scripting - \u2705 Parallel execution for performance - \u2705 Retry logic for reliability - \u2705 Integration with existing tools via JSON - \u2705 Works standalone or with API server</p> <p>Start simple, build complexity gradually!</p>"},{"location":"development/contributing/","title":"Contributing to apflow","text":""},{"location":"development/contributing/#documentation-maintenance","title":"Documentation Maintenance","text":"<ul> <li>Before adding new documentation, check for existing files to avoid duplication.</li> <li>Use cross-links instead of repeating content across files.</li> <li>Major changes or new sections should be reviewed by a maintainer.</li> <li>Keep code snippets and API references up to date with the latest codebase.</li> <li>For large changes, update the Table of Contents and cross-references as needed.</li> </ul> <p>Thank you for your interest in contributing to apflow! This document provides guidelines and instructions for contributing.</p>"},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers and help them learn</li> <li>Focus on constructive feedback</li> <li>Respect different viewpoints and experiences</li> </ul>"},{"location":"development/contributing/#how-to-contribute","title":"How to Contribute","text":""},{"location":"development/contributing/#reporting-bugs","title":"Reporting Bugs","text":"<ol> <li>Check existing issues: Search GitHub Issues to see if the bug is already reported</li> <li>Create a new issue: If not found, create a new issue with:</li> <li>Clear title and description</li> <li>Steps to reproduce</li> <li>Expected vs actual behavior</li> <li>Environment details (Python version, OS, etc.)</li> <li>Error messages or logs</li> </ol>"},{"location":"development/contributing/#suggesting-features","title":"Suggesting Features","text":"<ol> <li>Check existing discussions: Search GitHub Discussions</li> <li>Create a feature request: Include:</li> <li>Use case and motivation</li> <li>Proposed solution</li> <li>Alternatives considered</li> <li>Impact on existing code</li> </ol>"},{"location":"development/contributing/#contributing-code","title":"Contributing Code","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> <li>Make your changes</li> <li>Write/update tests</li> <li>Ensure all tests pass: <code>pytest</code></li> <li>Update documentation if needed</li> <li>Commit your changes: Follow commit message guidelines</li> <li>Push to your fork: <code>git push origin feature/my-feature</code></li> <li>Create a Pull Request</li> </ol>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<p>See DEVELOPMENT.md for detailed setup instructions.</p> <p>Quick Setup: <pre><code># Clone your fork\ngit clone https://github.com/YOUR_USERNAME/apflow.git\ncd apflow\n\n# Create virtual environment\npython3.10+ -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode\npip install -e \".[all,dev]\"\n\n# Run tests\npytest\n</code></pre></p>"},{"location":"development/contributing/#code-style","title":"Code Style","text":""},{"location":"development/contributing/#python-code","title":"Python Code","text":"<p>We use: - Black for code formatting (line length: 100) - Ruff for linting - mypy for type checking (optional, not strict)</p> <p>Format code: <pre><code>black src/ tests/\nruff check src/ tests/\n</code></pre></p> <p>Configuration: See <code>pyproject.toml</code> for tool settings.</p>"},{"location":"development/contributing/#code-style-guidelines","title":"Code Style Guidelines","text":"<ol> <li> <p>Type Hints: Use type hints for function parameters and return values    <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    ...\n</code></pre></p> </li> <li> <p>Docstrings: Use Google-style docstrings    <pre><code>def my_function(param: str) -&gt; int:\n    \"\"\"\n    Brief description.\n\n    Args:\n        param: Parameter description\n\n    Returns:\n        Return value description\n    \"\"\"\n</code></pre></p> </li> <li> <p>Naming Conventions:</p> </li> <li>Classes: <code>PascalCase</code></li> <li>Functions/Methods: <code>snake_case</code></li> <li>Constants: <code>UPPER_SNAKE_CASE</code></li> <li> <p>Private: Prefix with <code>_</code></p> </li> <li> <p>Imports: Organize imports:    <pre><code># Standard library\nimport asyncio\nfrom typing import Dict, Any\n\n# Third-party\nimport aiohttp\nfrom pydantic import BaseModel\n\n# Local\nfrom apflow import ExecutableTask\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=src/apflow --cov-report=html\n\n# Run specific test file\npytest tests/core/execution/test_task_manager.py\n\n# Run with verbose output\npytest -v\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<ol> <li>Test files: Place in <code>tests/</code> directory, mirroring source structure</li> <li>Test functions: Prefix with <code>test_</code></li> <li>Use fixtures: See <code>tests/conftest.py</code> for available fixtures</li> <li>Async tests: Use <code>@pytest.mark.asyncio</code> for async functions</li> <li>Test isolation: Ensure tests don't depend on execution order or side effects from other tests</li> </ol> <p>Example: <pre><code>import pytest\nfrom apflow import TaskManager, create_session\n\n@pytest.mark.asyncio\nasync def test_task_creation():\n    \"\"\"Test task creation\"\"\"\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    task = await task_manager.task_repository.create_task(\n        name=\"test_task\",\n        user_id=\"test_user\"\n    )\n\n    assert task.id is not None\n    assert task.name == \"test_task\"\n</code></pre></p>"},{"location":"development/contributing/#test-isolation-best-practices","title":"Test Isolation Best Practices","text":"<p>When writing tests that use the extension registry or other global state:</p> <ol> <li> <p>Use <code>autouse=True</code> fixtures to ensure required setup runs before each test:    <pre><code>@pytest.fixture(autouse=True)\ndef ensure_executor_registered():\n    \"\"\"Ensure custom executor is registered before each test\"\"\"\n    from apflow.core.extensions import get_registry\n\n    registry = get_registry()\n    if not registry.is_registered(\"custom_executor\"):\n        registry.register(\n            extension=CustomExecutor(),\n            executor_class=CustomExecutor,\n            override=True\n        )\n    yield\n</code></pre></p> </li> <li> <p>Skip tests when optional dependencies are unavailable:    <pre><code>from apflow.extensions.llm.llm_executor import LITELLM_AVAILABLE\n\n@pytest.mark.skipif(not LITELLM_AVAILABLE, reason=\"litellm not installed\")\n@pytest.mark.asyncio\nasync def test_llm_feature():\n    # Test code that requires litellm\n    pass\n</code></pre></p> </li> <li> <p>Clean up after tests to avoid affecting other tests (fixtures handle this automatically for database sessions)</p> </li> </ol>"},{"location":"development/contributing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Aim for high test coverage (&gt;80%)</li> <li>Focus on critical paths and edge cases</li> <li>Test both success and failure scenarios</li> </ul>"},{"location":"development/contributing/#import-performance-checks","title":"Import Performance Checks","text":"<p>apflow includes automated tools to prevent slow imports and circular dependencies. These checks run automatically in CI/CD and as pre-commit hooks.</p>"},{"location":"development/contributing/#available-checks","title":"Available Checks","text":"<pre><code># Check all import issues\nmake check-imports\n\n# Individual checks\nmake check-circular      # Detect circular imports\nmake check-performance   # Check import time\nmake check-heavy        # Detect heavy module-level imports\n</code></pre>"},{"location":"development/contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to automatically check on every commit:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>This will automatically run: - \u2713 Circular import detection - \u2713 Heavy dependency detection (litellm, crewai, etc.) - \u2713 Code formatting (ruff)</p>"},{"location":"development/contributing/#best-practices","title":"Best Practices","text":"<p>\u274c Avoid: <pre><code># Module-level heavy imports\nimport litellm  # Slows down CLI startup!\n\ndef simple_function():\n    ...  # Doesn't even use litellm\n</code></pre></p> <p>\u2705 Prefer: <pre><code># Lazy import when needed\ndef llm_function():\n    import litellm  # Import only when called\n    ...\n</code></pre></p> <p>See Import Performance Guide for detailed guidelines.</p>"},{"location":"development/contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow Conventional Commits format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre> <p>Types: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation changes - <code>style</code>: Code style changes (formatting, etc.) - <code>refactor</code>: Code refactoring - <code>test</code>: Test additions/changes - <code>chore</code>: Maintenance tasks</p> <p>Examples: <pre><code>feat(api): Add task cancellation endpoint\n\nAdd POST /tasks/cancel endpoint to support task cancellation.\nIncludes cancellation status tracking and cleanup.\n\nCloses #123\n</code></pre></p> <pre><code>fix(executor): Handle None inputs gracefully\n\nPreviously, None inputs would cause AttributeError.\nNow returns empty dict as default.\n\nFixes #456\n</code></pre>"},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"development/contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Update CHANGELOG.md: Add entry under <code>[Unreleased]</code></li> <li>Update documentation: If adding features</li> <li>Run tests: Ensure all tests pass</li> <li>Check code style: Run <code>black</code> and <code>ruff</code></li> <li>Update type hints: If changing function signatures</li> </ol>"},{"location":"development/contributing/#pr-description-template","title":"PR Description Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New feature\n- [ ] Breaking change\n- [ ] Documentation update\n\n## Testing\n- [ ] Tests added/updated\n- [ ] All tests pass\n- [ ] Manual testing completed\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Documentation updated\n- [ ] CHANGELOG.md updated\n- [ ] Tests added/updated\n- [ ] No breaking changes (or documented)\n</code></pre>"},{"location":"development/contributing/#review-process","title":"Review Process","text":"<ol> <li>Automated checks: CI will run tests and linting</li> <li>Code review: Maintainers will review your PR</li> <li>Address feedback: Make requested changes</li> <li>Merge: Once approved, maintainers will merge</li> </ol>"},{"location":"development/contributing/#project-structure","title":"Project Structure","text":"<pre><code>apflow/\n\u251c\u2500\u2500 src/apflow/    # Source code\n\u2502   \u251c\u2500\u2500 core/               # Core framework\n\u2502   \u251c\u2500\u2500 extensions/          # Extensions (crewai, stdio, etc.)\n\u2502   \u251c\u2500\u2500 api/                # API server\n\u2502   \u2514\u2500\u2500 cli/                # CLI tools\n\u251c\u2500\u2500 tests/                   # Test suite\n\u251c\u2500\u2500 docs/                    # Documentation\n\u2514\u2500\u2500 scripts/                 # Utility scripts\n</code></pre> <p>See DIRECTORY_STRUCTURE.md for details.</p>"},{"location":"development/contributing/#areas-for-contribution","title":"Areas for Contribution","text":""},{"location":"development/contributing/#high-priority","title":"High Priority","text":"<ol> <li>Documentation: Improve examples, tutorials, API docs</li> <li>Tests: Increase test coverage, add integration tests</li> <li>Examples: Add more practical examples</li> <li>Error Messages: Improve error messages and debugging info</li> </ol>"},{"location":"development/contributing/#feature-areas","title":"Feature Areas","text":"<ol> <li>New Executors: Create executors for different use cases</li> <li>Storage Backends: Add support for more databases</li> <li>Monitoring: Add observability and monitoring features</li> <li>Performance: Optimize task execution and storage</li> </ol>"},{"location":"development/contributing/#good-first-issues","title":"Good First Issues","text":"<p>Look for issues tagged with <code>good-first-issue</code> on GitHub.</p>"},{"location":"development/contributing/#questions","title":"Questions?","text":"<ul> <li>Documentation: Check docs/</li> <li>Discussions: GitHub Discussions</li> <li>Issues: GitHub Issues</li> </ul>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache-2.0 license.</p>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors will be: - Listed in CONTRIBUTORS.md (if we create one) - Acknowledged in release notes for significant contributions - Thanked in the project README</p> <p>Thank you for contributing to apflow! \ud83c\udf89</p>"},{"location":"development/distributed-development/","title":"Distributed Core \u2014 Architecture &amp; Development Design","text":"<p>Scope: Development-focused design for Distributed Core, based on <code>docs/development/distributed-orchestration-design.md</code> and current <code>src/</code> architecture. This document is implementation\u2011oriented and aligned with existing TaskManager/TaskExecutor patterns.</p>"},{"location":"development/distributed-development/#1-objectives-nongoals","title":"1. Objectives &amp; Non\u2011Goals","text":""},{"location":"development/distributed-development/#objectives","title":"Objectives","text":"<ul> <li>Enable multi\u2011node task execution with centralized coordination.</li> <li>Provide lease\u2011based task assignment with automatic expiry and reassignment.</li> <li>Maintain data consistency via single\u2011writer leader (no consensus layer).</li> <li>Use one codebase for all nodes (no separate coordinator/worker binaries).</li> <li>Support auto\u2011scaling by adding nodes without code changes.</li> <li>Allow leader promotion via configuration and automatic leader election.</li> <li>Preserve existing public APIs where possible.</li> <li>Support placement constraints and executor compatibility.</li> <li>Provide idempotent execution and retries without duplicate side effects.</li> </ul>"},{"location":"development/distributed-development/#nongoals","title":"Non\u2011Goals","text":"<ul> <li>Fully decentralized consensus (Raft/Paxos).</li> <li>Multi\u2011tenancy or RBAC.</li> <li>Built\u2011in secrets management.</li> <li>Cross\u2011region orchestration.</li> </ul>"},{"location":"development/distributed-development/#2-single-codebase-dynamic-roles","title":"2. Single Codebase, Dynamic Roles","text":"<p>All nodes run the same code. Role is determined at runtime by configuration and leader election.</p>"},{"location":"development/distributed-development/#roles-runtime-not-separate-deployments","title":"Roles (runtime, not separate deployments)","text":"<ul> <li>Leader (single writer)</li> <li>Owns task state writes in PostgreSQL.</li> <li>Handles lease acquisition/renewal and reassignment.</li> <li>Runs cleanup for expired leases.</li> <li> <p>Serves read/write API endpoints.</p> </li> <li> <p>Worker (stateless executor)</p> </li> <li>Polls for executable tasks.</li> <li>Acquires lease, executes, reports result.</li> <li> <p>Renews lease during long\u2011running tasks.</p> </li> <li> <p>Observer (read\u2011only)</p> </li> <li>Serves CLI or dashboard traffic.</li> <li>Optionally runs a local cache synced from leader.</li> </ul>"},{"location":"development/distributed-development/#role-selection-policy","title":"Role selection policy","text":"<ul> <li>Default (no config required): automatic leader election via the leader lease table.</li> <li>Optional override via config:</li> <li><code>APFLOW_NODE_ROLE=auto</code> (default if set): attempt leader election; fall back to worker if leader is taken.</li> <li><code>APFLOW_NODE_ROLE=leader</code>: force leader (fail if leadership cannot be acquired).</li> <li><code>APFLOW_NODE_ROLE=worker</code>: never try to become leader.</li> <li><code>APFLOW_NODE_ROLE=observer</code>: read\u2011only endpoints only.</li> </ul>"},{"location":"development/distributed-development/#3-architecture-overview","title":"3. Architecture Overview","text":"<p>Centralized coordination, decentralized execution with dynamic leadership: - A single leader writes task state in PostgreSQL. - All other nodes can execute tasks as workers. - Leadership can be promoted by config or acquired automatically. - Failures are recovered by lease expiry and reassignment.</p> <p>This avoids consensus complexity while supporting auto\u2011scaling and leader failover.</p>"},{"location":"development/distributed-development/#31-architecture-philosophy-rationale","title":"3.1 Architecture Philosophy (Rationale)","text":""},{"location":"development/distributed-development/#why-not-full-distributed-consensus","title":"Why not full distributed consensus?","text":"<p>Distributed consensus (Raft/Paxos) adds significant complexity without clear benefit for task orchestration. A single\u2011writer leader provides strong consistency, while external HA (k8s/HAProxy) provides acceptable availability.</p>"},{"location":"development/distributed-development/#why-leasebased-not-lockbased","title":"Why lease\u2011based, not lock\u2011based?","text":"<p>Leases auto\u2011expire on node failure and prevent deadlocks. A lease renewal loop is simpler and safer than lock release semantics for distributed workers.</p>"},{"location":"development/distributed-development/#why-postgresql","title":"Why PostgreSQL?","text":"<p>PostgreSQL provides the transactional primitives required for lease acquisition/renewal. DuckDB is single\u2011writer and is therefore unsuitable for multi\u2011node coordination.</p>"},{"location":"development/distributed-development/#4-data-model-extensions-postgresqlonly","title":"4. Data Model Extensions (PostgreSQL\u2011only)","text":""},{"location":"development/distributed-development/#task-fields-extend-taskmodel","title":"Task Fields (extend <code>TaskModel</code>)","text":"<ul> <li><code>lease_id: Optional[str]</code></li> <li><code>lease_expires_at: Optional[datetime]</code></li> <li><code>placement_constraints: Optional[dict]</code></li> <li><code>attempt_id: int</code> (increment on retry)</li> <li><code>idempotency_key: Optional[str]</code></li> <li><code>last_assigned_node: Optional[str]</code></li> </ul>"},{"location":"development/distributed-development/#new-tables","title":"New Tables","text":"<p>Nodes registry <pre><code> distributed_nodes(\n   node_id TEXT PRIMARY KEY,\n   executor_types TEXT[],\n   capabilities JSONB,\n   status ENUM('healthy','stale','dead'),\n   heartbeat_at TIMESTAMP,\n   registered_at TIMESTAMP\n )\n</code></pre></p> <p>Task leases <pre><code> task_leases(\n   task_id TEXT PRIMARY KEY REFERENCES apflow_tasks(id),\n   node_id TEXT REFERENCES distributed_nodes(node_id),\n   lease_token TEXT UNIQUE,\n   acquired_at TIMESTAMP,\n   expires_at TIMESTAMP\n )\n</code></pre></p> <p>Idempotency cache <pre><code> execution_idempotency(\n   task_id TEXT,\n   attempt_id INT,\n   idempotency_key TEXT UNIQUE,\n   result JSONB,\n   status ENUM('pending','completed','failed'),\n   created_at TIMESTAMP,\n   PRIMARY KEY (task_id, attempt_id)\n )\n</code></pre></p> <p>Task events (optional, observability) <pre><code> task_events(\n   event_id UUID PRIMARY KEY,\n   task_id TEXT REFERENCES apflow_tasks(id),\n   event_type ENUM('created','assigned','started','completed','failed','reassigned','cancelled'),\n   node_id TEXT,\n   details JSONB,\n   timestamp TIMESTAMP DEFAULT NOW()\n )\n</code></pre></p> <p>Leader lease (single\u2011writer election) <pre><code> cluster_leader(\n   leader_id TEXT PRIMARY KEY,\n   lease_token TEXT UNIQUE,\n   acquired_at TIMESTAMP,\n   expires_at TIMESTAMP\n )\n</code></pre></p>"},{"location":"development/distributed-development/#leader-election-algorithm-sqlbased-no-config-required","title":"Leader election algorithm (SQL\u2011based, no config required)","text":"<p>Acquire leadership (atomic): <pre><code>INSERT INTO cluster_leader (leader_id, lease_token, acquired_at, expires_at)\nVALUES (:node_id, :lease_token, NOW(), NOW() + INTERVAL '30 seconds')\nON CONFLICT (leader_id) DO NOTHING;\n</code></pre></p> <p>Renew leadership (only if you still own it): <pre><code>UPDATE cluster_leader\nSET expires_at = NOW() + INTERVAL '30 seconds'\nWHERE leader_id = :node_id AND lease_token = :lease_token;\n</code></pre></p> <p>Release leadership (graceful shutdown): <pre><code>DELETE FROM cluster_leader\nWHERE leader_id = :node_id AND lease_token = :lease_token;\n</code></pre></p> <p>Acquire when stale: <pre><code>DELETE FROM cluster_leader WHERE expires_at &lt; NOW();\n-- then attempt acquire again\n</code></pre></p> <p>Notes: - The first successful insert wins leadership. - No config is required; all nodes can attempt leadership on startup and periodically after lease expiry. - Leadership remains exclusive as long as lease renewal succeeds.</p>"},{"location":"development/distributed-development/#5-core-apis-leader","title":"5. Core APIs (Leader)","text":"<p>Protocol note: The primary API surface is the A2A server using JSON\u2011RPC (e.g., <code>POST /</code> with JSON\u2011RPC payloads). Distributed endpoints should follow the same A2A JSON\u2011RPC conventions.</p>"},{"location":"development/distributed-development/#node-management","title":"Node Management","text":"<ul> <li><code>register_node(node_id, capabilities, executor_types)</code></li> <li><code>heartbeat(node_id)</code></li> <li><code>deregister_node(node_id)</code></li> </ul>"},{"location":"development/distributed-development/#task-assignment","title":"Task Assignment","text":"<ul> <li><code>find_executable_tasks(node_id)</code></li> <li><code>acquire_lease(task_id, node_id)</code></li> <li><code>renew_lease(lease_token)</code></li> <li><code>release_lease(task_id)</code></li> </ul>"},{"location":"development/distributed-development/#execution-reporting","title":"Execution Reporting","text":"<ul> <li><code>report_completion(task_id, node_id, result, idempotency_key)</code></li> </ul>"},{"location":"development/distributed-development/#recovery","title":"Recovery","text":"<ul> <li><code>recover_stale_leases()</code></li> </ul>"},{"location":"development/distributed-development/#6-placement-constraints","title":"6. Placement Constraints","text":"<p>A task may specify <code>placement_constraints</code>: - <code>requires_executors: list[str]</code> - <code>requires_capabilities: dict</code> (e.g., <code>{\"gpu\": \"nvidia\"}</code>) - <code>allowed_nodes: Optional[list[str]]</code> - <code>forbidden_nodes: Optional[list[str]]</code> - <code>max_parallel_per_node: int</code></p> <p>Placement is enforced by coordinator when returning executable tasks.</p>"},{"location":"development/distributed-development/#7-execution-flow-worker","title":"7. Execution Flow (Worker)","text":"<ol> <li>Worker polls <code>find_executable_tasks(node_id)</code>.</li> <li>Attempts <code>acquire_lease(task_id)</code>.</li> <li>On success, executes task locally.</li> <li>Renews lease periodically during long tasks.</li> <li>Reports completion (<code>report_completion</code>) with idempotency key.</li> <li>Coordinator persists result and releases lease.</li> </ol>"},{"location":"development/distributed-development/#71-failure-scenarios-examples","title":"7.1 Failure Scenarios (Examples)","text":""},{"location":"development/distributed-development/#worker-crash-during-execution","title":"Worker crash during execution","text":"<ul> <li>Lease expires.</li> <li>Leader reverts task to pending and increments <code>attempt_id</code>.</li> <li>Another worker acquires the lease and retries (idempotency safe).</li> </ul>"},{"location":"development/distributed-development/#longrunning-task","title":"Long\u2011running task","text":"<ul> <li>Worker renews lease periodically.</li> <li>If renewal fails, task becomes eligible for reassignment after expiry.</li> </ul>"},{"location":"development/distributed-development/#leader-failure","title":"Leader failure","text":"<ul> <li>No new leases can be granted until leader is re\u2011elected.</li> <li>Workers can continue current tasks; expired tasks are recovered by the next leader.</li> </ul>"},{"location":"development/distributed-development/#8-idempotency-strategy","title":"8. Idempotency Strategy","text":"<ul> <li><code>idempotency_key = hash(task_id + attempt_id + inputs)</code>.</li> <li>If record exists with <code>completed</code>, return cached result instead of re\u2011executing.</li> <li>Prevents duplicate side effects under retry or reassignment.</li> </ul>"},{"location":"development/distributed-development/#9-module-design-proposed","title":"9. Module Design (Proposed)","text":""},{"location":"development/distributed-development/#new-modules","title":"New Modules","text":"<ul> <li><code>src/apflow/core/distributed/node_registry.py</code></li> <li><code>src/apflow/core/distributed/lease_manager.py</code></li> <li><code>src/apflow/core/distributed/placement.py</code></li> <li><code>src/apflow/core/distributed/idempotency.py</code></li> <li><code>src/apflow/core/distributed/leader_election.py</code></li> <li><code>src/apflow/core/distributed/runtime.py</code> (role selection + lifecycle)</li> </ul>"},{"location":"development/distributed-development/#modified-modules","title":"Modified Modules","text":"<ul> <li><code>core/execution/task_manager.py</code></li> <li>integrate distributed execution pathway (leader/worker runtime)</li> <li><code>core/execution/task_executor.py</code></li> <li>distributed runtime hooks (role\u2011aware entrypoints)</li> <li><code>core/storage/sqlalchemy/models.py</code></li> <li>add distributed fields</li> <li><code>core/storage/sqlalchemy/migrations/</code></li> <li>add migration for new tables and fields</li> </ul>"},{"location":"development/distributed-development/#10-configuration","title":"10. Configuration","text":""},{"location":"development/distributed-development/#cluster-enablement-optional","title":"Cluster Enablement (optional)","text":"<pre><code>APFLOW_CLUSTER_ENABLED=true\nAPFLOW_NODE_ID=node-1\nAPFLOW_NODE_ROLE=auto   # optional override: auto | leader | worker | observer\n</code></pre>"},{"location":"development/distributed-development/#leader-election-leases","title":"Leader Election &amp; Leases","text":"<pre><code>APFLOW_LEADER_LEASE_SECONDS=30\nAPFLOW_LEADER_RENEW_SECONDS=10\nAPFLOW_LEASE_DURATION_SECONDS=30\nAPFLOW_LEASE_CLEANUP_INTERVAL_SECONDS=10\n</code></pre>"},{"location":"development/distributed-development/#worker-polling","title":"Worker Polling","text":"<pre><code>APFLOW_POLL_INTERVAL_SECONDS=5\nAPFLOW_MAX_PARALLEL_TASKS_PER_NODE=4\n</code></pre>"},{"location":"development/distributed-development/#default-behavior-no-config","title":"Default Behavior (no config)","text":"<ul> <li>If distributed is enabled and PostgreSQL is configured, nodes use the leader lease table to elect a leader automatically.</li> <li>All nodes run the same code and can self\u2011promote when leadership becomes available.</li> <li>Config is only needed for explicit overrides or operational constraints.</li> </ul>"},{"location":"development/distributed-development/#11-runtime-state-machine","title":"11. Runtime State Machine","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Boot\n    Boot --&gt; AutoElection: distributed enabled\n    AutoElection --&gt; Leader: lease acquired\n    AutoElection --&gt; Worker: lease held by others\n\n    Leader --&gt; Leader: renew leader lease\n    Leader --&gt; Worker: lease renewal failed\n    Worker --&gt; AutoElection: periodic retry\n\n    Leader --&gt; Observer: config override\n    Worker --&gt; Observer: config override\n    Observer --&gt; AutoElection: override removed\n\n    note right of Leader\n      Single writer\n      Manages task leases\n    end note\n\n    note right of Worker\n      Executes tasks\n      Renews task leases\n    end note</code></pre>"},{"location":"development/distributed-development/#storage-requirements","title":"Storage Requirements","text":"<ul> <li>Distributed mode requires PostgreSQL (DuckDB is single-writer and unsuitable for multi-node coordination)</li> <li>Single-node mode supports both PostgreSQL and DuckDB (backward compatible)</li> <li>Migration scripts should detect dialect and skip distributed tables for DuckDB</li> </ul>"},{"location":"development/distributed-development/#12-implementation-plan-phased","title":"12. Implementation Plan (Phased)","text":"<p>Implementation Status: Based on analysis of current <code>apflow</code> v0.16.0 architecture. This plan provides detailed implementation guidance with code examples and integration points.</p>"},{"location":"development/distributed-development/#architecture-integration-analysis","title":"Architecture Integration Analysis","text":""},{"location":"development/distributed-development/#strong-alignment-with-current-architecture","title":"Strong Alignment with Current Architecture \u2705","text":"<ul> <li>Storage abstraction: <code>TaskRepository</code> already abstracts DB operations \u2014 distributed writes can be isolated to leader</li> <li>Hook system: Pre/post hooks work perfectly for lease renewal and idempotency checks</li> <li>Session management: <code>SessionProxy</code> supports both sync and async \u2014 leader services can use async</li> <li>Extension registry: Distributed services can be registered as extensions</li> <li>A2A protocol: JSON-RPC pattern already established \u2014 distributed endpoints fit naturally</li> </ul>"},{"location":"development/distributed-development/#areas-requiring-careful-design","title":"Areas Requiring Careful Design \u26a0\ufe0f","text":"<ul> <li>TaskExecutor singleton: Currently assumes local execution \u2014 needs worker runtime mode</li> <li>Task tree execution: <code>distribute_task_tree_recursive()</code> assumes local dependency resolution</li> <li>Streaming: <code>EventQueue</code> is in-memory \u2014 needs distributed event broadcasting (Phase 5)</li> <li>Cancellation: Requires executor instance reference \u2014 needs distributed cancellation protocol</li> <li>Progress tracking: <code>TaskTracker</code> is local \u2014 needs cluster-wide tracking</li> </ul>"},{"location":"development/distributed-development/#phase-0-preparation-week-1","title":"Phase 0 \u2014 Preparation (Week 1)","text":"<p>Goal: Ensure foundation is ready for distributed changes</p> <p>Tasks: 1. Ensure all existing tests pass on PostgreSQL with connection pooling 2. Refactor TaskExecutor for testability (extract execution logic from singleton) 3. Document current execution flow (sequence diagrams)</p> <p>Acceptance Criteria: - All tests pass on PostgreSQL - TaskExecutor can be tested with mocked dependencies - Clear documentation of current execution flow</p>"},{"location":"development/distributed-development/#phase-1-storage-schema-weeks-1-2","title":"Phase 1 \u2014 Storage &amp; Schema (Weeks 1-2)","text":"<p>Goal: Add distributed tables and fields without changing behavior</p>"},{"location":"development/distributed-development/#database-schema-changes","title":"Database Schema Changes","text":"<p>File: <code>src/apflow/core/storage/sqlalchemy/models.py</code></p> <p>Add these new models: <pre><code>class DistributedNode(Base):\n    \"\"\"Distributed node registry\"\"\"\n    __tablename__ = \"apflow_distributed_nodes\"\n    node_id: Mapped[str] = mapped_column(String(100), primary_key=True)\n    executor_types: Mapped[list[str]] = mapped_column(JSON)\n    capabilities: Mapped[dict] = mapped_column(JSON, default=dict)\n    status: Mapped[str] = mapped_column(String(20))  # healthy, stale, dead\n    heartbeat_at: Mapped[datetime] = mapped_column(DateTime(timezone=True))\n    registered_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=func.now())\n\nclass TaskLease(Base):\n    \"\"\"Task execution leases\"\"\"\n    __tablename__ = \"apflow_task_leases\"\n    task_id: Mapped[str] = mapped_column(String(100), ForeignKey(\"apflow_tasks.id\"), primary_key=True)\n    node_id: Mapped[str] = mapped_column(String(100), ForeignKey(\"apflow_distributed_nodes.node_id\"))\n    lease_token: Mapped[str] = mapped_column(String(100), unique=True, index=True)\n    acquired_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=func.now())\n    expires_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), index=True)\n    attempt_id: Mapped[int] = mapped_column(Integer, default=0)\n\nclass ExecutionIdempotency(Base):\n    \"\"\"Idempotency tracking for retries\"\"\"\n    __tablename__ = \"apflow_execution_idempotency\"\n    task_id: Mapped[str] = mapped_column(String(100), primary_key=True)\n    attempt_id: Mapped[int] = mapped_column(Integer, primary_key=True)\n    idempotency_key: Mapped[str] = mapped_column(String(255), unique=True, index=True)\n    result: Mapped[dict] = mapped_column(JSON, nullable=True)\n    status: Mapped[str] = mapped_column(String(20))  # pending, completed, failed\n    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=func.now())\n\nclass ClusterLeader(Base):\n    \"\"\"Leader election lease (single row table)\"\"\"\n    __tablename__ = \"apflow_cluster_leader\"\n    leader_id: Mapped[str] = mapped_column(String(100), primary_key=True, default=\"singleton\")\n    node_id: Mapped[str] = mapped_column(String(100), ForeignKey(\"apflow_distributed_nodes.node_id\"))\n    lease_token: Mapped[str] = mapped_column(String(100), unique=True)\n    acquired_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=func.now())\n    expires_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), index=True)\n\nclass TaskEvent(Base):\n    \"\"\"Task lifecycle events for observability\"\"\"\n    __tablename__ = \"apflow_task_events\"\n    event_id: Mapped[str] = mapped_column(String(100), primary_key=True, default=lambda: str(uuid.uuid4()))\n    task_id: Mapped[str] = mapped_column(String(100), ForeignKey(\"apflow_tasks.id\", ondelete=\"CASCADE\"), index=True)\n    event_type: Mapped[str] = mapped_column(String(50))\n    node_id: Mapped[str] = mapped_column(String(100), nullable=True)\n    details: Mapped[dict] = mapped_column(JSON, default=dict)\n    timestamp: Mapped[datetime] = mapped_column(DateTime(timezone=True), default=func.now(), index=True)\n</code></pre></p> <p>Extend <code>TaskModel</code> with nullable distributed fields: <pre><code># Add to TaskModel (backward compatible)\nlease_id: Mapped[str | None] = mapped_column(String(100), nullable=True)\nlease_expires_at: Mapped[datetime | None] = mapped_column(DateTime(timezone=True), nullable=True)\nplacement_constraints: Mapped[dict | None] = mapped_column(JSON, nullable=True)\nattempt_id: Mapped[int] = mapped_column(Integer, default=0)\nidempotency_key: Mapped[str | None] = mapped_column(String(255), nullable=True)\nlast_assigned_node: Mapped[str | None] = mapped_column(String(100), nullable=True)\n</code></pre></p>"},{"location":"development/distributed-development/#migration-strategy","title":"Migration Strategy","text":"<p>File: <code>src/apflow/core/storage/sqlalchemy/migrations/versions/00XX_add_distributed_support.py</code></p> <p>Critical: Migration must be dialect-aware to maintain DuckDB support in single-node mode:</p> <pre><code>def upgrade() -&gt; None:\n    \"\"\"Add distributed tables (PostgreSQL only)\"\"\"\n    conn = op.get_bind()\n    dialect_name = conn.dialect.name\n\n    if dialect_name == \"duckdb\":\n        # Skip distributed tables for DuckDB\n        # Only add nullable columns to TaskModel\n        op.add_column('apflow_tasks', sa.Column('attempt_id', sa.Integer(), server_default='0', nullable=False))\n        op.add_column('apflow_tasks', sa.Column('lease_id', sa.String(100), nullable=True))\n        op.add_column('apflow_tasks', sa.Column('lease_expires_at', sa.DateTime(timezone=True), nullable=True))\n        op.add_column('apflow_tasks', sa.Column('placement_constraints', sa.JSON(), nullable=True))\n        op.add_column('apflow_tasks', sa.Column('idempotency_key', sa.String(255), nullable=True))\n        op.add_column('apflow_tasks', sa.Column('last_assigned_node', sa.String(100), nullable=True))\n        return\n\n    # PostgreSQL: create all distributed tables\n    op.create_table(\n        'apflow_distributed_nodes',\n        # ... (as shown above)\n    )\n    op.create_table('apflow_task_leases', ...)\n    op.create_table('apflow_execution_idempotency', ...)\n    op.create_table('apflow_cluster_leader', ...)\n    op.create_table('apflow_task_events', ...)\n\n    # Add distributed columns to TaskModel\n    op.add_column('apflow_tasks', sa.Column('attempt_id', sa.Integer(), server_default='0', nullable=False))\n    # ... (other columns)\n\ndef downgrade() -&gt; None:\n    \"\"\"Rollback distributed changes\"\"\"\n    conn = op.get_bind()\n    dialect_name = conn.dialect.name\n\n    # Drop columns from TaskModel (both dialects)\n    op.drop_column('apflow_tasks', 'last_assigned_node')\n    # ... (other columns)\n\n    if dialect_name == \"postgresql\":\n        # Drop distributed tables (PostgreSQL only)\n        op.drop_table('apflow_task_events')\n        op.drop_table('apflow_cluster_leader')\n        op.drop_table('apflow_execution_idempotency')\n        op.drop_table('apflow_task_leases')\n        op.drop_table('apflow_distributed_nodes')\n</code></pre> <p>Testing: - Test migration on empty PostgreSQL database - Test migration on PostgreSQL database with existing tasks - Test migration on DuckDB (should only add columns, skip distributed tables) - Verify rollback works correctly on both dialects - Verify single-node mode works on DuckDB after migration</p>"},{"location":"development/distributed-development/#configuration-extension","title":"Configuration Extension","text":"<p>File: <code>src/apflow/core/config/registry.py</code></p> <pre><code>@dataclass\nclass DistributedConfig:\n    \"\"\"Distributed cluster configuration\"\"\"\n    enabled: bool = False\n    node_id: str | None = None\n    node_role: str = \"auto\"  # auto | leader | worker | observer\n\n    # Leader election\n    leader_lease_seconds: int = 30\n    leader_renew_seconds: int = 10\n\n    # Task leases\n    lease_duration_seconds: int = 30\n    lease_cleanup_interval_seconds: int = 10\n\n    # Worker\n    poll_interval_seconds: int = 5\n    max_parallel_tasks_per_node: int = 4\n\n    # Heartbeat\n    heartbeat_interval_seconds: int = 10\n    node_stale_threshold_seconds: int = 30\n    node_dead_threshold_seconds: int = 120\n\ndef set_distributed_config(config: DistributedConfig): ...\ndef get_distributed_config() -&gt; DistributedConfig: ...\n</code></pre>"},{"location":"development/distributed-development/#runtime-validation","title":"Runtime Validation","text":"<p>File: <code>src/apflow/core/distributed/runtime.py</code></p> <p>Add validation when distributed mode is enabled:</p> <pre><code>class DistributedRuntime:\n    def __init__(self, config: DistributedConfig, session_factory, executor):\n        # Validate database dialect\n        dialect = get_dialect()\n        if dialect == \"duckdb\":\n            raise ValueError(\n                \"Distributed mode requires PostgreSQL. \"\n                \"DuckDB is single-writer and does not support multi-node coordination. \"\n                \"To use DuckDB, disable distributed mode (APFLOW_CLUSTER_ENABLED=false).\"\n            )\n        if dialect != \"postgresql\":\n            raise ValueError(f\"Distributed mode requires PostgreSQL, got: {dialect}\")\n\n        # ... rest of initialization\n</code></pre> <p>Acceptance Criteria: - [ ] Migration runs successfully on PostgreSQL and DuckDB - [ ] Existing tasks remain accessible - [ ] No behavior changes in single-node mode - [ ] All existing tests pass</p>"},{"location":"development/distributed-development/#phase-2-leader-services-weeks-3-5","title":"Phase 2 \u2014 Leader Services (Weeks 3-5)","text":"<p>Goal: Implement leader-only services (no execution changes yet)</p>"},{"location":"development/distributed-development/#core-services","title":"Core Services","text":"<ol> <li>NodeRegistry (<code>src/apflow/core/distributed/node_registry.py</code>)</li> <li>Register/deregister nodes</li> <li>Track heartbeats</li> <li>Mark stale/dead nodes</li> <li> <p>Query by executor types and capabilities</p> </li> <li> <p>LeaseManager (<code>src/apflow/core/distributed/lease_manager.py</code>)</p> </li> <li>Atomic lease acquisition (PostgreSQL <code>INSERT ... WHERE NOT EXISTS</code>)</li> <li>Lease renewal with token validation</li> <li>Cleanup expired leases</li> <li> <p>Idempotent acquisition support</p> </li> <li> <p>LeaderElection (<code>src/apflow/core/distributed/leader_election.py</code>)</p> </li> <li>Try acquire leadership (atomic insert with ON CONFLICT DO NOTHING)</li> <li>Renew leadership lease</li> <li>Release leadership gracefully</li> <li> <p>Detect leadership loss</p> </li> <li> <p>PlacementEngine (<code>src/apflow/core/distributed/placement.py</code>)</p> </li> <li>Evaluate placement constraints</li> <li>Filter eligible nodes</li> <li> <p>Match executor types and capabilities</p> </li> <li> <p>IdempotencyManager (<code>src/apflow/core/distributed/idempotency.py</code>)</p> </li> <li>Generate idempotency keys: <code>hash(task_id + attempt_id + inputs)</code></li> <li>Check cached results</li> <li>Store execution results</li> </ol> <p>Acceptance Criteria: - [ ] All services implemented with full type hints - [ ] Unit tests \u226590% coverage - [ ] Services can be instantiated independently - [ ] Integration tests pass</p>"},{"location":"development/distributed-development/#phase-3-worker-runtime-weeks-6-8","title":"Phase 3 \u2014 Worker Runtime (Weeks 6-8)","text":"<p>Goal: Implement worker execution loop</p>"},{"location":"development/distributed-development/#workerruntime-srcapflowcoredistributedworkerpy","title":"WorkerRuntime (<code>src/apflow/core/distributed/worker.py</code>)","text":"<p>Responsibilities: - Poll for executable tasks (status=pending, dependencies satisfied, no active lease) - Acquire lease atomically - Execute task locally using existing <code>TaskManager.distribute_task_tree()</code> - Renew lease periodically during execution - Report completion/failure - Handle idempotency caching</p> <p>Key Flow: <pre><code>async def _execute_task(self, task: TaskModel):\n    # 1. Acquire lease\n    lease = await self.lease_manager.acquire_lease(task.id, self.node_id)\n    if not lease:\n        return  # Already leased by another worker\n\n    # 2. Start lease renewal loop (background task)\n    renewal_task = asyncio.create_task(self._renew_lease_loop(task.id, lease.lease_token))\n\n    # 3. Check idempotency cache\n    idempotency_key = IdempotencyManager.generate_key(task.id, task.attempt_id, task.inputs)\n    is_cached, result = await self.idempotency.check_cached_result(idempotency_key)\n\n    if not is_cached:\n        # 4. Execute task using existing TaskExecutor\n        result = await self._execute_task_local(task)\n        await self.idempotency.store_result(task.id, task.attempt_id, idempotency_key, result, \"completed\")\n\n    # 5. Report completion\n    await self._report_completion(task.id, lease.lease_token, result)\n\n    # 6. Cleanup (cancel renewal, remove from running tasks)\n</code></pre></p> <p>Acceptance Criteria: - [ ] Worker polls and acquires leases - [ ] Lease renewal works for long tasks - [ ] Idempotency prevents duplicate execution - [ ] Heartbeat keeps node alive</p>"},{"location":"development/distributed-development/#phase-4-integration-weeks-9-11","title":"Phase 4 \u2014 Integration (Weeks 9-11)","text":"<p>Goal: Wire distributed services into TaskExecutor/TaskManager with role selection</p>"},{"location":"development/distributed-development/#distributedruntime-srcapflowcoredistributedruntimepy","title":"DistributedRuntime (<code>src/apflow/core/distributed/runtime.py</code>)","text":"<p>Responsibilities: - Role selection (auto/leader/worker/observer) - Lifecycle management (startup/shutdown) - Background tasks (leader renewal, lease cleanup, node cleanup)</p> <p>Role Selection Logic: - auto: Attempt leader election; fall back to worker; retry leadership periodically - leader: Force leader (fail if cannot acquire) - worker: Never try to become leader - observer: Read-only endpoints only</p>"},{"location":"development/distributed-development/#taskexecutor-integration","title":"TaskExecutor Integration","text":"<p>Modify <code>TaskExecutor.__init__</code>: <pre><code># Initialize distributed runtime if enabled\ndistributed_config = get_distributed_config()\nif distributed_config.enabled:\n    # Validate dialect (DistributedRuntime will raise error if not PostgreSQL)\n    self._distributed_runtime = DistributedRuntime(distributed_config, session_factory, self)\n    asyncio.create_task(self._distributed_runtime.start())\nelse:\n    # Single-node mode: supports both PostgreSQL and DuckDB\n    self._distributed_runtime = None\n</code></pre></p> <p>Modify <code>execute_task_tree</code>: <pre><code>if self._distributed_runtime:\n    if self._distributed_runtime.is_leader:\n        return await self._execute_task_tree_local(...)\n    else:\n        raise RuntimeError(\"Non-leader nodes cannot execute tasks directly\")\nreturn await self._execute_task_tree_local(...)\n</code></pre></p>"},{"location":"development/distributed-development/#api-layer-integration","title":"API Layer Integration","text":"<p>Add leader-only checks to write operations in <code>TaskRoutes</code>: <pre><code>def _require_leader(self):\n    if self._distributed_runtime and not self._distributed_runtime.is_leader:\n        current_leader = await self._distributed_runtime.leader_election.get_current_leader()\n        raise HTTPException(503, f\"Write operation requires leader. Current leader: {current_leader}\")\n</code></pre></p> <p>Acceptance Criteria: - [ ] TaskExecutor initializes distributed runtime - [ ] Role selection works correctly - [ ] API enforces leader-only writes - [ ] Backward compatibility preserved</p>"},{"location":"development/distributed-development/#phase-5-observability-weeks-12-14","title":"Phase 5 \u2014 Observability (Weeks 12-14)","text":"<p>Goal: Add comprehensive observability and end-to-end tests</p>"},{"location":"development/distributed-development/#task-event-logging","title":"Task Event Logging","text":"<ul> <li>Emit events: created, assigned, started, completed, failed, reassigned, cancelled</li> <li>Store in <code>task_events</code> table</li> </ul>"},{"location":"development/distributed-development/#metrics-collection","title":"Metrics Collection","text":"<ul> <li>Active leases per node</li> <li>Lease acquisition/expiry rate</li> <li>Task execution time by executor type</li> <li>Node health status</li> <li>Leader election changes</li> </ul>"},{"location":"development/distributed-development/#integration-tests","title":"Integration Tests","text":"<ol> <li>Leader + 2 workers end-to-end</li> <li>Worker crash during execution</li> <li>Long task with lease renewal</li> <li>Leader restart recovery</li> <li>Placement constraints</li> <li>Concurrent lease acquisition</li> </ol> <p>Acceptance Criteria: - [ ] Task events logged - [ ] Metrics collected - [ ] All integration tests pass - [ ] Performance overhead \u226410%</p>"},{"location":"development/distributed-development/#13-testing-strategy","title":"13. Testing Strategy","text":""},{"location":"development/distributed-development/#unit-tests-90-coverage","title":"Unit Tests (\u226590% Coverage)","text":"<ul> <li><code>test_node_registry.py</code>: Registration, heartbeat, status transitions</li> <li><code>test_lease_manager.py</code>: Lease acquisition, renewal, expiry, cleanup</li> <li><code>test_leader_election.py</code>: Leadership acquisition, renewal, loss</li> <li><code>test_placement.py</code>: Constraint evaluation, node filtering</li> <li><code>test_idempotency.py</code>: Key generation, caching, retrieval</li> <li><code>test_worker.py</code>: Task polling, execution, lease renewal</li> </ul>"},{"location":"development/distributed-development/#integration-tests_1","title":"Integration Tests","text":"<ul> <li>Leader + 2 Workers: End-to-end task distribution and execution</li> <li>Worker Crash: Lease expiry, task reassignment, idempotency verification</li> <li>Long Task: Lease renewal for 60s+ tasks</li> <li>Leader Failover: New leader election, stale lease cleanup</li> <li>Placement Constraints: Executor requirements, node filtering</li> <li>Concurrent Acquisition: 5 workers attempting same task</li> </ul>"},{"location":"development/distributed-development/#test-infrastructure","title":"Test Infrastructure","text":"<pre><code>@pytest.fixture\nasync def distributed_cluster():\n    \"\"\"Start PostgreSQL + leader + N workers\"\"\"\n    # Start containers, yield cluster, teardown\n</code></pre>"},{"location":"development/distributed-development/#code-quality-requirements-per-claudemd","title":"Code Quality Requirements (Per CLAUDE.md)","text":"<p>Before each commit: <pre><code>ruff check --fix .\nblack .\npyright .\npytest tests/unit/distributed/ -v\npytest --cov=src/apflow/core/distributed --cov-report=term-missing\npytest tests/  # Full suite\n</code></pre></p> <p>Mandatory: - Full type annotations (no <code>Any</code> except external data) - Functions \u226450 lines, single responsibility - Logging (no print statements) - Explicit error handling (no bare except) - Context managers for all DB sessions - Zero ruff/black/pyright errors</p>"},{"location":"development/distributed-development/#14-migration-path","title":"14. Migration Path","text":""},{"location":"development/distributed-development/#for-existing-users","title":"For Existing Users","text":"<p>Step 1: Upgrade (No Behavior Change) <pre><code>pip install --upgrade apflow\napflow db migrate  # Adds distributed tables, but disabled by default\napflow task execute --task-id &lt;id&gt;  # Existing functionality unchanged\n</code></pre></p> <p>Step 2: Enable Distributed Mode <pre><code># Set environment variables\nexport APFLOW_CLUSTER_ENABLED=true\nexport APFLOW_NODE_ID=leader-01\nexport APFLOW_NODE_ROLE=leader\n\napflow server start\n</code></pre></p> <p>Step 3: Add Workers Gradually <pre><code># On worker nodes\nexport APFLOW_CLUSTER_ENABLED=true\nexport APFLOW_NODE_ID=worker-01\nexport APFLOW_NODE_ROLE=worker\n\napflow server start\n</code></pre></p>"},{"location":"development/distributed-development/#backward-compatibility-guarantees","title":"Backward Compatibility Guarantees","text":"<ul> <li>API: All existing REST/A2A endpoints work unchanged</li> <li>Data: Existing tasks readable and executable; distributed fields nullable</li> <li>Configuration: Distributed mode opt-in (default: disabled)</li> <li>Executors: Existing executors work without changes</li> <li>Database:</li> <li>DuckDB: Fully supported in single-node mode (distributed mode disabled)</li> <li>PostgreSQL: Supported in both single-node and distributed modes</li> <li>Migration adds only nullable columns to TaskModel for DuckDB</li> <li>Distributed tables created only for PostgreSQL</li> </ul>"},{"location":"development/distributed-development/#15-risks-mitigations","title":"15. Risks &amp; Mitigations","text":""},{"location":"development/distributed-development/#technical-risks","title":"Technical Risks","text":"Risk Impact Likelihood Mitigation Leader failure causes downtime High Medium External HA (k8s/HAProxy), automatic re-election Lease starvation under high load Medium Low Ensure cleanup interval &lt; lease duration, add metrics Duplicate execution despite idempotency High Low Comprehensive idempotency tests, atomic lease acquisition Task dependency deadlock Medium Low Dependency validation before assignment, timeout detection Performance overhead Medium Medium Benchmark each phase, optimize lease queries with indexes Data inconsistency High Low Single-writer leader, transactional updates, integration tests Network partition High Low Lease expiry handles split-brain, leader election on recovery"},{"location":"development/distributed-development/#development-risks","title":"Development Risks","text":"Risk Impact Likelihood Mitigation Scope creep High Medium Strict phase boundaries, defer non-essential features Breaking changes High Low Comprehensive backward compatibility tests Test complexity Medium High Invest in test infrastructure (fixtures, helpers) Integration conflicts Medium Medium Frequent integration testing, CI/CD enforcement"},{"location":"development/distributed-development/#16-performance-considerations","title":"16. Performance Considerations","text":""},{"location":"development/distributed-development/#expected-overhead","title":"Expected Overhead","text":"<p>Single-Node Mode (no distributed overhead): - No changes to execution path - No additional DB queries</p> <p>Distributed Mode (Leader): - Lease table queries: +2 queries per task assignment - Leader election renewal: +1 query every 10s - Node status checks: +1 query every 20s - Estimated overhead: ~5-10% for task assignment</p> <p>Distributed Mode (Worker): - Task polling: +1 query every 5s per worker - Lease renewal: +1 query every 10s per task - Heartbeat: +1 query every 10s per worker - Estimated overhead: Minimal (async background tasks)</p>"},{"location":"development/distributed-development/#optimization-strategies","title":"Optimization Strategies","text":"<p>Critical Database Indexes: <pre><code>CREATE INDEX idx_task_leases_expires_at ON apflow_task_leases(expires_at);\nCREATE INDEX idx_task_leases_node_id ON apflow_task_leases(node_id);\nCREATE INDEX idx_distributed_nodes_status ON apflow_distributed_nodes(status);\nCREATE INDEX idx_distributed_nodes_heartbeat ON apflow_distributed_nodes(heartbeat_at);\nCREATE INDEX idx_task_events_task_id_timestamp ON apflow_task_events(task_id, timestamp);\n</code></pre></p> <p>Query Optimization: - Batch node status updates (single <code>UPDATE WHERE IN</code>) - Use <code>SELECT FOR UPDATE SKIP LOCKED</code> for lease acquisition - Limit task polling queries (<code>TOP N WHERE status = pending</code>)</p> <p>Connection Pooling: - <code>SessionPoolManager</code> already handles pooling - Ensure pool size \u2265 max_parallel_tasks_per_node + background tasks</p> <p>Future Optimizations: - Cache node capabilities (TTL: 60s) - Cache placement constraints (invalidate on task update)</p>"},{"location":"development/distributed-development/#17-success-criteria","title":"17. Success Criteria","text":""},{"location":"development/distributed-development/#phase-acceptance","title":"Phase Acceptance","text":"<p>Phase 0: Foundation Ready - [ ] All tests pass on PostgreSQL with connection pooling - [ ] TaskExecutor refactored for testability - [ ] Current execution flow documented</p> <p>Phase 1: Storage Complete - [ ] All distributed tables created - [ ] Migration runs successfully on PostgreSQL and DuckDB - [ ] No behavior changes in single-node mode - [ ] All existing tests pass</p> <p>Phase 2: Leader Services Complete - [ ] All services implemented with full type hints - [ ] Unit tests \u226590% coverage - [ ] Services can be instantiated independently - [ ] Integration tests pass</p> <p>Phase 3: Worker Runtime Complete - [ ] Worker polls and acquires leases correctly - [ ] Lease renewal works for long-running tasks - [ ] Idempotency prevents duplicate execution - [ ] Heartbeat keeps nodes alive</p> <p>Phase 4: Integration Complete - [ ] TaskExecutor integrates distributed runtime - [ ] Role selection works (auto/leader/worker/observer) - [ ] API enforces leader-only writes - [ ] Backward compatibility preserved</p> <p>Phase 5: Production Ready - [ ] Task events logged - [ ] Metrics collected - [ ] All integration tests pass - [ ] Performance overhead \u226410%</p>"},{"location":"development/distributed-development/#final-acceptance","title":"Final Acceptance","text":"<ul> <li> All tests pass (unit + integration)</li> <li> Code quality checks pass (ruff, black, pyright)</li> <li> Documentation complete</li> <li> Demo environment running (leader + 2 workers)</li> <li> Migration guide validated with existing users</li> </ul>"},{"location":"development/distributed-development/#18-open-questions-future-work","title":"18. Open Questions &amp; Future Work","text":""},{"location":"development/distributed-development/#open-questions-resolve-in-phase-1","title":"Open Questions (Resolve in Phase 1)","text":"<ol> <li>DuckDB support in distributed mode: Should we support distributed mode on DuckDB (via shared filesystem)?    Answer: No, PostgreSQL-only for distributed mode.</li> <li>DuckDB is single-writer and unsuitable for multi-node coordination</li> <li>Single-node mode will continue to support DuckDB (backward compatible)</li> <li> <p>Migration scripts are dialect-aware and skip distributed tables for DuckDB</p> </li> <li> <p>Streaming events: How to broadcast events in distributed mode?    Recommendation: PostgreSQL LISTEN/NOTIFY or Redis pub/sub (defer to Phase 5).</p> </li> <li> <p>Task tree execution: Should task tree span multiple workers or execute on single worker?    Recommendation: Start with single-worker execution (simplifies dependency resolution).</p> </li> </ol>"},{"location":"development/distributed-development/#future-work-post-v10","title":"Future Work (Post-v1.0)","text":"<ul> <li>Cross-region support: Multi-region leader election</li> <li>Advanced scheduling: Time-based task assignment, priority queues</li> <li>Distributed streaming: Real-time event broadcasting (Redis, Kafka)</li> <li>Auto-scaling: Dynamic worker scaling based on load</li> <li>Multi-tenancy: Tenant isolation for distributed tasks</li> <li>Observability: Grafana dashboards, Prometheus metrics</li> <li>CLI enhancements: <code>apflow cluster status</code>, <code>apflow cluster nodes</code></li> </ul>"},{"location":"development/distributed-development/#19-recommended-first-steps","title":"19. Recommended First Steps","text":""},{"location":"development/distributed-development/#week-1-preparation","title":"Week 1: Preparation","text":"<ol> <li>Review existing test coverage for TaskManager/TaskExecutor</li> <li>Set up PostgreSQL test environment</li> <li>Create development branch: <code>feature/distributed-core</code></li> <li>Document current execution flow (sequence diagrams)</li> </ol>"},{"location":"development/distributed-development/#week-2-phase-1-start","title":"Week 2: Phase 1 Start","text":"<ol> <li>Implement database models in <code>models.py</code></li> <li>Create Alembic migration for distributed tables</li> <li>Add <code>DistributedConfig</code> to <code>ConfigRegistry</code></li> <li>Write unit tests for models</li> <li>Verify migration on PostgreSQL</li> </ol>"},{"location":"development/distributed-development/#weeks-3-4-phase-1-complete-phase-2-start","title":"Weeks 3-4: Phase 1 Complete + Phase 2 Start","text":"<ol> <li>Complete Phase 1 acceptance criteria</li> <li>Implement <code>NodeRegistry</code> service</li> <li>Implement <code>LeaseManager</code> service</li> <li>Write comprehensive unit tests</li> <li>Begin integration tests</li> </ol>"},{"location":"development/distributed-development/#20-references-resources","title":"20. References &amp; Resources","text":"<ul> <li><code>docs/development/distributed-orchestration-design.md</code> \u2014 Original design document</li> <li><code>docs/architecture/overview.md</code> \u2014 Current architecture overview</li> <li><code>docs/architecture/task-tree-lifecycle.md</code> \u2014 Task lifecycle documentation</li> <li><code>src/apflow/core/execution/task_manager.py</code> \u2014 Current TaskManager implementation</li> <li><code>src/apflow/core/execution/task_executor.py</code> \u2014 Current TaskExecutor implementation</li> <li><code>src/apflow/core/storage/sqlalchemy/</code> \u2014 Storage layer</li> <li><code>CLAUDE.md</code> \u2014 Code quality standards and project guidelines</li> <li><code>pyproject.toml</code> \u2014 Project configuration</li> </ul>"},{"location":"development/distributed-development/#summary","title":"Summary","text":"<p>The distributed architecture design is well-aligned with apflow's current architecture and can be implemented incrementally without disrupting existing functionality. Key success factors:</p> <ol> <li>Strict phasing: Complete each phase before moving to next</li> <li>Test-driven: Write tests before implementation (\u226590% coverage)</li> <li>Backward compatibility: Single-node mode remains unchanged</li> <li>Code quality: Follow CLAUDE.md standards rigorously</li> <li>Incremental validation: Integration tests after each phase</li> </ol> <p>Estimated Timeline: 12-14 weeks for full implementation Phase 1 Timeline: 2 weeks (can validate schema early)</p> <p>Next Step: Begin Phase 0 (preparation) to ensure foundation is solid, then proceed with Phase 1 (storage) for early feedback on schema design.</p>"},{"location":"development/import-performance/","title":"Import Performance Best Practices","text":""},{"location":"development/import-performance/#background","title":"Background","text":"<p>Python's import mechanism can cause performance issues: - Circular imports: Module A imports B, and B imports A, causing modules to be loaded multiple times. - Eager imports: Importing all dependencies at the module level, even if they are not needed. - Heavyweight dependencies: Importing large libraries (e.g., litellm takes 2.4s, crewai takes 5.4s) slows down startup.</p>"},{"location":"development/import-performance/#best-practices","title":"Best Practices","text":""},{"location":"development/import-performance/#1-avoid-heavyweight-imports-at-module-level","title":"1. Avoid Heavyweight Imports at Module Level","text":"<p>\u274c Bad Practice: <pre><code># cli/commands/tasks.py\nimport litellm  # 2.4 seconds! Even if LLM functionality is not needed\nfrom crewai import Agent  # 5.4 seconds!\n\ndef list_tasks():\n    # Only queries the database, no need for these libraries\n    ...\n</code></pre></p> <p>\u2705 Good Practice: <pre><code># cli/commands/tasks.py\n\ndef run_llm_task():\n    # Import only when needed\n    import litellm\n    ...\n</code></pre></p>"},{"location":"development/import-performance/#2-use-lazy-loading-__getattr__","title":"2. Use Lazy Loading (<code>__getattr__</code>)","text":"<p>\u274c Bad Practice: <pre><code># package/__init__.py\nfrom .heavy_module import HeavyClass  # Loaded immediately\nfrom .another_heavy import AnotherClass\n</code></pre></p> <p>\u2705 Good Practice: <pre><code># package/__init__.py\n__all__ = [\"HeavyClass\", \"AnotherClass\"]\n\ndef __getattr__(name):\n    \"\"\"Load on demand\"\"\"\n    if name == \"HeavyClass\":\n        from .heavy_module import HeavyClass\n        return HeavyClass\n    elif name == \"AnotherClass\":\n        from .another_heavy import AnotherClass\n        return AnotherClass\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n</code></pre></p>"},{"location":"development/import-performance/#3-avoid-circular-imports","title":"3. Avoid Circular Imports","text":"<p>\u274c Bad Practice: <pre><code># module_a.py\nfrom module_b import func_b\n\ndef func_a():\n    return func_b()\n\n# module_b.py\nfrom module_a import func_a  # Circular!\n\ndef func_b():\n    return func_a()\n</code></pre></p> <p>\u2705 Good Practices: <pre><code># Option 1: Refactor code, extract shared logic into a third module\n# shared.py\ndef shared_logic():\n    ...\n\n# module_a.py\nfrom shared import shared_logic\n\n# module_b.py\nfrom shared import shared_logic\n\n# Option 2: Use local imports\ndef func_b():\n    from module_a import func_a  # Import inside the function\n    return func_a()\n</code></pre></p>"},{"location":"development/import-performance/#4-extension-registry-lazy-loading","title":"4. Extension Registry Lazy Loading","text":"<p>For plugin systems, automatically load on first access:</p> <pre><code>_registry = ExtensionRegistry()\n_extensions_loaded = False\n\ndef get_registry():\n    global _extensions_loaded\n    if not _extensions_loaded:\n        _extensions_loaded = True\n        import apflow.extensions  # Automatically register all plugins\n    return _registry\n</code></pre>"},{"location":"development/import-performance/#tools-and-checks","title":"Tools and Checks","text":""},{"location":"development/import-performance/#local-checks","title":"Local Checks","text":"<pre><code># Detect circular imports\nmake check-circular\n\n# Check import performance\nmake check-performance\n\n# Check heavyweight module imports\nmake check-heavy\n\n# Run all checks\nmake check-imports\n</code></pre>"},{"location":"development/import-performance/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks: <pre><code>pip install pre-commit\npre-commit install\n</code></pre></p> <p>Automatically checks on every commit: - Circular import detection - Heavyweight module-level import detection</p>"},{"location":"development/import-performance/#cicd","title":"CI/CD","text":"<p>GitHub Actions automatically checks on every PR: - Circular imports - CLI startup time (must be &lt; 1.5 seconds) - Heavyweight dependencies should not be loaded during CLI startup</p>"},{"location":"development/import-performance/#python-built-in-tools","title":"Python Built-in Tools","text":"<p>Use <code>-X importtime</code> to analyze import times: <pre><code>python -X importtime -c \"import apflow.cli.main\" 2&gt;&amp;1 | grep apflow\n</code></pre></p>"},{"location":"development/import-performance/#professional-tools","title":"Professional Tools","text":"<ol> <li> <p>tuna - Visualize import times    <pre><code>pip install tuna\npython -X importtime -c \"import apflow\" 2&gt; import.log\ntuna import.log\n</code></pre></p> </li> <li> <p>pydeps - Visualize dependency graphs    <pre><code>pip install pydeps\npydeps apflow --max-bacon=2 -o deps.png\n</code></pre></p> </li> <li> <p>import-profiler <pre><code>pip install import-profiler\npython -m import_profiler apflow.cli.main\n</code></pre></p> </li> </ol>"},{"location":"development/import-performance/#performance-goals","title":"Performance Goals","text":"<ul> <li>\u2705 CLI startup: &lt; 1.5 seconds</li> <li>\u2705 Package import: &lt; 0.5 seconds</li> <li>\u2705 Zero circular imports</li> <li>\u2705 Heavyweight dependencies loaded only when needed</li> </ul>"},{"location":"development/import-performance/#monitoring","title":"Monitoring","text":"<p>Run after every code change: <pre><code># Quick check\ntime python -c \"import apflow.cli.main\"\n\n# Detailed analysis\npython scripts/analyze_import_performance.py\n</code></pre></p>"},{"location":"development/import-performance/#common-mistakes-and-fixes","title":"Common Mistakes and Fixes","text":""},{"location":"development/import-performance/#mistake-1-module-level-import-of-taskexecutor","title":"Mistake 1: Module-Level Import of TaskExecutor","text":"<pre><code># \u274c Loads all extensions\nfrom apflow.core.execution.task_executor import TaskExecutor\n\ndef some_query_function():\n    # Only queries the database, no need for TaskExecutor\n    ...\n</code></pre> <p>Fix: Move it to where it's actually needed <pre><code>def execute_task_function():\n    from apflow.core.execution.task_executor import TaskExecutor\n    executor = TaskExecutor()\n    ...\n</code></pre></p>"},{"location":"development/import-performance/#mistake-2-package-__init__py-eagerly-imports-all-submodules","title":"Mistake 2: Package <code>__init__.py</code> Eagerly Imports All Submodules","text":"<pre><code># \u274c package/__init__.py\nfrom .submodule_a import *\nfrom .submodule_b import *\nfrom .submodule_c import *  # Loads everything!\n</code></pre> <p>Fix: Use <code>__getattr__</code> <pre><code># \u2705 package/__init__.py\ndef __getattr__(name):\n    if name == \"ClassA\":\n        from .submodule_a import ClassA\n        return ClassA\n    ...\n</code></pre></p>"},{"location":"development/import-performance/#mistake-3-extensions-auto-registered-on-import","title":"Mistake 3: Extensions Auto-Registered on Import","text":"<pre><code># \u274c task_executor.py\nimport apflow.extensions  # Loads all extensions on startup\n</code></pre> <p>Fix: Delay until actually needed <pre><code># \u2705 registry.py\ndef get_registry():\n    if not _extensions_loaded:\n        import apflow.extensions  # Load only on first access\n    return _registry\n</code></pre></p>"},{"location":"development/import-performance/#case-study-apflow-optimization-journey","title":"Case Study: apflow Optimization Journey","text":"<p>Problem: CLI startup took 7 seconds Causes: 1. <code>apflow.core.__init__.py</code> eagerly imported all modules 2. <code>task_executor.py</code> automatically imported <code>apflow.extensions</code> 3. Extensions automatically imported litellm (2.4s) and crewai (5.4s)</p> <p>Fixes: 1. Changed all package <code>__init__.py</code> files to lazy loading 2. Removed auto-import of TaskExecutor 3. Load extensions in Registry only on first access 4. Import TaskExecutor on demand in CLI commands</p>"},{"location":"development/import-performance/#reference-resources","title":"Reference Resources","text":"<ul> <li>PEP 562 - Module getattr</li> <li>Python Import System</li> <li>Circular Imports in Python</li> </ul>"},{"location":"development/import-tools-guide/","title":"Import Performance Tools Usage Guide","text":""},{"location":"development/import-tools-guide/#goal","title":"\ud83c\udfaf Goal","text":"<p>Prevent future issues with slow imports and circular dependencies.</p>"},{"location":"development/import-tools-guide/#installed-tools","title":"\ud83d\udce6 Installed Tools","text":""},{"location":"development/import-tools-guide/#1-circular-import-detection","title":"1. Circular Import Detection","text":"<pre><code># Detect all circular imports\nmake check-circular\n\n# Or run directly\npython scripts/detect_circular_imports.py\n</code></pre> <p>Output example: <pre><code>\u2705 No circular imports detected!\n</code></pre></p>"},{"location":"development/import-tools-guide/#2-heavy-import-detection","title":"2. Heavy Import Detection","text":"<pre><code># Check all Python files\nmake check-heavy\n\n# Or check specific files\npython scripts/check_heavy_imports.py src/apflow/cli/commands/*.py\n</code></pre> <p>Will warn about these heavyweight libraries: - <code>litellm</code> (2.4s) - <code>crewai</code> (5.4s) - <code>torch</code>, <code>transformers</code>, <code>tensorflow</code>, <code>langchain</code></p>"},{"location":"development/import-tools-guide/#3-import-performance-analysis","title":"3. Import Performance Analysis","text":"<pre><code># Full performance report\nmake check-performance\n\n# Or use Python's built-in tool directly\npython -X importtime -c \"import apflow.cli.main\" 2&gt;&amp;1 | grep apflow\n</code></pre>"},{"location":"development/import-tools-guide/#4-one-click-check-all","title":"4. One-Click Check All","text":"<pre><code>make check-imports\n</code></pre>"},{"location":"development/import-tools-guide/#pre-commit-hooks","title":"\ud83d\udd27 Pre-commit Hooks","text":"<p>Automatically configured checks that run before every commit:</p> <pre><code># Install (first time only)\npip install pre-commit\npre-commit install\n\n# Now every git commit will automatically check:\n# \u2713 Circular imports\n# \u2713 Heavyweight module-level imports\n# \u2713 Code formatting (ruff)\n</code></pre>"},{"location":"development/import-tools-guide/#cicd-automatic-checks","title":"\ud83d\ude80 CI/CD Automatic Checks","text":"<p>Every PR will automatically verify: - \u2713 Circular import detection - \u2713 CLI startup time &lt; 1.5 seconds - \u2713 No heavyweight dependencies loaded at CLI startup</p> <p>See <code>.github/workflows/import-performance.yml</code></p>"},{"location":"development/import-tools-guide/#development-guidelines","title":"\ud83d\udcda Development Guidelines","text":""},{"location":"development/import-tools-guide/#avoid-these-mistakes","title":"\u274c Avoid These Mistakes","text":"<ol> <li> <p>Module-level heavyweight imports <pre><code># \u274c BAD: In CLI files\nimport litellm  # Slows down startup!\n\ndef list_tasks():\n    ...  # Doesn't even use litellm\n</code></pre></p> </li> <li> <p>Package <code>__init__.py</code> Eager Imports <pre><code># \u274c BAD: package/__init__.py\nfrom .heavy_module import HeavyClass  # Loads everything immediately!\n</code></pre></p> </li> <li> <p>Automatically importing all Extensions <pre><code># \u274c BAD\nimport apflow.extensions  # Loads all executors at module level\n</code></pre></p> </li> </ol>"},{"location":"development/import-tools-guide/#correct-practices","title":"\u2705 Correct Practices","text":"<ol> <li> <p>Import on Demand <pre><code># \u2705 GOOD\ndef execute_llm_task():\n    import litellm  # Import only when needed\n    ...\n</code></pre></p> </li> <li> <p>Lazy Loading for Packages <pre><code># \u2705 GOOD: package/__init__.py\ndef __getattr__(name):\n    if name == \"HeavyClass\":\n        from .heavy_module import HeavyClass\n        return HeavyClass\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n</code></pre></p> </li> <li> <p>Registry Lazy Loading <pre><code># \u2705 GOOD\ndef get_registry():\n    global _extensions_loaded\n    if not _extensions_loaded:\n        import apflow.extensions  # Load only on first access\n    return _registry\n</code></pre></p> </li> </ol>"},{"location":"development/import-tools-guide/#recommended-professional-tools","title":"\ud83d\udd0d Recommended Professional Tools","text":""},{"location":"development/import-tools-guide/#tuna-visualize-import-time","title":"tuna - Visualize Import Time","text":"<pre><code>pip install tuna\npython -X importtime -c \"import apflow\" 2&gt; import.log\ntuna import.log  # Opens browser with visual chart\n</code></pre>"},{"location":"development/import-tools-guide/#pydeps-dependency-graph","title":"pydeps - Dependency Graph","text":"<pre><code>pip install pydeps graphviz\npydeps apflow --max-bacon=2 -o deps.png\nopen deps.png\n</code></pre>"},{"location":"development/import-tools-guide/#import-profiler","title":"import-profiler","text":"<pre><code>pip install import-profiler\npython -m import_profiler apflow.cli.main\n</code></pre>"},{"location":"development/import-tools-guide/#performance-benchmarks","title":"\ud83d\udcca Performance Benchmarks","text":"<p>Current metrics: - \u2705 CLI startup: 1.3 seconds (target &lt; 1.5 seconds) - \u2705 <code>import apflow.core</code>: 0.8 seconds - \u2705 <code>import apflow.cli.main</code>: 0.7 seconds - \u2705 Zero circular imports</p> <p>Historical comparison: - Before optimization: 7.0 seconds \ud83d\ude31 - After optimization: 1.3 seconds \u2728 - Improvement: 5.4\u00d7 \ud83d\ude80</p>"},{"location":"development/import-tools-guide/#daily-usage","title":"\ud83d\udee0\ufe0f Daily Usage","text":""},{"location":"development/import-tools-guide/#before-developing-a-new-feature","title":"Before Developing a New Feature","text":"<pre><code># Check current baseline\ntime python -c \"import apflow.cli.main\"\n</code></pre>"},{"location":"development/import-tools-guide/#after-finishing-development","title":"After Finishing Development","text":"<pre><code># Run all checks\nmake check-imports\n\n# Or individual checks\nmake check-circular\nmake check-performance\nmake check-heavy\n</code></pre>"},{"location":"development/import-tools-guide/#git-commit","title":"Git Commit","text":"<pre><code>git add .\ngit commit -m \"feat: new feature\"\n# Pre-commit hooks run checks automatically\n</code></pre>"},{"location":"development/import-tools-guide/#when-encountering-warnings","title":"When Encountering Warnings","text":"<p>Read <code>docs/development/import-performance.md</code> for detailed fix solutions.</p>"},{"location":"development/import-tools-guide/#learning-resources","title":"\ud83c\udf93 Learning Resources","text":"<ul> <li>docs/development/import-performance.md - Complete best practices guide</li> <li>PEP 562 - Module getattr</li> <li>Python Import System</li> </ul>"},{"location":"development/import-tools-guide/#remember","title":"\ud83d\udca1 Remember","text":"<p>\"If it takes more than 1 second to run <code>--help</code>, it's too slow.\"</p> <p>Import performance directly affects user experience. Stay vigilant and avoid: 1. Module-level heavyweight imports 2. Eager loading of all modules 3. Circular dependencies</p> <p>Use these tools to continuously monitor! \ud83c\udfaf</p>"},{"location":"development/roadmap/","title":"Roadmap","text":"<p>Pure orchestration library + Optional framework components</p>"},{"location":"development/roadmap/#current-status-v1x","title":"Current Status (v1.x)","text":"<p>Core orchestration is stable with 800+ tests. Key capabilities: - Task orchestration with dependency trees and priority execution - 12+ executors (REST, WebSocket, gRPC, SSH, Docker, CrewAI, LiteLLM, MCP) - Multi-protocol API (A2A, MCP, JSON-RPC) - Built-in scheduler (internal polling + external gateway) - CLI and ConfigManager - DuckDB + PostgreSQL storage</p>"},{"location":"development/roadmap/#near-term","title":"Near-term","text":"Feature Status Description Distributed Core Planned Multi-node orchestration with task leasing Protocol Abstraction In Progress Unified adapter interface for all protocols GraphQL Adapter Planned Query interface for complex task trees"},{"location":"development/roadmap/#distributed-core","title":"Distributed Core","text":"<p>Enable multi-node deployments with centralized coordination. See design doc for details.</p> <ul> <li>Node registry with health checks</li> <li>Task leasing with automatic expiry</li> <li>Placement constraints (executor type, resources)</li> <li>PostgreSQL-based coordination</li> </ul>"},{"location":"development/roadmap/#protocol-abstraction","title":"Protocol Abstraction","text":"<p>Unified interface for protocol adapters. Phase 1 complete:</p> <ul> <li>Capabilities registry (<code>api/capabilities.py</code>) as single source of truth for all 15 operations</li> <li>A2A adapter simplified to agent-level actions only (execute, generate, cancel)</li> <li>MCP adapter auto-generates 15 tools from the registry</li> <li>Native API (<code>POST /tasks</code>) as primary programmatic interface</li> <li>Method discovery endpoint (<code>GET /tasks/methods</code>)</li> </ul> <p>Remaining:</p> <pre><code>class ProtocolAdapter(Protocol):\n    async def handle_execute_request(self, request: dict) -&gt; dict: ...\n    async def handle_status_request(self, request: dict) -&gt; dict: ...\n</code></pre>"},{"location":"development/roadmap/#graphql-adapter","title":"GraphQL Adapter","text":"<p>Optional <code>strawberry-graphql</code> based adapter for querying task trees.</p>"},{"location":"development/roadmap/#mid-term","title":"Mid-term","text":"Feature Status Description MQTT Adapter Planned IoT/Edge AI agent communication Observability Hooks Planned Pluggable metrics (Prometheus, OpenTelemetry) Workflow Patterns Planned Map-Reduce, Fan-Out/Fan-In, Circuit Breaker Testing Utilities Planned TaskMocker, workflow simulation"},{"location":"development/roadmap/#future","title":"Future","text":"Feature Status Description VS Code Extension Idea Task tree visualization Hot Reload Idea Auto-reload on code changes WebSocket Server Idea Bidirectional agent collaboration"},{"location":"development/roadmap/#not-planned","title":"Not Planned","text":"<p>These are application-level concerns, not orchestration:</p> <ul> <li>User Management / Auth / RBAC</li> <li>Multi-Tenancy</li> <li>Audit Logging (use observability hooks)</li> <li>Secret Management (use Vault, AWS Secrets Manager)</li> <li>Dashboard UI (separate project: apflow-webapp)</li> </ul>"},{"location":"development/roadmap/#completed","title":"Completed","text":"Feature Version Notes Fluent API (TaskBuilder) v1.x Type-safe chainable task creation CLI \u2192 API Gateway v1.x CLI routes through API when configured ConfigManager v1.x Unified configuration management Task Model Extensions v1.x task_tree_id, origin_type, migrations Executor Access Control v1.x Environment-based filtering Scheduler v1.x Internal scheduler + external gateway integration"},{"location":"development/setup/","title":"apflow Development Guide","text":"<p>This document is for developers working on the <code>apflow</code> project. For user documentation, see README.md.</p>"},{"location":"development/setup/#project-structure","title":"Project Structure","text":"<p>See docs/architecture/DIRECTORY_STRUCTURE.md for detailed directory structure including source code, tests, and all modules.</p>"},{"location":"development/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (3.12+ recommended, see note below)</li> <li>DuckDB (default embedded storage, no setup required)</li> <li>PostgreSQL (optional, for distributed/production scenarios)</li> </ul> <p>Note: The project uses Python 3.12 for compatibility. Python 3.13 may have compatibility issues.</p>"},{"location":"development/setup/#quick-start","title":"Quick Start","text":""},{"location":"development/setup/#1-clone-and-setup","title":"1. Clone and Setup","text":"<pre><code>git clone &lt;repository-url&gt;\ncd apflow\n\n# Create virtual environment\npython3.12 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre>"},{"location":"development/setup/#2-install-dependencies","title":"2. Install Dependencies","text":""},{"location":"development/setup/#option-a-using-uv-recommended-fastest","title":"Option A: Using uv (Recommended - Fastest)","text":"<pre><code># Install uv if you haven't already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install standard development environment (recommended)\nuv pip install -e \".[standard,dev]\"\n\n# OR install with all features\nuv pip install -e \".[all,dev]\"\n\n# OR install with specific extras\nuv pip install -e \".[crewai,cli,dev]\"  # CrewAI + CLI + dev tools\n</code></pre>"},{"location":"development/setup/#option-b-using-pip-traditional","title":"Option B: Using pip (Traditional)","text":"<pre><code># Install standard development environment (recommended)\npip install -e \".[standard,dev]\"\n\n# OR install with all features\npip install -e \".[all,dev]\"\n\n# OR install with specific extras\npip install -e \".[crewai,cli,dev]\"  # CrewAI + CLI + dev tools\n</code></pre>"},{"location":"development/setup/#option-c-using-poetry-if-configured","title":"Option C: Using Poetry (If configured)","text":"<pre><code># Install poetry if you haven't already\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Install all dependencies\npoetry install --with dev\n</code></pre>"},{"location":"development/setup/#3-environment-configuration","title":"3. Environment Configuration","text":"<p>Create a <code>.env</code> file in the project root (optional, for API service configuration):</p> <pre><code># API Service Configuration\nAPFLOW_API_HOST=0.0.0.0  # Or use API_HOST (fallback)\nAPFLOW_API_PORT=8000     # Or use API_PORT (fallback)\n\n# Database Configuration\n# Priority: APFLOW_DATABASE_URL &gt; DATABASE_URL (fallback)\n# \n# Option 1: PostgreSQL\n# APFLOW_DATABASE_URL=postgresql+asyncpg://user:password@localhost/apflow\n#\n# Option 2: DuckDB with custom path\n# APFLOW_DATABASE_URL=duckdb:///.data/my_custom.duckdb\n#\n# Option 3: Default (auto-detected)\n# - In project: .data/apflow.duckdb (created automatically)\n# - Outside project: ~/.aipartnerup/data/apflow.duckdb\n\n# Logging\nAPFLOW_LOG_LEVEL=INFO  # Or use LOG_LEVEL (fallback). Options: DEBUG, INFO, WARNING, ERROR, CRITICAL\n\n# CLI Configuration (optional, stored in .data/ or ~/.aipartnerup/apflow/)\n# APFLOW_CONFIG_DIR=/custom/config/path  # Override config directory location\n</code></pre> <p>Environment Variable Naming Convention:</p> <p>apflow uses a consistent naming pattern: - Preferred: <code>APFLOW_*</code> prefix (e.g., <code>APFLOW_LOG_LEVEL</code>, <code>APFLOW_DATABASE_URL</code>) - Fallback: Generic names without prefix (e.g., <code>LOG_LEVEL</code>, <code>DATABASE_URL</code>)</p> <p>This allows apflow to work seamlessly in multi-service environments while maintaining isolation.</p>"},{"location":"development/setup/#cli-configuration","title":"CLI Configuration","text":"<p>Configuration is managed through the <code>apflow config</code> command and stored securely:</p> <pre><code># Setup API server configuration\napflow config init-server --url http://localhost:8000 --role admin\n\n# Or manually configure\napflow config set api_server_url http://localhost:8000\napflow config gen-token --role admin --save\n\n# View configuration (tokens masked)\napflow config list\napflow config show-path  # Show file locations and priorities\n</code></pre> <p>Configuration Storage: - Project-local (highest priority): <code>.data/</code> directory - User-global (fallback): <code>~/.aipartnerup/apflow/</code> directory - Files:   - <code>config.cli.yaml</code> (600) - All CLI settings (sensitive and non-sensitive)</p>"},{"location":"development/setup/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code># Check installation\npython -c \"import apflow; print(apflow.__version__)\"\n\n# Run tests to verify everything works\npytest tests/ -v\n</code></pre>"},{"location":"development/setup/#development-workflow","title":"Development Workflow","text":""},{"location":"development/setup/#running-the-project","title":"Running the Project","text":""},{"location":"development/setup/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests (recommended)\npytest tests/ -v\n\n# Run specific test file\npytest tests/test_task_manager.py -v\n\n# Run with coverage\npytest --cov=apflow --cov-report=html tests/\n\n# Run only unit tests (exclude integration tests)\npytest -m \"not integration\" tests/\n\n# Run only integration tests\npytest -m integration tests/\n\n# Run CLI tests specifically\npytest tests/cli/ -v\n</code></pre> <p>Note: A2A tests are excluded by default (optional a2a dependency). To run them: <pre><code>pip install -e \".[a2a]\"\npytest tests/api/a2a/ -v\n</code></pre></p>"},{"location":"development/setup/#run-api-server-development","title":"Run API Server (Development)","text":"<pre><code># Method 1: Using CLI (if installed with [cli] extra)\napflow serve --port 8000 --reload\n# Or use shorthand:\napflow serve --port 8000 --reload\n# Note: 'serve --port 8000' (without 'start') also works\n\n# Method 2: Using Python module directly (recommended)\npython -m apflow.api.main\n\n# Method 3: Using entry point (if installed with [a2a] extra)\napflow-server\n\n# Method 4: Direct execution of serve command (for development)\npython src/apflow/cli/commands/serve.py start --port 8000 --reload\n</code></pre>"},{"location":"development/setup/#run-cli-commands","title":"Run CLI Commands","text":"<pre><code># Run a flow (standard mode with tasks array)\napflow run flow --tasks '[{\"id\": \"task1\", \"name\": \"Task 1\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}}]'\n\n# Or legacy mode (executor ID + inputs)\napflow run flow system_info_executor --inputs '{\"resource\": \"cpu\"}'\n\n# Start daemon mode\napflow daemon start\n</code></pre>"},{"location":"development/setup/#code-quality","title":"Code Quality","text":""},{"location":"development/setup/#pre-commit-hooks-recommended","title":"Pre-commit Hooks (Recommended)","text":"<p>Set up pre-commit hooks to automatically run <code>ruff check --fix</code> before each commit:</p> <pre><code># Install pre-commit (included in [dev] extra)\npip install pre-commit\n\n# Install git hooks\npre-commit install\n</code></pre> <p>After setup, <code>ruff check src/ tests/ --fix</code> will run automatically on every <code>git commit</code>.</p>"},{"location":"development/setup/#format-code","title":"Format Code","text":"<pre><code># Format all code\nblack src/ tests/\n\n# Check formatting without applying\nblack --check src/ tests/\n</code></pre>"},{"location":"development/setup/#lint-code","title":"Lint Code","text":"<pre><code># Run linter\nruff check src/ tests/\n\n# Auto-fix linting issues\nruff check --fix src/ tests/\n</code></pre>"},{"location":"development/setup/#type-checking","title":"Type Checking","text":"<pre><code># Run type checker\nmypy src/apflow/\n\n# Check specific module\nmypy src/apflow/core/interfaces/ src/apflow/core/execution/ src/apflow/core/storage/\n### Continuous Integration\n\nThe project uses GitHub Actions for CI. The workflow is defined in `.github/workflows/ci.yml`.\n\nIt runs tests (`pytest`) across Python 3.10, 3.11, and 3.12.\nYou can run them locally using the commands mentioned above.\n\n### Database Operations\n\n#### Default DuckDB (No Setup Required)\n\nDuckDB is the default embedded storage. It requires no external setup - it creates database files locally.\n\n```bash\n# Test storage (creates temporary DuckDB file)\npytest tests/test_storage.py -v\n</code></pre>"},{"location":"development/setup/#postgresql-optional","title":"PostgreSQL (Optional)","text":"<p>If you want to test with PostgreSQL:</p> <pre><code># Install PostgreSQL extra\npip install -e \".[postgres]\"\n\n# Set environment variable\nexport DATABASE_URL=\"postgresql+asyncpg://user:password@localhost/apflow\"\n\n# Run database migrations (if using Alembic)\nalembic upgrade head\n</code></pre>"},{"location":"development/setup/#database-migrations","title":"Database Migrations","text":"<pre><code># Create new migration\nalembic revision --autogenerate -m \"Description of changes\"\n\n# Apply migrations\nalembic upgrade head\n\n# Rollback migration\nalembic downgrade -1\n</code></pre>"},{"location":"development/setup/#running-services","title":"Running Services","text":""},{"location":"development/setup/#api-service","title":"API Service","text":"<pre><code># Development mode (auto-reload)\napflow serve --port 8000 --reload\n\n# Production mode\napflow serve --port 8000 --workers 4\n</code></pre>"},{"location":"development/setup/#daemon-mode","title":"Daemon Mode","text":"<pre><code># Start daemon\napflow daemon start\n\n# Stop daemon\napflow daemon stop\n\n# Check daemon status\napflow daemon status\n</code></pre>"},{"location":"development/setup/#dependency-management","title":"Dependency Management","text":""},{"location":"development/setup/#core-dependencies","title":"Core Dependencies","text":"<p>Installed with <code>pip install apflow</code> (pure orchestration framework):</p> <ul> <li><code>pydantic</code> - Data validation</li> <li><code>sqlalchemy</code> - ORM</li> <li><code>alembic</code> - Database migrations</li> <li><code>duckdb-engine</code> - Default embedded storage</li> </ul> <p>Note: CrewAI is NOT in core dependencies - it's available via [crewai] extra.</p>"},{"location":"development/setup/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"development/setup/#a2a-protocol-server-a2a","title":"A2A Protocol Server (<code>[a2a]</code>)","text":"<pre><code>pip install -e \".[a2a]\"\n</code></pre> <p>Includes: - <code>fastapi</code> - Web framework - <code>uvicorn</code> - ASGI server - <code>a2a-sdk[http-server]</code> - A2A protocol support - <code>httpx</code>, <code>aiohttp</code> - HTTP clients - <code>websockets</code> - WebSocket support</p>"},{"location":"development/setup/#cli-tools-cli","title":"CLI Tools (<code>[cli]</code>)","text":"<pre><code>pip install -e \".[cli]\"\n</code></pre> <p>Includes: - <code>click</code>, <code>rich</code>, <code>typer</code> - CLI framework and utilities</p>"},{"location":"development/setup/#postgresql-postgres","title":"PostgreSQL (<code>[postgres]</code>)","text":"<pre><code>pip install -e \".[postgres]\"\n</code></pre> <p>Includes: - <code>asyncpg</code> - Async PostgreSQL driver - <code>psycopg2-binary</code> - Sync PostgreSQL driver</p>"},{"location":"development/setup/#crewai-support-crewai","title":"CrewAI Support (<code>[crewai]</code>)","text":"<pre><code>pip install -e \".[crewai]\"\n</code></pre> <p>Includes: - <code>crewai[tools]</code> - Core CrewAI orchestration engine - <code>crewai-tools</code> - CrewAI tools - CrewaiExecutor for LLM-based agent crews - BatchCrewaiExecutor for atomic batch execution of multiple crews</p> <p>Note: BatchCrewaiExecutor is part of [crewai] because it's specifically designed for batching CrewAI crews together.</p> <p>Note: For examples and learning templates, see the test cases in <code>tests/integration/</code> and <code>tests/extensions/</code>. Test cases serve as comprehensive examples demonstrating real-world usage patterns.</p>"},{"location":"development/setup/#llm-support-llm","title":"LLM Support (<code>[llm]</code>)","text":"<pre><code>pip install -e \".[llm]\"\n</code></pre> <p>Includes: - <code>litellm</code> - Unified LLM interface supporting 100+ providers</p>"},{"location":"development/setup/#standard-standard","title":"Standard (<code>[standard]</code>)","text":"<pre><code># Recommended for most developers\npip install -e \".[standard,dev]\"\n</code></pre> <p>Includes: - A2A Protocol Server - Agent-to-Agent communication protocol - CLI Tools - Command-line interface - CrewAI Support - LLM-based agent crew orchestration - LLM Support - Direct LLM interaction via LiteLLM - Development Tools - When combined with [dev]</p> <p>This is the recommended installation profile for most use cases as it provides: - API server capability (A2A Protocol) - CLI tools for task management and execution - LLM support for AI-powered tasks - Batch execution via CrewAI - Full development environment when combined with [dev]</p>"},{"location":"development/setup/#development-dev","title":"Development (<code>[dev]</code>)","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>Includes: - <code>pytest</code>, <code>pytest-asyncio</code>, <code>pytest-cov</code> - Testing - <code>black</code> - Code formatting - <code>ruff</code> - Linting - <code>mypy</code> - Type checking</p>"},{"location":"development/setup/#standard-installation-recommended","title":"Standard Installation (Recommended)","text":"<pre><code># Install standard features + development tools (recommended)\npip install -e \".[standard,dev]\"\n\n# This installs:\n# - A2A Protocol Server\n# - CLI tools\n# - CrewAI and LLM support\n# - Development tools (pytest, ruff, mypy, etc.)\n</code></pre>"},{"location":"development/setup/#full-installation","title":"Full Installation","text":"<pre><code># Install everything (all extras + dev tools)\npip install -e \".[all,dev]\"\n</code></pre>"},{"location":"development/setup/#testing","title":"Testing","text":""},{"location":"development/setup/#test-structure","title":"Test Structure","text":"<p>See docs/architecture/DIRECTORY_STRUCTURE.md for complete test directory structure.</p> <p>Test structure mirrors source code structure: - <code>tests/core/</code> - Core framework tests - <code>tests/extensions/</code> - Extension tests - <code>tests/api/a2a/</code> - A2A Protocol Server tests - <code>tests/cli/</code> - CLI tests - <code>tests/integration/</code> - Integration tests</p>"},{"location":"development/setup/#writing-tests","title":"Writing Tests","text":""},{"location":"development/setup/#test-fixtures","title":"Test Fixtures","text":"<p>Use the provided fixtures from <code>conftest.py</code>:</p> <pre><code>import pytest\n\n@pytest.mark.asyncio\nasync def test_my_feature(sync_db_session, sample_task_data):\n    # Use sync_db_session for database operations\n    # Use sample_task_data for test data\n    pass\n</code></pre>"},{"location":"development/setup/#test-markers","title":"Test Markers","text":"<pre><code># Mark as integration test (requires external services)\n@pytest.mark.integration\nasync def test_external_service():\n    pass\n\n# Mark as slow test\n@pytest.mark.slow\ndef test_performance():\n    pass\n\n# Mark as requiring API keys\n@pytest.mark.requires_api_keys\nasync def test_api_integration(api_keys_available):\n    pass\n</code></pre>"},{"location":"development/setup/#test-coverage","title":"Test Coverage","text":"<pre><code># Generate coverage report\npytest --cov=apflow --cov-report=html tests/\n\n# View HTML report\nopen htmlcov/index.html  # macOS\n# or\nxdg-open htmlcov/index.html  # Linux\n</code></pre>"},{"location":"development/setup/#code-organization","title":"Code Organization","text":""},{"location":"development/setup/#module-structure","title":"Module Structure","text":"<p>Core Modules (always included with <code>pip install apflow</code>): - <code>execution/</code>: Task orchestration specifications (TaskManager, StreamingCallbacks) - <code>interfaces/</code>: Core interfaces (ExecutableTask, BaseTask, TaskStorage) - <code>storage/</code>: Storage abstractions and implementations (DuckDB default, PostgreSQL optional) - <code>utils/</code>: Utility functions</p> <p>Optional Extension Modules: - <code>extensions/crewai/</code>: CrewAI LLM task support [crewai extra]   - <code>crewai_executor.py</code>: CrewaiExecutor for LLM-based agent crews   - <code>batch_crewai_executor.py</code>: BatchCrewaiExecutor for atomic batch execution of multiple crews   - <code>types.py</code>: CrewaiExecutorState, BatchState   - Note: BatchCrewaiExecutor is included in [crewai] as it's specifically for batching CrewAI crews</p> <p>Learning Resources: - Test cases: Serve as examples (see <code>tests/integration/</code> and <code>tests/extensions/</code>)   - Integration tests demonstrate real-world usage patterns   - Extension tests show how to use specific executors   - Test cases can be used as learning templates</p> <p>Service Modules: - <code>api/</code>: API layer (A2A server, route handlers) [a2a extra] - <code>cli/</code>: Command-line interface [cli extra] Protocol Standard: The framework adopts A2A (Agent-to-Agent) Protocol as the standard protocol. See <code>api/</code> module for A2A Protocol implementation.</p>"},{"location":"development/setup/#adding-new-features","title":"Adding New Features","text":"<ol> <li>New Custom Task: Implement <code>ExecutableTask</code> interface (core)</li> <li>New CrewAI Crew: Add to <code>ext/crews/</code> [ext extra]</li> <li>New Batch: Add to <code>ext/batches/</code> [ext extra]</li> <li>New Storage Backend: Add dialect to <code>storage/dialects/</code></li> <li>New API Endpoint: Add handler to <code>api/routes/</code> (protocol-agnostic route handlers)</li> <li>New CLI Command: Add to <code>cli/commands/</code></li> </ol>"},{"location":"development/setup/#code-style","title":"Code Style","text":"<ul> <li>Line length: 100 characters</li> <li>Type hints: Use type hints for function parameters and return values</li> <li>Docstrings: Use Google-style docstrings</li> <li>Imports: Sort imports with <code>ruff</code></li> <li>Comments: Write comments in English</li> </ul>"},{"location":"development/setup/#debugging","title":"Debugging","text":""},{"location":"development/setup/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>import logging\nfrom apflow.utils.logger import get_logger\n\nlogger = get_logger(__name__)\nlogger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"development/setup/#using-debugger","title":"Using Debugger","text":"<pre><code># Run with Python debugger\npython -m pdb -m pytest tests/test_task_manager.py::TestTaskManager::test_create_task\n</code></pre>"},{"location":"development/setup/#common-issues","title":"Common Issues","text":""},{"location":"development/setup/#import-errors","title":"Import Errors","text":"<pre><code># Ensure package is installed in development mode\npip install -e \".\"\n\n# Check Python path\npython -c \"import sys; print(sys.path)\"\n</code></pre>"},{"location":"development/setup/#database-connection-issues","title":"Database Connection Issues","text":"<pre><code># For DuckDB: Check file permissions\nls -la *.duckdb\n\n# For PostgreSQL: Verify connection string\npython -c \"from sqlalchemy import create_engine; engine = create_engine('YOUR_CONNECTION_STRING'); print(engine.connect())\"\n</code></pre>"},{"location":"development/setup/#building-and-distribution","title":"Building and Distribution","text":""},{"location":"development/setup/#build-package","title":"Build Package","text":"<pre><code># Build source distribution\npython -m build\n\n# Build wheel\npython -m build --wheel\n</code></pre>"},{"location":"development/setup/#local-installation-test","title":"Local Installation Test","text":"<pre><code># Install from local build\npip install dist/apflow-0.2.0-py3-none-any.whl\n</code></pre>"},{"location":"development/setup/#contributing","title":"Contributing","text":""},{"location":"development/setup/#development-workflow_1","title":"Development Workflow","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch <pre><code>git checkout -b feature/my-feature\n</code></pre></li> <li>Make your changes</li> <li>Write code</li> <li>Add tests</li> <li>Update documentation</li> <li>Run quality checks <pre><code>black src/ tests/\nruff check --fix src/ tests/\nmypy src/apflow/\npytest tests/\n</code></pre></li> <li>Commit changes <pre><code>git commit -m \"feat: Add my feature\"\n</code></pre></li> <li>Push and create PR <pre><code>git push origin feature/my-feature\n</code></pre></li> </ol>"},{"location":"development/setup/#commit-message-format","title":"Commit Message Format","text":"<p>Follow Conventional Commits:</p> <ul> <li><code>feat:</code> - New feature</li> <li><code>fix:</code> - Bug fix</li> <li><code>docs:</code> - Documentation changes</li> <li><code>style:</code> - Code style changes (formatting)</li> <li><code>refactor:</code> - Code refactoring</li> <li><code>test:</code> - Test changes</li> <li><code>chore:</code> - Maintenance tasks</li> </ul>"},{"location":"development/setup/#pull-request-checklist","title":"Pull Request Checklist","text":"<ul> <li> Code follows project style guidelines</li> <li> Tests pass (<code>pytest tests/</code>)</li> <li> Code is formatted (<code>black src/ tests/</code>)</li> <li> No linting errors (<code>ruff check src/ tests/</code>)</li> <li> Type checking passes (<code>mypy src/apflow/</code>)</li> <li> Documentation updated (if needed)</li> <li> CHANGELOG.md updated (if needed)</li> </ul>"},{"location":"development/setup/#resources","title":"Resources","text":"<ul> <li>User Documentation: README.md</li> <li>Changelog: CHANGELOG.md</li> <li>Website: aipartnerup.com</li> <li>Issue Tracker: GitHub Issues</li> </ul>"},{"location":"development/setup/#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open a GitHub issue</li> <li>Bugs: Report via GitHub issues</li> <li>Feature Requests: Open a GitHub discussion</li> <li>Documentation: Check docs/ directory</li> </ul>"},{"location":"development/setup/#license","title":"License","text":"<p>Apache-2.0 - See LICENSE file for details.</p>"},{"location":"development/design/aggregate-results-design/","title":"Aggregate Results Design Analysis","text":""},{"location":"development/design/aggregate-results-design/#problem-background","title":"Problem Background","text":"<p><code>aggregate_results</code> is a feature for aggregating dependency task results. The current implementation is a built-in special handling in TaskManager. We need to analyze whether it should be: 1. A built-in TaskManager feature (current implementation) 2. An executor 3. Support hooks</p>"},{"location":"development/design/aggregate-results-design/#solution-comparison","title":"Solution Comparison","text":""},{"location":"development/design/aggregate-results-design/#solution-1-as-taskmanager-built-in-feature-original-implementation","title":"Solution 1: As TaskManager Built-in Feature (Original Implementation)","text":"<p>Advantages: - \u2705 Simple and direct, no additional executor needed - \u2705 Good performance, no executor instance creation required - \u2705 Centralized logic, core functionality of task orchestration - \u2705 Tightly integrated with dependency resolution flow</p> <p>Disadvantages: - \u274c Poor extensibility, custom aggregation logic requires modifying TaskManager - \u274c Inconsistent with executor system, adds special handling - \u274c Hard-coded logic, difficult to support multiple aggregation strategies - \u274c High testing and maintenance costs (special path)</p>"},{"location":"development/design/aggregate-results-design/#solution-2-as-executor-recommended","title":"Solution 2: As Executor (Recommended)","text":"<p>Advantages: - \u2705 Consistent architecture, all execution logic goes through executor - \u2705 High extensibility, users can create custom aggregation executors - \u2705 Users can customize aggregation logic without modifying the framework - \u2705 Follows single responsibility principle - \u2705 Reuses executor's testing, registration, and lifecycle management</p> <p>Disadvantages: - \u26a0\ufe0f Requires creating an executor class (but it's simple) - \u26a0\ufe0f Requires registering executor (done automatically)</p>"},{"location":"development/design/aggregate-results-design/#solution-3-support-hooks-post-hook","title":"Solution 3: Support Hooks (post-hook)","text":"<p>Advantages: - \u2705 High flexibility, users can customize aggregation in post-hook - \u2705 No framework code modification needed</p> <p>Disadvantages: - \u274c Aggregation logic is scattered, not centralized - \u274c For complex aggregation scenarios, hooks may not be intuitive enough - \u274c Requires users to understand hooks mechanism - \u274c Cannot be executed as an independent task (hooks are callbacks after task completion)</p>"},{"location":"development/design/aggregate-results-design/#final-solution-solution-2-as-executor-backward-compatibility","title":"Final Solution: Solution 2 (As Executor) + Backward Compatibility","text":""},{"location":"development/design/aggregate-results-design/#design-decision","title":"Design Decision","text":"<p>Reasons for adopting Solution 2 (As Executor):</p> <ol> <li>Architectural Consistency: All execution logic goes through executor, reducing special handling</li> <li>Extensibility: Users can create custom aggregation strategies without modifying the framework</li> <li>Maintainability: Unified executor interface, easy to test and maintain</li> <li>Backward Compatibility: Keep built-in implementation as fallback, existing code doesn't need changes</li> </ol>"},{"location":"development/design/aggregate-results-design/#implementation-details","title":"Implementation Details","text":""},{"location":"development/design/aggregate-results-design/#1-create-aggregateresultsexecutor","title":"1. Create AggregateResultsExecutor","text":"<p>Location: <code>src/apflow/extensions/core/aggregate_results_executor.py</code></p> <ul> <li>Implements <code>BaseTask</code> interface</li> <li>Uses <code>@executor_register()</code> for automatic registration</li> <li>ID: <code>aggregate_results_executor</code></li> <li>Extracts dependency results from inputs and aggregates them</li> </ul>"},{"location":"development/design/aggregate-results-design/#2-modify-taskmanager-to-support-backward-compatibility","title":"2. Modify TaskManager to Support Backward Compatibility","text":"<ul> <li>Keep support for <code>method=\"aggregate_results\"</code> (deprecated)</li> <li>Prefer using <code>aggregate_results_executor</code></li> <li>Fallback to built-in implementation if executor is not registered</li> </ul>"},{"location":"development/design/aggregate-results-design/#3-usage","title":"3. Usage","text":"<p>New Way (Recommended): <pre><code>{\n    \"schemas\": {\n        \"input_schema\": {...}\n    },\n    \"params\": {\n        \"executor_id\": \"aggregate_results_executor\"\n    },\n    \"dependencies\": [\n        {\"id\": \"task-1\", \"required\": True},\n        {\"id\": \"task-2\", \"required\": True}\n    ],\n    \"inputs\": {}  # Dependency results will be automatically merged here\n}\n</code></pre></p> <p>Old Way (Backward Compatible, Deprecated): <pre><code>{\n    \"schemas\": {\n        \"method\": \"aggregate_results\"  # Still supported, but will show warning\n    },\n    \"dependencies\": [...],\n    \"inputs\": {}\n}\n</code></pre></p>"},{"location":"development/design/aggregate-results-design/#extensibility-example","title":"Extensibility Example","text":"<p>Users can create custom aggregation executors:</p> <pre><code>@executor_register()\nclass CustomAggregator(BaseTask):\n    id = \"custom_aggregator\"\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        # Custom aggregation logic\n        # For example: calculate average, merge specific fields, etc.\n        return aggregated_result\n</code></pre>"},{"location":"development/design/aggregate-results-design/#summary","title":"Summary","text":"<p>Recommended Solution: Solution 2 (As Executor)</p> <ul> <li>\u2705 Consistent architecture, easy to maintain</li> <li>\u2705 High extensibility, supports customization</li> <li>\u2705 Backward compatible, doesn't affect existing code</li> <li>\u2705 Follows framework design principles</li> </ul> <p>Reasons for not recommending Solution 3 (Hooks): - Hooks are more suitable for post-task processing (logging, notifications, etc.) - Aggregation is core logic of task execution, should be an independent task/executor - Hooks cannot be executed as independent tasks, limited flexibility</p>"},{"location":"development/design/cli-design/","title":"CLI Design and Development Guide","text":""},{"location":"development/design/cli-design/#architecture-overview","title":"Architecture Overview","text":"<p>The CLI is designed to provide the same functionality as the API, but through command-line interface. It uses the same execution path as the API to ensure consistency.</p>"},{"location":"development/design/cli-design/#design-principles","title":"Design Principles","text":"<ol> <li>Unified Execution Path: CLI and API both use <code>TaskExecutor</code> to execute tasks</li> <li>Task Array Format: Tasks are represented as JSON arrays (same format as API)</li> <li>No Direct Executor Calls: CLI never directly calls executors (BatchCrewaiExecutor, CrewaiExecutor, etc.)</li> <li>Database as Source of Truth: Task status and results come from database</li> <li>TaskTracker for Runtime State: In-memory tracking for running tasks</li> </ol>"},{"location":"development/design/cli-design/#cli-structure","title":"CLI Structure","text":"<pre><code>apflow/\n\u251c\u2500\u2500 cli/\n\u2502   \u251c\u2500\u2500 main.py              # Main entry point, registers all commands\n\u2502   \u251c\u2500\u2500 commands/\n\u2502   \u2502   \u251c\u2500\u2500 run.py           # Execute tasks (creates task array \u2192 TaskExecutor)\n\u2502   \u2502   \u251c\u2500\u2500 tasks.py         # Query and manage tasks (list, status, count, cancel)\n\u2502   \u2502   \u251c\u2500\u2500 serve.py         # Start API server\n\u2502   \u2502   \u2514\u2500\u2500 daemon.py        # Manage daemon service\n</code></pre>"},{"location":"development/design/cli-design/#command-design","title":"Command Design","text":""},{"location":"development/design/cli-design/#1-execute-tasks-run-flow","title":"1. Execute Tasks (<code>run flow</code>)","text":"<p>Purpose: Execute tasks through TaskExecutor. Supports both single executor execution (legacy) and task array execution (standard).</p> <p>Flow: <pre><code>CLI \u2192 Parse tasks (JSON array) \u2192 Group by root \u2192 TaskExecutor.execute_tasks() (per group) \u2192 TaskManager \u2192 ExtensionRegistry \u2192 Executor\n</code></pre></p> <p>Two Modes:</p> <ol> <li> <p>Legacy Mode (backward compatible):    <pre><code># Execute single executor\napflow run flow example_executor --inputs '{\"data\": \"test\"}'\n</code></pre></p> </li> <li> <p>Standard Mode (recommended):    <pre><code># Execute task array (single task tree)\napflow run flow --tasks '[{\"id\": \"task1\", \"name\": \"Task 1\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}}]'\n\n# Execute multiple unrelated tasks (multiple root tasks)\napflow run flow --tasks '[{\"id\": \"task1\", ...}, {\"id\": \"task2\", ...}]'\n\n# With tasks file\napflow run flow --tasks-file tasks.json --output result.json\n</code></pre></p> </li> </ol> <p>Implementation: - Standard mode: Accepts tasks JSON array (list of task objects) - Legacy mode: Accepts <code>executor_id</code> + <code>inputs</code>, creates single task automatically - Multiple unrelated tasks: CLI groups tasks by root (parent_id=None), executes each group separately   - TaskExecutor only supports single root task tree   - CLI handles multiple unrelated tasks by grouping and executing separately - Calls <code>TaskExecutor.execute_tasks()</code> for each task group (same as API) - Waits for execution to complete - Retrieves result from database</p>"},{"location":"development/design/cli-design/#2-query-tasks-tasks","title":"2. Query Tasks (<code>tasks</code>)","text":"<p>Purpose: Query task status, list running tasks, count tasks, and manage tasks.</p> <p>Commands: - <code>tasks list</code> - List running tasks (from TaskTracker) - <code>tasks all</code> - List all tasks from database with filters - <code>tasks get &lt;task_id&gt;</code> - Get task details - <code>tasks status &lt;task_id&gt;...</code> - Get status of specific tasks - <code>tasks count</code> - Count running tasks - <code>tasks tree &lt;task_id&gt;</code> - Get task tree structure - <code>tasks children --parent-id &lt;id&gt;</code> - Get child tasks - <code>tasks create --file &lt;file&gt;|--stdin</code> - Create task tree - <code>tasks update &lt;task_id&gt; [options]</code> - Update task fields - <code>tasks delete &lt;task_id&gt; [--force]</code> - Delete task - <code>tasks cancel &lt;task_id&gt;... [--force] [--error-message]</code> - Cancel running tasks - <code>tasks clone &lt;task_id&gt; [--children]</code> - Clone/copy task tree (copy is an alias) - <code>tasks watch [--task-id &lt;id&gt;|--all]</code> - Watch task status in real-time</p> <p>Data Sources: - TaskTracker: In-memory set of running task IDs (fast, real-time) - Database: Full task details (status, progress, result, error)</p> <p>Example: <pre><code># List all running tasks\napflow tasks list\n\n# Check status of specific tasks\napflow tasks status task-123 task-456\n\n# Count running tasks\napflow tasks count --user-id user-123\n</code></pre></p>"},{"location":"development/design/cli-design/#3-interactive-mode-future","title":"3. Interactive Mode (Future)","text":"<p>Purpose: Provide interactive shell for continuous task management.</p> <p>Design: <pre><code># Start interactive mode\napflow interactive\n\n# In interactive mode:\n&gt; run flow executor_id --inputs '{\"data\": \"test\"}'\nTask started: task-123\n\n&gt; tasks status task-123\nStatus: in_progress, Progress: 45%\n\n&gt; tasks cancel task-123\nTask cancelled: task-123\n\n&gt; exit\n</code></pre></p>"},{"location":"development/design/cli-design/#task-execution-flow","title":"Task Execution Flow","text":""},{"location":"development/design/cli-design/#cli-execution-path","title":"CLI Execution Path","text":""},{"location":"development/design/cli-design/#standard-mode-task-array","title":"Standard Mode (Task Array)","text":"<pre><code>1. User: apflow run flow --tasks '[{\"id\": \"task1\", \"schemas\": {\"method\": \"system_info_executor\"}, ...}, {\"id\": \"task2\", ...}]'\n   \u2193\n2. CLI parses tasks JSON array\n   \u2193\n3. CLI groups tasks by root (parent_id=None)\n   - If multiple root tasks: group into separate task trees\n   - If single root task: use all tasks as one tree\n   \u2193\n4. For each task group:\n   \u2193\n5. TaskExecutor.execute_tasks(task_group)\n   \u2193\n6. TaskCreator/TaskExecutor builds task tree (single root)\n   \u2193\n7. TaskManager.distribute_task_tree()\n   \u2193\n8. TaskManager._execute_task_with_schemas()\n   \u2193\n9. ExtensionRegistry.get_by_id(executor_id)\n   \u2193\n10. Executor.execute(inputs)\n   \u2193\n11. Result saved to database\n   \u2193\n12. CLI collects all results and displays\n</code></pre>"},{"location":"development/design/cli-design/#legacy-mode-executor-id-inputs","title":"Legacy Mode (Executor ID + Inputs)","text":"<pre><code>1. User: apflow run flow example_executor --inputs '{\"key\": \"value\"}'\n   \u2193\n2. CLI creates single task:\n   [\n     {\n       \"id\": \"uuid\",\n       \"name\": \"Execute example_executor\",\n       \"user_id\": \"cli_user\",\n       \"schemas\": {\"method\": \"example_executor\"},\n       \"inputs\": {\"key\": \"value\"},\n       ...\n     }\n   ]\n   \u2193\n3. (Same as standard mode from step 3)\n</code></pre> <p>Key Points: - TaskExecutor limitation: Only supports single root task tree - CLI handles multiple unrelated tasks: Groups by root, executes separately - Standard format: Tasks array (JSON) - same as API</p>"},{"location":"development/design/cli-design/#api-execution-path-for-comparison","title":"API Execution Path (for comparison)","text":"<pre><code>1. API receives tasks array in A2A protocol message\n   \u2193\n2. AIPartnerUpFlowAgentExecutor._extract_tasks_from_context()\n   \u2193\n3. TaskExecutor.execute_tasks(tasks)  # Same as CLI!\n   \u2193\n4. ... (same as CLI from step 4)\n</code></pre> <p>Key Point: Both CLI and API use the same <code>TaskExecutor.execute_tasks()</code> method!</p>"},{"location":"development/design/cli-design/#task-status-query","title":"Task Status Query","text":""},{"location":"development/design/cli-design/#data-flow","title":"Data Flow","text":"<pre><code>CLI: tasks status task-123\n  \u2193\nTaskExecutor.is_task_running(task_id)  # Check TaskTracker (in-memory)\n  \u2193\nTaskRepository.get_task_by_id(task_id)  # Get full details from database\n  \u2193\nReturn: {\n  \"task_id\": \"task-123\",\n  \"status\": \"in_progress\",\n  \"progress\": 45.0,\n  \"is_running\": true,  # From TaskTracker\n  \"result\": null,\n  \"error\": null\n}\n</code></pre>"},{"location":"development/design/cli-design/#two-level-status","title":"Two-Level Status","text":"<ol> <li>TaskTracker (in-memory):</li> <li>Fast lookup: <code>is_task_running(task_id)</code></li> <li>Lists all running task IDs</li> <li> <p>Updated when tasks start/stop</p> </li> <li> <p>Database (persistent):</p> </li> <li>Full task details: status, progress, result, error</li> <li>Historical records</li> <li>Survives process restarts</li> </ol>"},{"location":"development/design/cli-design/#task-cancellation","title":"Task Cancellation","text":""},{"location":"development/design/cli-design/#design-implemented","title":"Design (Implemented)","text":"<p>Approach: Mark task as cancelled in database, TaskManager checks cancellation flag at multiple checkpoints.</p> <p>Flow: <pre><code>1. CLI/API: tasks cancel task-123\n   \u2193\n2. TaskRepository.update_task(task_id, status=\"cancelled\")\n   \u2193\n3. TaskManager._execute_single_task() checks status at multiple points:\n   - Before starting execution\n   - After dependency resolution\n   - Before calling executor\n   - After executor returns\n   \u2193\n4. If status == \"cancelled\", stop execution and return\n   \u2193\n5. TaskTracker stops tracking the task\n</code></pre></p> <p>Supported States: - \u2705 pending: Can be cancelled (will not start execution) - \u2705 in_progress: Can be cancelled (TaskManager will check and stop at next checkpoint) - \u274c completed/failed/cancelled: Cannot be cancelled (already finished)</p> <p>Executor-Level Cancellation Support:</p> Executor <code>cancelable</code> Cancellation Support Notes BatchCrewaiExecutor <code>True</code> \u2705 Supported Checks cancellation before each work. Preserves token_usage from completed works. Can stop before executing remaining works. CrewaiExecutor <code>False</code> \u274c Not Supported During Execution CrewAI Limitation: <code>kickoff()</code> is a synchronous blocking call with no cancellation support. Cancellation can only be checked before execution starts. If cancelled during execution, the crew will complete normally, then TaskManager will mark it as cancelled. Token_usage is preserved. CommandExecutor <code>False</code> \u274c Not Implemented Cancellation checking not implemented. Could be added by checking <code>cancellation_checker</code> during subprocess execution. SystemInfoExecutor <code>False</code> \u274c Not Supported Fast execution (&lt; 1 second), cancellation not needed. Other Executors <code>False</code> (default) \u26a0\ufe0f Varies Depends on executor implementation. TaskManager checks before/after execution. Executors can set <code>cancelable=True</code> and implement cancellation checking in their own execution loops. <p>Implementation Details:</p> <ol> <li>TaskManager Level (Always Active):</li> <li>Checks cancellation at multiple checkpoints:<ul> <li>Before starting execution</li> <li>After dependency resolution</li> <li>Before calling executor</li> <li>After executor returns</li> </ul> </li> <li> <p>If cancelled, stops execution immediately</p> </li> <li> <p>Executor Level (Optional, Implemented for BatchCrewaiExecutor):</p> </li> <li>BatchCrewaiExecutor: Checks cancellation before each work execution (can stop mid-batch)</li> <li>CrewaiExecutor: Checks cancellation before execution only (CrewAI limitation: cannot cancel during execution)</li> <li> <p>SystemInfoExecutor: No cancellation needed (fast execution)</p> </li> <li> <p>Token Usage Preservation:</p> </li> <li>\u2705 BatchCrewaiExecutor: Preserves token_usage from completed works when cancelled</li> <li>\u2705 CrewaiExecutor: Preserves token_usage even if cancelled</li> <li>\u2705 All executors: Token_usage is preserved in result even on cancellation</li> </ol> <p>Limitations: - CrewAI Library Limitation: CrewAI's <code>kickoff()</code> is a synchronous blocking call with no cancellation support. Once <code>kickoff()</code> starts executing, it cannot be interrupted. This means:   - Cancellation can only be checked before execution starts   - If cancellation is requested during execution, the crew will complete normally   - TaskManager will detect cancellation after execution and mark the task as cancelled   - Token usage is still preserved even if cancelled   - This is a fundamental limitation of the CrewAI library, not our implementation - Long-running crews: For very long-running crews, cancellation will not take effect until the crew completes. This is unavoidable due to CrewAI's design. - Force cancellation: Currently, force cancellation (<code>--force</code>) only affects the error message, not the cancellation behavior. True force cancellation (immediate stop) would require process/thread termination, which is not yet implemented and may not be possible for CrewAI crews.</p> <p>Implementation Points: - \u2705 TaskManager checks cancellation flag at multiple checkpoints - \u2705 CLI cancel command implemented - \u2705 BatchCrewaiExecutor executor-level cancellation check (before each work, can stop mid-batch) - \u2705 CrewaiExecutor executor-level cancellation check (before execution only) - \u2705 Token_usage preservation on cancellation - \u274c CrewAI library limitation: No cancellation during execution (CrewAI's <code>kickoff()</code> is blocking with no cancellation support) - \u26a0\ufe0f Force cancellation (immediate stop via process termination) - not yet implemented</p>"},{"location":"development/design/cli-design/#interactive-mode-design","title":"Interactive Mode Design","text":""},{"location":"development/design/cli-design/#use-cases","title":"Use Cases","text":"<ol> <li>Continuous Task Management: Start tasks, monitor status, cancel if needed</li> <li>Development/Testing: Quick iteration without restarting CLI</li> <li>Monitoring: Watch multiple tasks simultaneously</li> </ol>"},{"location":"development/design/cli-design/#implementation-options","title":"Implementation Options","text":"<p>Option 1: Simple REPL <pre><code>import cmd\n\nclass InteractiveShell(cmd.Cmd):\n    prompt = 'apflow&gt; '\n\n    def do_run(self, args):\n        # Parse and execute run command\n        pass\n\n    def do_tasks(self, args):\n        # Parse and execute tasks command\n        pass\n</code></pre></p> <p>Option 2: Rich Interactive UI - Use <code>rich</code> library for better UI - Real-time status updates - Table view of running tasks - Color-coded status</p> <p>Option 3: Watch Mode <pre><code># Watch task status in real-time\napflow tasks watch task-123\n\n# Watch all tasks\napflow tasks watch --all\n</code></pre></p>"},{"location":"development/design/cli-design/#development-guidelines","title":"Development Guidelines","text":""},{"location":"development/design/cli-design/#adding-new-commands","title":"Adding New Commands","text":"<ol> <li>Create command file in <code>cli/commands/</code></li> <li>Use Typer for command definition</li> <li>Follow the pattern: CLI \u2192 TaskExecutor \u2192 Database</li> <li>Never directly call executors</li> </ol>"},{"location":"development/design/cli-design/#example-adding-a-new-command","title":"Example: Adding a New Command","text":"<pre><code># cli/commands/my_command.py\nimport typer\nfrom apflow.core.execution.task_executor import TaskExecutor\n\napp = typer.Typer(name=\"mycommand\")\n\n@app.command()\ndef do_something(task_id: str):\n    \"\"\"Do something with a task\"\"\"\n    task_executor = TaskExecutor()\n    # Use task_executor methods\n    # Query database via TaskRepository\n    # Never call executors directly\n</code></pre>"},{"location":"development/design/cli-design/#error-handling","title":"Error Handling","text":"<ul> <li>Use <code>typer.Exit(1)</code> for errors</li> <li>Log errors with <code>logger.exception()</code></li> <li>Provide helpful error messages</li> <li>Return JSON for programmatic use</li> </ul>"},{"location":"development/design/cli-design/#output-format","title":"Output Format","text":"<ul> <li>Human-readable: Use <code>typer.echo()</code> with formatting</li> <li>Machine-readable: Use JSON output (for scripts)</li> <li>Interactive: Use <code>rich</code> for tables, progress bars</li> </ul>"},{"location":"development/design/cli-design/#multiple-unrelated-tasks-support","title":"Multiple Unrelated Tasks Support","text":""},{"location":"development/design/cli-design/#problem","title":"Problem","text":"<p><code>TaskExecutor.execute_tasks()</code> only supports single root task tree. If you pass multiple unrelated tasks (multiple tasks with <code>parent_id=None</code>), TaskExecutor will only process the first root task.</p>"},{"location":"development/design/cli-design/#solution-cli-level-grouping","title":"Solution: CLI-Level Grouping","text":"<p>CLI handles this by: 1. Grouping tasks by root: <code>_group_tasks_by_root()</code> function groups tasks into separate task trees 2. Executing separately: Each task group is executed via <code>TaskExecutor.execute_tasks()</code> separately 3. Collecting results: All results are collected and returned together</p>"},{"location":"development/design/cli-design/#example","title":"Example","text":"<pre><code># Multiple unrelated tasks\napflow run flow --tasks '[\n  {\"id\": \"task1\", \"name\": \"Get CPU Info\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}},\n  {\"id\": \"task2\", \"name\": \"Get Memory Info\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"memory\"}}\n]'\n\n# CLI will:\n# 1. Detect 2 root tasks (both have parent_id=None)\n# 2. Group into 2 separate task groups\n# 3. Execute group 1: TaskExecutor.execute_tasks([task1])\n# 4. Execute group 2: TaskExecutor.execute_tasks([task2])\n# 5. Return combined results\n</code></pre>"},{"location":"development/design/cli-design/#task-tree-vs-unrelated-tasks","title":"Task Tree vs Unrelated Tasks","text":"<ul> <li>Task Tree: Tasks with parent-child relationships   <pre><code>[\n  {\"id\": \"root\", \"parent_id\": null, ...},\n  {\"id\": \"child1\", \"parent_id\": \"root\", ...},\n  {\"id\": \"child2\", \"parent_id\": \"root\", ...}\n]\n</code></pre></li> <li>Single root task</li> <li> <p>Executed as one task tree</p> </li> <li> <p>Unrelated Tasks: Multiple independent tasks   <pre><code>[\n  {\"id\": \"task1\", \"parent_id\": null, ...},\n  {\"id\": \"task2\", \"parent_id\": null, ...}\n]\n</code></pre></p> </li> <li>Multiple root tasks</li> <li>CLI groups and executes separately</li> </ul>"},{"location":"development/design/cli-design/#comparison-cli-vs-api","title":"Comparison: CLI vs API","text":"Feature CLI API Execution <code>TaskExecutor.execute_tasks()</code> (per group) <code>TaskExecutor.execute_tasks()</code> Task Format JSON array (tasks list) JSON array (A2A protocol) Multiple Unrelated Tasks \u2705 Supported (CLI groups by root) \u274c Not supported (single root only) Streaming Not yet supported Supported via EventQueue Status Query <code>TaskTracker + Database</code> <code>TaskTracker + Database</code> Cancellation \u2705 Implemented \u2705 Implemented Hooks Supported (via TaskExecutor) Supported (via TaskExecutor) Database Same database Same database"},{"location":"development/design/cli-design/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Interactive Mode: REPL for continuous task management</li> <li>Watch Mode: Real-time status monitoring</li> <li>Streaming Support: Real-time progress updates in CLI</li> <li>Task Filtering: Filter by status, user, date range</li> </ol>"},{"location":"examples/basic_task/","title":"Basic Task Examples","text":"<p>See also: - Real-World Examples for advanced, production scenarios - Task Tree Examples for dependency and execution order patterns</p> <p>This document provides practical, copy-paste ready examples for common use cases with apflow. Each example is complete and runnable.</p>"},{"location":"examples/basic_task/#before-you-start","title":"Before You Start","text":"<p>Prerequisites: - apflow installed: <code>pip install apflow</code> - Python 3.10+ with async/await support - Basic understanding of Python</p> <p>What You'll Learn: - How to use built-in executors - How to create custom executors - How to work with task dependencies - How to handle errors - Common patterns and best practices</p>"},{"location":"examples/basic_task/#quick-taskbuilder-fluent-configmanager","title":"Quick: TaskBuilder (Fluent) + ConfigManager","text":"<pre><code>import asyncio\nfrom pathlib import Path\n\nfrom apflow import TaskManager, create_session\nfrom apflow.core.builders import TaskBuilder\nfrom apflow.core.config_manager import get_config_manager\n\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    config = get_config_manager()\n    config.register_pre_hook(lambda task: task.inputs.update({\"source\": \"config_manager\"}))\n    config.load_env_files([Path(\".env\")], override=False)\n\n    builder = TaskBuilder(task_manager, \"system_info_executor\")\n    result = await (\n        builder.with_name(\"System Info\")\n        .with_inputs({\"resource\": \"cpu\"})\n        .enable_demo_mode(sleep_scale=0.5)\n        .execute()\n    )\n    print(result[\"result\"])\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <ul> <li>Builder handles task creation + execution fluently (name, inputs, demo mode).</li> <li>ConfigManager keeps env loading and dynamic hooks in one place for CLI/API.</li> <li>See docs/api/quick-reference.md     for the decorator vs ConfigManager hook pattern.</li> </ul>"},{"location":"examples/basic_task/#example-1-using-built-in-executor-simplest","title":"Example 1: Using Built-in Executor (Simplest)","text":"<p>What it does: Gets system CPU information using the built-in <code>system_info_executor</code>.</p> <p>Why start here: No custom code needed - just use what's already available!</p>"},{"location":"examples/basic_task/#complete-runnable-code","title":"Complete Runnable Code","text":"<p>Create <code>example_01_builtin.py</code>:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    # Step 1: Setup\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Step 2: Create task using built-in executor\n    # system_info_executor is already registered - just use it!\n    task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",  # Built-in executor ID\n        user_id=\"example_user\",\n        inputs={\"resource\": \"cpu\"}    # Get CPU info\n    )\n\n    # Step 3: Execute\n    task_tree = TaskTreeNode(task)\n    await task_manager.distribute_task_tree(task_tree)\n\n    # Step 4: Get result\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Status: {result.status}\")\n    print(f\"Result: {result.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#run-it","title":"Run It","text":"<pre><code>python example_01_builtin.py\n</code></pre>"},{"location":"examples/basic_task/#expected-output","title":"Expected Output","text":"<pre><code>Status: completed\nResult: {'system': 'Darwin', 'cores': 8, 'cpu_count': 8, ...}\n</code></pre>"},{"location":"examples/basic_task/#understanding-the-code","title":"Understanding the Code","text":"<ol> <li><code>create_session()</code>: Creates a database connection (DuckDB by default)</li> <li><code>TaskManager(db)</code>: Creates the orchestrator</li> <li><code>create_task()</code>: Creates a task definition</li> <li><code>name</code>: Must match an executor ID</li> <li><code>inputs</code>: Parameters for the executor</li> <li><code>TaskTreeNode()</code>: Wraps task in tree structure</li> <li><code>distribute_task_tree()</code>: Executes the task</li> <li><code>get_task_by_id()</code>: Retrieves updated task with results</li> </ol>"},{"location":"examples/basic_task/#try-modifying","title":"Try Modifying","text":"<pre><code># Get memory instead\ninputs={\"resource\": \"memory\"}\n\n# Get disk instead\ninputs={\"resource\": \"disk\"}\n\n# Get all resources\ninputs={\"resource\": \"all\"}\n</code></pre>"},{"location":"examples/basic_task/#example-2-simple-custom-executor","title":"Example 2: Simple Custom Executor","text":"<p>What it does: Creates a custom executor that processes text data.</p> <p>Why this example: Shows the basic pattern for creating custom executors.</p>"},{"location":"examples/basic_task/#complete-runnable-code_1","title":"Complete Runnable Code","text":"<p>Create <code>example_02_custom.py</code>:</p> <pre><code>import asyncio\nfrom apflow import BaseTask, executor_register, TaskManager, TaskTreeNode, create_session\nfrom typing import ClassVar, Dict, Any, Literal\nfrom pydantic import BaseModel, Field\n\n# Step 1: Define input schema\nclass TextProcessorInputSchema(BaseModel):\n    \"\"\"Input schema for text processor\"\"\"\n    text: str = Field(description=\"Text to process\")\n    operation: Literal[\"count\", \"reverse\", \"uppercase\"] = Field(\n        default=\"count\", description=\"Operation to perform\"\n    )\n\n# Step 2: Define your custom executor\n@executor_register()\nclass TextProcessor(BaseTask):\n    \"\"\"Processes text data\"\"\"\n\n    id = \"text_processor\"\n    name = \"Text Processor\"\n    description = \"Processes text: count words, reverse, uppercase\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = TextProcessorInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute text processing\"\"\"\n        text = inputs.get(\"text\", \"\")\n        operation = inputs.get(\"operation\", \"count\")\n\n        if operation == \"count\":\n            result = len(text.split())\n        elif operation == \"reverse\":\n            result = text[::-1]\n        elif operation == \"uppercase\":\n            result = text.upper()\n        else:\n            raise ValueError(f\"Unknown operation: {operation}\")\n\n        return {\n            \"operation\": operation,\n            \"input_text\": text,\n            \"result\": result\n        }\n\n# Step 2: Use your executor\nasync def main():\n    # Import the executor (auto-registered via decorator)\n    from example_02_custom import TextProcessor\n\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create task using your custom executor\n    task = await task_manager.task_repository.create_task(\n        name=\"text_processor\",  # Must match executor ID\n        user_id=\"example_user\",\n        inputs={\n            \"text\": \"Hello, apflow!\",\n            \"operation\": \"count\"\n        }\n    )\n\n    # Execute\n    task_tree = TaskTreeNode(task)\n    await task_manager.distribute_task_tree(task_tree)\n\n    # Get result\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Result: {result.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#run-it_1","title":"Run It","text":"<pre><code>python example_02_custom.py\n</code></pre>"},{"location":"examples/basic_task/#expected-output_1","title":"Expected Output","text":"<pre><code>Result: {'operation': 'count', 'input_text': 'Hello, apflow!', 'result': 2}\n</code></pre>"},{"location":"examples/basic_task/#understanding-the-code_1","title":"Understanding the Code","text":"<p>Key Points: - <code>@executor_register()</code>: Automatically registers the executor - <code>id</code>: Must be unique, used in <code>name</code> when creating tasks - <code>execute()</code>: Async function that does the actual work - <code>inputs_schema</code>: Pydantic model defining expected inputs (auto-converted to JSON Schema)</p> <p>Try Different Operations: <pre><code>inputs={\"text\": \"Hello\", \"operation\": \"reverse\"}  # \"olleH\"\ninputs={\"text\": \"Hello\", \"operation\": \"uppercase\"}  # \"HELLO\"\n</code></pre></p>"},{"location":"examples/basic_task/#example-3-http-api-call-task","title":"Example 3: HTTP API Call Task","text":"<p>What it does: Calls an external HTTP API and returns the response.</p> <p>Why this example: Shows how to integrate with external services.</p>"},{"location":"examples/basic_task/#complete-runnable-code_2","title":"Complete Runnable Code","text":"<p>Create <code>example_03_api.py</code>:</p> <pre><code>import asyncio\nimport aiohttp\nfrom apflow import BaseTask, executor_register, TaskManager, TaskTreeNode, create_session\nfrom typing import ClassVar, Dict, Any, Literal, Optional\nfrom pydantic import BaseModel, Field\n\nclass APICallInputSchema(BaseModel):\n    \"\"\"Input schema for API call task\"\"\"\n    url: str = Field(description=\"API endpoint URL\")\n    method: Literal[\"GET\", \"POST\"] = Field(default=\"GET\", description=\"HTTP method\")\n    data: Optional[Dict[str, Any]] = Field(default=None, description=\"Request body for POST requests\")\n    headers: Optional[Dict[str, str]] = Field(default=None, description=\"HTTP headers\")\n\n@executor_register()\nclass APICallTask(BaseTask):\n    \"\"\"Calls an external HTTP API\"\"\"\n\n    id = \"api_call_task\"\n    name = \"API Call Task\"\n    description = \"Calls an external HTTP API and returns the response\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = APICallInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute API call\"\"\"\n        url = inputs.get(\"url\")\n        method = inputs.get(\"method\", \"GET\")\n        data = inputs.get(\"data\")\n        headers = inputs.get(\"headers\", {})\n\n        async with aiohttp.ClientSession() as session:\n            try:\n                if method == \"GET\":\n                    async with session.get(url, headers=headers) as response:\n                        result = await response.json()\n                        status_code = response.status\n                elif method == \"POST\":\n                    async with session.post(url, json=data, headers=headers) as response:\n                        result = await response.json()\n                        status_code = response.status\n                else:\n                    raise ValueError(f\"Unsupported method: {method}\")\n\n                return {\n                    \"status\": \"completed\",\n                    \"status_code\": status_code,\n                    \"data\": result\n                }\n            except Exception as e:\n                return {\n                    \"status\": \"failed\",\n                    \"error\": str(e)\n                }\n\nasync def main():\n    from example_03_api import APICallTask\n\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Call a public API (JSONPlaceholder)\n    task = await task_manager.task_repository.create_task(\n        name=\"api_call_task\",\n        user_id=\"example_user\",\n        inputs={\n            \"url\": \"https://jsonplaceholder.typicode.com/posts/1\",\n            \"method\": \"GET\"\n        }\n    )\n\n    task_tree = TaskTreeNode(task)\n    await task_manager.distribute_task_tree(task_tree)\n\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Status: {result.status}\")\n    if result.status == \"completed\":\n        print(f\"API Response: {result.result['data']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#run-it_2","title":"Run It","text":"<pre><code># Install aiohttp if needed\npip install aiohttp\npython example_03_api.py\n</code></pre>"},{"location":"examples/basic_task/#understanding-the-code_2","title":"Understanding the Code","text":"<p>Error Handling: - Wrapped in try/except to handle network errors - Returns error information in result - Task status will be \"failed\" if exception occurs</p> <p>Try Different APIs: <pre><code># POST request\ninputs={\n    \"url\": \"https://api.example.com/data\",\n    \"method\": \"POST\",\n    \"data\": {\"key\": \"value\"}\n}\n\n# With headers\ninputs={\n    \"url\": \"https://api.example.com/data\",\n    \"headers\": {\"Authorization\": \"Bearer token\"}\n}\n</code></pre></p>"},{"location":"examples/basic_task/#example-4-task-with-dependencies","title":"Example 4: Task with Dependencies","text":"<p>What it does: Creates a pipeline where tasks depend on each other.</p> <p>Why this example: Shows how dependencies control execution order.</p>"},{"location":"examples/basic_task/#complete-runnable-code_3","title":"Complete Runnable Code","text":"<p>Create <code>example_04_dependencies.py</code>:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Task 1: Get CPU info\n    cpu_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        priority=1,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Task 2: Get memory info (depends on CPU task)\n    # This will wait for cpu_task to complete!\n    memory_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        parent_id=cpu_task.id,  # Organizational\n        dependencies=[{\"id\": cpu_task.id, \"required\": True}],  # Execution order\n        priority=2,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    # Task 3: Get disk info (depends on memory task)\n    disk_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        parent_id=cpu_task.id,\n        dependencies=[{\"id\": memory_task.id, \"required\": True}],\n        priority=2,\n        inputs={\"resource\": \"disk\"}\n    )\n\n    # Build task tree\n    root = TaskTreeNode(cpu_task)\n    root.add_child(TaskTreeNode(memory_task))\n    root.add_child(TaskTreeNode(disk_task))\n\n    # Execute\n    # Execution order: CPU \u2192 Memory \u2192 Disk (automatic!)\n    await task_manager.distribute_task_tree(root)\n\n    # Check results\n    cpu_result = await task_manager.task_repository.get_task_by_id(cpu_task.id)\n    memory_result = await task_manager.task_repository.get_task_by_id(memory_task.id)\n    disk_result = await task_manager.task_repository.get_task_by_id(disk_task.id)\n\n    print(f\"\u2705 CPU: {cpu_result.status}\")\n    print(f\"\u2705 Memory: {memory_result.status}\")\n    print(f\"\u2705 Disk: {disk_result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#understanding-dependencies","title":"Understanding Dependencies","text":"<p>Key Concept: <code>dependencies</code> control execution order, not <code>parent_id</code>!</p> <pre><code>Execution Flow:\nCPU Task (no dependencies) \u2192 runs first\n    \u2193\nMemory Task (depends on CPU) \u2192 waits, then runs\n    \u2193\nDisk Task (depends on Memory) \u2192 waits, then runs\n</code></pre> <p>Visual Representation: <pre><code>Root Task\n\u2502\n\u251c\u2500\u2500 CPU Task (runs first)\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 Memory Task (waits for CPU, then runs)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u2514\u2500\u2500 Disk Task (waits for Memory, then runs)\n</code></pre></p>"},{"location":"examples/basic_task/#example-5-parallel-tasks","title":"Example 5: Parallel Tasks","text":"<p>What it does: Creates multiple tasks that run in parallel (no dependencies).</p> <p>Why this example: Shows how tasks without dependencies execute simultaneously.</p>"},{"location":"examples/basic_task/#complete-runnable-code_4","title":"Complete Runnable Code","text":"<p>Create <code>example_05_parallel.py</code>:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create root task\n    root_task = await task_manager.task_repository.create_task(\n        name=\"root_task\",\n        user_id=\"example_user\",\n        priority=1\n    )\n\n    # Create three tasks with NO dependencies\n    # They can all run in parallel!\n    cpu_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        parent_id=root_task.id,\n        priority=2,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    memory_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        parent_id=root_task.id,\n        priority=2,  # Same priority\n        inputs={\"resource\": \"memory\"}\n        # No dependencies - can run in parallel with cpu_task!\n    )\n\n    disk_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        parent_id=root_task.id,\n        priority=2,\n        inputs={\"resource\": \"disk\"}\n        # No dependencies - can run in parallel!\n    )\n\n    # Build task tree\n    root = TaskTreeNode(root_task)\n    root.add_child(TaskTreeNode(cpu_task))\n    root.add_child(TaskTreeNode(memory_task))\n    root.add_child(TaskTreeNode(disk_task))\n\n    # Execute\n    # All three tasks run in parallel (no dependencies)\n    await task_manager.distribute_task_tree(root)\n\n    # Check results\n    tasks = [cpu_task, memory_task, disk_task]\n    for task in tasks:\n        result = await task_manager.task_repository.get_task_by_id(task.id)\n        print(f\"\u2705 {task.id}: {result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#understanding-parallel-execution","title":"Understanding Parallel Execution","text":"<p>When tasks run in parallel: - They have no dependencies on each other - They have the same priority (or compatible priorities) - TaskManager automatically handles parallel execution</p> <p>Performance Benefit: - 3 tasks in parallel = ~3x faster than sequential - Great for independent operations!</p>"},{"location":"examples/basic_task/#example-6-data-processing-pipeline","title":"Example 6: Data Processing Pipeline","text":"<p>What it does: Creates a complete pipeline: fetch \u2192 process \u2192 save.</p> <p>Why this example: Shows a real-world pattern with multiple steps.</p>"},{"location":"examples/basic_task/#complete-runnable-code_5","title":"Complete Runnable Code","text":"<p>Create <code>example_06_pipeline.py</code>:</p> <pre><code>import asyncio\nfrom apflow import BaseTask, executor_register, TaskManager, TaskTreeNode, create_session\nfrom typing import ClassVar, Dict, Any, Literal, Optional\nfrom pydantic import BaseModel, Field\n\n# Step 1: Fetch data executor\nclass FetchDataInputSchema(BaseModel):\n    \"\"\"Input schema for fetch data task\"\"\"\n    source: str = Field(default=\"api\", description=\"Data source\")\n\n@executor_register()\nclass FetchDataTask(BaseTask):\n    \"\"\"Fetches data from a source\"\"\"\n\n    id = \"fetch_data\"\n    name = \"Fetch Data\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = FetchDataInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        data_source = inputs.get(\"source\", \"api\")\n        return {\n            \"source\": data_source,\n            \"data\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        }\n\n# Step 2: Process data executor\nclass ProcessDataInputSchema(BaseModel):\n    \"\"\"Input schema for process data task\"\"\"\n    data: list[float] = Field(description=\"Array of numbers\")\n    operation: Literal[\"sum\", \"average\", \"count\"] = Field(\n        default=\"sum\", description=\"Operation to perform\"\n    )\n\n@executor_register()\nclass ProcessDataTask(BaseTask):\n    \"\"\"Processes data\"\"\"\n\n    id = \"process_data\"\n    name = \"Process Data\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = ProcessDataInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        data = inputs.get(\"data\", [])\n        operation = inputs.get(\"operation\", \"sum\")\n\n        if operation == \"sum\":\n            result = sum(data)\n        elif operation == \"average\":\n            result = sum(data) / len(data) if data else 0\n        else:\n            result = len(data)\n\n        return {\n            \"operation\": operation,\n            \"input_count\": len(data),\n            \"result\": result\n        }\n\n# Step 3: Save results executor\nclass SaveResultsInputSchema(BaseModel):\n    \"\"\"Input schema for save results task\"\"\"\n    result: float = Field(description=\"Result to save\")\n\n@executor_register()\nclass SaveResultsTask(BaseTask):\n    \"\"\"Saves results\"\"\"\n\n    id = \"save_results\"\n    name = \"Save Results\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = SaveResultsInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        result = inputs.get(\"result\")\n        return {\n            \"saved\": True,\n            \"result\": result,\n            \"timestamp\": \"2024-01-01T00:00:00Z\"\n        }\n\nasync def main():\n    # Import executors\n    from example_06_pipeline import FetchDataTask, ProcessDataTask, SaveResultsTask\n\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Step 1: Fetch data\n    fetch_task = await task_manager.task_repository.create_task(\n        name=\"fetch_data\",\n        user_id=\"example_user\",\n        priority=1,\n        inputs={\"source\": \"api\"}\n    )\n\n    # Step 2: Process data (depends on fetch)\n    process_task = await task_manager.task_repository.create_task(\n        name=\"process_data\",\n        user_id=\"example_user\",\n        parent_id=fetch_task.id,\n        dependencies=[{\"id\": fetch_task.id, \"required\": True}],\n        priority=2,\n        inputs={\n            \"data\": [],  # Will be populated from fetch_task result\n            \"operation\": \"average\"\n        }\n    )\n\n    # Step 3: Save results (depends on process)\n    save_task = await task_manager.task_repository.create_task(\n        name=\"save_results\",\n        user_id=\"example_user\",\n        parent_id=fetch_task.id,\n        dependencies=[{\"id\": process_task.id, \"required\": True}],\n        priority=3,\n        inputs={\"result\": 0}  # Will be populated from process_task result\n    )\n\n    # Build pipeline\n    root = TaskTreeNode(fetch_task)\n    root.add_child(TaskTreeNode(process_task))\n    root.add_child(TaskTreeNode(save_task))\n\n    # Execute pipeline\n    # Order: Fetch \u2192 Process \u2192 Save (automatic!)\n    await task_manager.distribute_task_tree(root)\n\n    # Check final result\n    save_result = await task_manager.task_repository.get_task_by_id(save_task.id)\n    print(f\"\u2705 Pipeline completed: {save_result.status}\")\n    print(f\"\ud83d\udcbe Saved result: {save_result.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#understanding-the-pipeline","title":"Understanding the Pipeline","text":"<p>Execution Flow: <pre><code>Fetch Data (gets data)\n    \u2193\nProcess Data (processes fetched data)\n    \u2193\nSave Results (saves processed result)\n</code></pre></p> <p>Key Points: - Each step depends on the previous - TaskManager handles dependency resolution automatically - Results flow from one task to the next</p>"},{"location":"examples/basic_task/#example-7-error-handling","title":"Example 7: Error Handling","text":"<p>What it does: Shows how to handle errors gracefully in custom executors.</p> <p>Why this example: Error handling is crucial for production code.</p>"},{"location":"examples/basic_task/#complete-runnable-code_6","title":"Complete Runnable Code","text":"<p>Create <code>example_07_errors.py</code>:</p> <pre><code>import asyncio\nfrom apflow import BaseTask, executor_register, TaskManager, TaskTreeNode, create_session\nfrom typing import ClassVar, Dict, Any\nfrom pydantic import BaseModel, Field\n\nclass RobustTaskInputSchema(BaseModel):\n    \"\"\"Input schema for robust task\"\"\"\n    data: list[float] = Field(description=\"Array of numbers\")\n\n@executor_register()\nclass RobustTask(BaseTask):\n    \"\"\"Task with comprehensive error handling\"\"\"\n\n    id = \"robust_task\"\n    name = \"Robust Task\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = RobustTaskInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute with error handling\"\"\"\n        try:\n            # Validate inputs against schema\n            self.check_input_schema(inputs)\n\n            data = inputs.get(\"data\")\n\n            # Process data\n            result = sum(data) / len(data) if data else 0\n\n            return {\n                \"status\": \"completed\",\n                \"result\": result,\n                \"processed_count\": len(data)\n            }\n        except ValueError as e:\n            # Validation errors - return error info\n            return {\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"error_type\": \"validation_error\"\n            }\n        except Exception as e:\n            # Other errors\n            return {\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"error_type\": \"execution_error\"\n            }\n\nasync def main():\n    from example_07_errors import RobustTask\n\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Test 1: Valid input\n    print(\"Test 1: Valid input\")\n    task1 = await task_manager.task_repository.create_task(\n        name=\"robust_task\",\n        user_id=\"example_user\",\n        inputs={\"data\": [1, 2, 3, 4, 5]}\n    )\n    tree1 = TaskTreeNode(task1)\n    await task_manager.distribute_task_tree(tree1)\n    result1 = await task_manager.task_repository.get_task_by_id(task1.id)\n    print(f\"Status: {result1.status}\")\n    print(f\"Result: {result1.result}\\n\")\n\n    # Test 2: Invalid input (missing data)\n    print(\"Test 2: Invalid input\")\n    task2 = await task_manager.task_repository.create_task(\n        name=\"robust_task\",\n        user_id=\"example_user\",\n        inputs={}  # Missing required field\n    )\n    tree2 = TaskTreeNode(task2)\n    await task_manager.distribute_task_tree(tree2)\n    result2 = await task_manager.task_repository.get_task_by_id(task2.id)\n    print(f\"Status: {result2.status}\")\n    print(f\"Error: {result2.result.get('error')}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#understanding-error-handling","title":"Understanding Error Handling","text":"<p>Best Practices: 1. Validate inputs early: Check requirements before processing 2. Return error information: Include error type and message 3. Don't raise exceptions: Return error info in result instead 4. Check task status: Always check <code>task.status</code> after execution</p> <p>Error States: - <code>status == \"failed\"</code>: Task execution failed - <code>task.error</code>: Error message (if available) - <code>task.result</code>: May contain error information</p>"},{"location":"examples/basic_task/#example-8-using-crewai-llm-tasks","title":"Example 8: Using CrewAI (LLM Tasks)","text":"<p>What it does: Uses CrewAI to execute LLM-based tasks.</p> <p>Why this example: Shows how to use optional LLM features.</p>"},{"location":"examples/basic_task/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install apflow[crewai]\n</code></pre>"},{"location":"examples/basic_task/#complete-runnable-code_7","title":"Complete Runnable Code","text":"<p>Create <code>example_08_crewai.py</code>:</p> <pre><code>import asyncio\nfrom apflow.extensions.crewai import CrewaiExecutor\nfrom apflow import TaskManager, TaskTreeNode, create_session\nfrom apflow.core.extensions import get_registry\n\nasync def main():\n    # Create a CrewAI executor\n    crew = CrewaiExecutor(\n        id=\"simple_analysis_crew\",\n        name=\"Simple Analysis Crew\",\n        description=\"Analyzes text using AI\",\n        agents=[\n            {\n                \"role\": \"Analyst\",\n                \"goal\": \"Analyze the provided text and extract key insights\",\n                \"backstory\": \"You are an expert data analyst\"\n            }\n        ],\n        tasks=[\n            {\n                \"description\": \"Analyze the following text: {text}\",\n                \"agent\": \"Analyst\"\n            }\n        ]\n    )\n\n    # Register the configured instance\n    get_registry().register(crew)\n\n    # Use it via TaskManager\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    task = await task_manager.task_repository.create_task(\n        name=\"simple_analysis_crew\",  # Must match crew ID\n        user_id=\"example_user\",\n        inputs={\n            \"text\": \"Sales increased by 20% this quarter. Customer satisfaction is at 95%.\"\n        }\n    )\n\n    task_tree = TaskTreeNode(task)\n    await task_manager.distribute_task_tree(task_tree)\n\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Status: {result.status}\")\n    if result.status == \"completed\":\n        print(f\"Analysis: {result.result}\")\n\nif __name__ == \"__main__\":\n    # Note: Requires LLM API key\n    # Set via environment variable or request header\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#understanding-crewai-integration","title":"Understanding CrewAI Integration","text":"<p>Key Points: - CrewAI is optional (requires <code>[crewai]</code> extra) - CrewaiExecutor is a special executor that needs configuration - Register the configured instance before use - LLM API keys are required (OpenAI, Anthropic, etc.)</p> <p>Setting LLM API Key: <pre><code># Via environment variable\nexport OPENAI_API_KEY=\"sk-your-key\"\n\n# Or via request header (for API server)\nX-LLM-API-KEY: openai:sk-your-key\n</code></pre></p>"},{"location":"examples/basic_task/#example-9-task-priorities","title":"Example 9: Task Priorities","text":"<p>What it does: Shows how priorities control execution order.</p> <p>Why this example: Priorities help manage task scheduling.</p>"},{"location":"examples/basic_task/#complete-runnable-code_8","title":"Complete Runnable Code","text":"<p>Create <code>example_09_priorities.py</code>:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create root task\n    root = await task_manager.task_repository.create_task(\n        name=\"root_task\",\n        user_id=\"example_user\",\n        priority=1\n    )\n\n    # Priority 0 = urgent (highest)\n    urgent = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        parent_id=root.id,\n        priority=0,  # Executes first!\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Priority 2 = normal\n    normal = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        parent_id=root.id,\n        priority=2,  # Executes after urgent\n        inputs={\"resource\": \"memory\"}\n    )\n\n    # Priority 3 = low\n    low = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"example_user\",\n        parent_id=root.id,\n        priority=3,  # Executes last\n        inputs={\"resource\": \"disk\"}\n    )\n\n    # Build tree\n    tree = TaskTreeNode(root)\n    tree.add_child(TaskTreeNode(urgent))\n    tree.add_child(TaskTreeNode(normal))\n    tree.add_child(TaskTreeNode(low))\n\n    # Execute\n    # Order: Urgent (0) \u2192 Normal (2) \u2192 Low (3)\n    await task_manager.distribute_task_tree(tree)\n\n    # Check execution order\n    for task in [urgent, normal, low]:\n        result = await task_manager.task_repository.get_task_by_id(task.id)\n        print(f\"Priority {task.priority}: {result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#understanding-priorities","title":"Understanding Priorities","text":"<p>Priority Levels: - 0: Urgent (highest priority) - 1: High - 2: Normal (default) - 3: Low (lowest priority)</p> <p>Rule: Lower numbers = higher priority = execute first</p> <p>Note: Priorities only matter when tasks are ready to run. Dependencies still take precedence!</p>"},{"location":"examples/basic_task/#example-10-complete-workflow","title":"Example 10: Complete Workflow","text":"<p>What it does: Combines everything - custom executors, dependencies, error handling.</p> <p>Why this example: Shows a complete, production-ready pattern.</p>"},{"location":"examples/basic_task/#complete-runnable-code_9","title":"Complete Runnable Code","text":"<p>Create <code>example_10_complete.py</code>:</p> <pre><code>import asyncio\nfrom apflow import BaseTask, executor_register, TaskManager, TaskTreeNode, create_session\nfrom typing import ClassVar, Dict, Any, Literal\nfrom pydantic import BaseModel, Field\n\n# Executor 1: Data fetcher\nclass DataFetcherInputSchema(BaseModel):\n    \"\"\"Input schema for data fetcher\"\"\"\n    source: str = Field(default=\"default\", description=\"Data source\")\n\n@executor_register()\nclass DataFetcher(BaseTask):\n    id = \"data_fetcher\"\n    name = \"Data Fetcher\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = DataFetcherInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        source = inputs.get(\"source\", \"default\")\n        return {\n            \"source\": source,\n            \"data\": [10, 20, 30, 40, 50],\n            \"count\": 5\n        }\n\n# Executor 2: Data processor\nclass DataProcessorInputSchema(BaseModel):\n    \"\"\"Input schema for data processor\"\"\"\n    data: list[float] = Field(description=\"Array of numbers\")\n    operation: Literal[\"sum\", \"average\", \"max\"] = Field(\n        default=\"sum\", description=\"Operation to perform\"\n    )\n\n@executor_register()\nclass DataProcessor(BaseTask):\n    id = \"data_processor\"\n    name = \"Data Processor\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = DataProcessorInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        data = inputs.get(\"data\", [])\n        operation = inputs.get(\"operation\", \"sum\")\n\n        if operation == \"sum\":\n            result = sum(data)\n        elif operation == \"average\":\n            result = sum(data) / len(data) if data else 0\n        else:\n            result = max(data) if data else 0\n\n        return {\n            \"operation\": operation,\n            \"result\": result,\n            \"input_count\": len(data)\n        }\n\n# Executor 3: Result formatter\nclass ResultFormatterInputSchema(BaseModel):\n    \"\"\"Input schema for result formatter\"\"\"\n    result: float = Field(description=\"Result to format\")\n    format: Literal[\"json\", \"text\"] = Field(default=\"json\", description=\"Output format\")\n\n@executor_register()\nclass ResultFormatter(BaseTask):\n    id = \"result_formatter\"\n    name = \"Result Formatter\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = ResultFormatterInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        result = inputs.get(\"result\")\n        format_type = inputs.get(\"format\", \"json\")\n\n        if format_type == \"json\":\n            output = {\"result\": result, \"formatted\": True}\n        else:\n            output = f\"Result: {result}\"\n\n        return {\n            \"format\": format_type,\n            \"output\": output\n        }\n\nasync def main():\n    from example_10_complete import DataFetcher, DataProcessor, ResultFormatter\n\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Step 1: Fetch data\n    fetch = await task_manager.task_repository.create_task(\n        name=\"data_fetcher\",\n        user_id=\"example_user\",\n        priority=1,\n        inputs={\"source\": \"api\"}\n    )\n\n    # Step 2: Process data (depends on fetch)\n    process = await task_manager.task_repository.create_task(\n        name=\"data_processor\",\n        user_id=\"example_user\",\n        parent_id=fetch.id,\n        dependencies=[{\"id\": fetch.id, \"required\": True}],\n        priority=2,\n        inputs={\n            \"data\": [],  # From fetch result\n            \"operation\": \"average\"\n        }\n    )\n\n    # Step 3: Format result (depends on process)\n    format_task = await task_manager.task_repository.create_task(\n        name=\"result_formatter\",\n        user_id=\"example_user\",\n        parent_id=fetch.id,\n        dependencies=[{\"id\": process.id, \"required\": True}],\n        priority=3,\n        inputs={\n            \"result\": 0,  # From process result\n            \"format\": \"json\"\n        }\n    )\n\n    # Build workflow\n    root = TaskTreeNode(fetch)\n    root.add_child(TaskTreeNode(process))\n    root.add_child(TaskTreeNode(format_task))\n\n    # Execute complete workflow\n    await task_manager.distribute_task_tree(root)\n\n    # Get final result\n    final = await task_manager.task_repository.get_task_by_id(format_task.id)\n    print(f\"\u2705 Workflow completed: {final.status}\")\n    print(f\"\ud83d\udcca Final output: {final.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic_task/#understanding-the-complete-workflow","title":"Understanding the Complete Workflow","text":"<p>Execution Flow: <pre><code>Fetch Data\n    \u2193\nProcess Data (uses fetched data)\n    \u2193\nFormat Result (uses processed result)\n</code></pre></p> <p>Key Features: - \u2705 Custom executors - \u2705 Dependencies - \u2705 Error handling - \u2705 Result flow between tasks</p>"},{"location":"examples/basic_task/#common-patterns-summary","title":"Common Patterns Summary","text":""},{"location":"examples/basic_task/#pattern-1-simple-task","title":"Pattern 1: Simple Task","text":"<pre><code>task = await task_repository.create_task(name=\"executor_id\", ...)\ntree = TaskTreeNode(task)\nawait task_manager.distribute_task_tree(tree)\n</code></pre>"},{"location":"examples/basic_task/#pattern-2-sequential-dependencies","title":"Pattern 2: Sequential (Dependencies)","text":"<pre><code>task1 = await task_repository.create_task(...)\ntask2 = await task_repository.create_task(\n    dependencies=[{\"id\": task1.id}],\n    ...\n)\n</code></pre>"},{"location":"examples/basic_task/#pattern-3-parallel-no-dependencies","title":"Pattern 3: Parallel (No Dependencies)","text":"<pre><code>task1 = await task_repository.create_task(...)\ntask2 = await task_repository.create_task(...)  # No dependency\n# Both run in parallel\n</code></pre>"},{"location":"examples/basic_task/#next-steps","title":"Next Steps","text":"<ul> <li>Task Orchestration Guide - Deep dive into orchestration</li> <li>Custom Tasks Guide - Advanced executor creation</li> <li>Real-World Examples - Production-ready examples</li> <li>Best Practices - Learn from experts</li> </ul> <p>Need help? Check the FAQ or Quick Start Guide</p>"},{"location":"examples/email-executor/","title":"Email Executor Guide","text":"<p>The <code>send_email_executor</code> provides email sending capabilities via two providers: - Resend: Cloud email API via HTTP (recommended for simplicity) - SMTP: Traditional SMTP protocol (for self-hosted or existing mail servers)</p>"},{"location":"examples/email-executor/#installation","title":"Installation","text":"<p>For SMTP support, install the email extras:</p> <pre><code>pip install apflow[email]\n</code></pre> <p>Resend provider works out of the box (uses HTTP).</p>"},{"location":"examples/email-executor/#quick-start","title":"Quick Start","text":""},{"location":"examples/email-executor/#using-environment-variables-recommended","title":"Using Environment Variables (Recommended)","text":"<p>Configure once in <code>.env</code>, then send emails with minimal input:</p> <pre><code># .env file\nRESEND_API_KEY=re_xxxxxxxxxxxxx\nFROM_EMAIL=noreply@example.com\n</code></pre> <pre><code># Send email with just recipient, subject, and body\napflow run - &lt;&lt;EOF\n{\n  \"id\": \"send-welcome\",\n  \"name\": \"Send Welcome Email\",\n  \"schemas\": {\"method\": \"send_email_executor\"},\n  \"inputs\": {\n    \"to\": \"user@example.com\",\n    \"subject\": \"Welcome!\",\n    \"body\": \"Welcome to our service!\"\n  }\n}\nEOF\n</code></pre>"},{"location":"examples/email-executor/#explicit-configuration","title":"Explicit Configuration","text":"<p>Override env vars or use without them:</p> <pre><code>apflow run - &lt;&lt;EOF\n{\n  \"id\": \"send-notification\",\n  \"name\": \"Send Notification\",\n  \"schemas\": {\"method\": \"send_email_executor\"},\n  \"inputs\": {\n    \"provider\": \"resend\",\n    \"api_key\": \"re_xxxxxxxxxxxxx\",\n    \"to\": \"user@example.com\",\n    \"from_email\": \"noreply@example.com\",\n    \"subject\": \"Order Shipped\",\n    \"body\": \"Your order has been shipped!\"\n  }\n}\nEOF\n</code></pre>"},{"location":"examples/email-executor/#environment-variables","title":"Environment Variables","text":"Variable Type Default Description <code>RESEND_API_KEY</code> string - Resend API key <code>SMTP_HOST</code> string - SMTP server hostname <code>SMTP_PORT</code> integer 587 SMTP server port <code>SMTP_USERNAME</code> string - SMTP auth username <code>SMTP_PASSWORD</code> string - SMTP auth password <code>SMTP_USE_TLS</code> string true Use STARTTLS (\"true\"/\"false\") <code>FROM_EMAIL</code> string - Default sender email <p>Priority: Input values always override environment variables.</p> <p>Auto-Detection: If <code>provider</code> is not specified: - <code>RESEND_API_KEY</code> set \u2192 uses <code>resend</code> - <code>SMTP_HOST</code> set \u2192 uses <code>smtp</code></p>"},{"location":"examples/email-executor/#input-schema","title":"Input Schema","text":"Field Type Required Description <code>provider</code> string No* <code>\"resend\"</code> or <code>\"smtp\"</code> (auto-detected from env) <code>to</code> string/array Yes Recipient email(s) <code>subject</code> string Yes Email subject <code>from_email</code> string No* Sender email (falls back to <code>FROM_EMAIL</code> env) <code>body</code> string No** Plain text body <code>html</code> string No** HTML body <code>cc</code> string/array No CC recipients <code>bcc</code> string/array No BCC recipients <code>reply_to</code> string No Reply-to address <code>timeout</code> float No Request timeout in seconds (default: 30) <code>api_key</code> string No* Resend API key (falls back to <code>RESEND_API_KEY</code> env) <code>smtp_host</code> string No* SMTP hostname (falls back to <code>SMTP_HOST</code> env) <code>smtp_port</code> integer No SMTP port (falls back to <code>SMTP_PORT</code> env, default: 587) <code>smtp_username</code> string No SMTP username (falls back to <code>SMTP_USERNAME</code> env) <code>smtp_password</code> string No SMTP password (falls back to <code>SMTP_PASSWORD</code> env) <code>smtp_use_tls</code> boolean No Use STARTTLS (falls back to <code>SMTP_USE_TLS</code> env) <p>* Required unless provided via environment variable ** At least one of <code>body</code> or <code>html</code> is required</p>"},{"location":"examples/email-executor/#output-schema","title":"Output Schema","text":"Field Type Description <code>success</code> boolean Whether email was sent successfully <code>provider</code> string Provider used (<code>resend</code> or <code>smtp</code>) <code>message_id</code> string Message ID (Resend only) <code>status_code</code> integer HTTP status code (Resend only) <code>message</code> string Success message (SMTP only) <code>error</code> string Error details (on failure)"},{"location":"examples/email-executor/#examples","title":"Examples","text":""},{"location":"examples/email-executor/#example-1-simple-text-email-resend","title":"Example 1: Simple Text Email (Resend)","text":"<pre><code># With env vars: RESEND_API_KEY, FROM_EMAIL\napflow run - &lt;&lt;EOF\n{\n  \"id\": \"simple-email\",\n  \"name\": \"Simple Email\",\n  \"schemas\": {\"method\": \"send_email_executor\"},\n  \"inputs\": {\n    \"to\": \"user@example.com\",\n    \"subject\": \"Hello\",\n    \"body\": \"This is a simple text email.\"\n  }\n}\nEOF\n</code></pre>"},{"location":"examples/email-executor/#example-2-html-email-with-ccbcc","title":"Example 2: HTML Email with CC/BCC","text":"<pre><code>{\n  \"id\": \"html-email\",\n  \"name\": \"HTML Email\",\n  \"schemas\": {\"method\": \"send_email_executor\"},\n  \"inputs\": {\n    \"to\": [\"primary@example.com\", \"secondary@example.com\"],\n    \"cc\": \"manager@example.com\",\n    \"bcc\": [\"audit@example.com\"],\n    \"subject\": \"Monthly Report\",\n    \"html\": \"&lt;h1&gt;Monthly Report&lt;/h1&gt;&lt;p&gt;Please find the report attached.&lt;/p&gt;\",\n    \"body\": \"Monthly Report\\n\\nPlease find the report attached.\",\n    \"reply_to\": \"reports@example.com\"\n  }\n}\n</code></pre>"},{"location":"examples/email-executor/#example-3-smtp-configuration","title":"Example 3: SMTP Configuration","text":"<pre><code># .env file for SMTP\nSMTP_HOST=smtp.gmail.com\nSMTP_PORT=587\nSMTP_USERNAME=your-email@gmail.com\nSMTP_PASSWORD=your-app-password\nFROM_EMAIL=your-email@gmail.com\n</code></pre> <pre><code>{\n  \"id\": \"smtp-email\",\n  \"name\": \"SMTP Email\",\n  \"schemas\": {\"method\": \"send_email_executor\"},\n  \"inputs\": {\n    \"to\": \"recipient@example.com\",\n    \"subject\": \"Test via SMTP\",\n    \"body\": \"This email was sent via SMTP.\"\n  }\n}\n</code></pre>"},{"location":"examples/email-executor/#example-4-task-tree-with-email-notification","title":"Example 4: Task Tree with Email Notification","text":"<pre><code>{\n  \"id\": \"workflow-with-email\",\n  \"name\": \"Data Processing with Email Notification\",\n  \"schemas\": {\"method\": \"aggregate_results_executor\"},\n  \"children\": [\n    {\n      \"id\": \"fetch-data\",\n      \"name\": \"Fetch Data\",\n      \"schemas\": {\"method\": \"rest_executor\"},\n      \"inputs\": {\n        \"method\": \"GET\",\n        \"url\": \"https://api.example.com/data\"\n      }\n    },\n    {\n      \"id\": \"notify-complete\",\n      \"name\": \"Send Completion Email\",\n      \"schemas\": {\"method\": \"send_email_executor\"},\n      \"inputs\": {\n        \"to\": \"admin@example.com\",\n        \"subject\": \"Data Processing Complete\",\n        \"body\": \"The data processing workflow has completed successfully.\"\n      },\n      \"dependencies\": [\"fetch-data\"]\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/email-executor/#example-5-python-api-usage","title":"Example 5: Python API Usage","text":"<pre><code>import asyncio\nfrom apflow.extensions.email import SendEmailExecutor\n\nasync def send_email():\n    executor = SendEmailExecutor()\n\n    # With env vars configured, minimal input needed\n    result = await executor.execute({\n        \"to\": \"user@example.com\",\n        \"subject\": \"Hello from Python\",\n        \"body\": \"This email was sent using the Python API.\"\n    })\n\n    if result[\"success\"]:\n        print(f\"Email sent! Message ID: {result.get('message_id')}\")\n    else:\n        print(f\"Failed to send: {result.get('error')}\")\n\nasyncio.run(send_email())\n</code></pre>"},{"location":"examples/email-executor/#provider-comparison","title":"Provider Comparison","text":"Feature Resend SMTP Setup Complexity Low (just API key) Medium (host, port, auth) Dependency None (uses HTTP) <code>aiosmtplib</code> Message ID Yes No Deliverability High (managed service) Depends on server Cost Free tier available Free (self-hosted) Best For Transactional emails Existing mail servers"},{"location":"examples/email-executor/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/email-executor/#provider-is-required-error","title":"\"provider is required\" Error","text":"<p>Cause: No provider specified and no env vars set.</p> <p>Solution: Either: 1. Set <code>RESEND_API_KEY</code> or <code>SMTP_HOST</code> environment variable 2. Specify <code>provider</code> in task inputs</p>"},{"location":"examples/email-executor/#api_key-is-required-for-resend-provider-error","title":"\"api_key is required for resend provider\" Error","text":"<p>Cause: Using Resend without API key.</p> <p>Solution: Either: 1. Set <code>RESEND_API_KEY</code> environment variable 2. Provide <code>api_key</code> in task inputs</p>"},{"location":"examples/email-executor/#aiosmtplib-is-not-installed-error","title":"\"aiosmtplib is not installed\" Error","text":"<p>Cause: SMTP provider requires optional dependency.</p> <p>Solution: Install email extras: <pre><code>pip install apflow[email]\n</code></pre></p>"},{"location":"examples/email-executor/#smtp-connection-timeout","title":"SMTP Connection Timeout","text":"<p>Cause: Wrong host/port or firewall blocking.</p> <p>Solutions: 1. Verify <code>SMTP_HOST</code> and <code>SMTP_PORT</code> are correct 2. Check firewall allows outbound connections 3. Try port 465 (SSL) or 25 (no encryption) if 587 fails 4. Increase <code>timeout</code> value in inputs</p>"},{"location":"examples/email-executor/#gmail-less-secure-app-error","title":"Gmail \"Less secure app\" Error","text":"<p>Cause: Gmail blocks basic auth.</p> <p>Solution: Use App Passwords: 1. Enable 2-Factor Authentication on Google Account 2. Generate App Password: Google Account \u2192 Security \u2192 App Passwords 3. Use the generated password as <code>SMTP_PASSWORD</code></p>"},{"location":"examples/email-executor/#see-also","title":"See Also","text":"<ul> <li>Environment Variables Reference</li> <li>Task Orchestration Guide</li> <li>REST Executor Guide</li> </ul>"},{"location":"examples/generate-executor/","title":"Generate Executor Guide","text":"<p>This guide demonstrates the <code>generate_executor</code>'s intelligent task tree generation capabilities. The generate executor uses LLM to understand business requirements and automatically creates appropriate task trees.</p>"},{"location":"examples/generate-executor/#overview","title":"Overview","text":"<p>The generate executor provides two generation modes:</p> <ul> <li>Single-Shot Mode (default): Fast generation using a single LLM call. Best for simple requirements.</li> <li>Multi-Phase Mode: Higher quality generation using 4 specialized phases. Best for complex multi-executor workflows.</li> </ul> <p>See also: Generate Executor Improvements for detailed technical information about the dual-mode system, validation, and auto-fix mechanisms.</p>"},{"location":"examples/generate-executor/#quick-start","title":"Quick Start","text":""},{"location":"examples/generate-executor/#cli-usage","title":"CLI Usage","text":"<p>Generate a task tree from natural language:</p> <pre><code>apflow generate task-tree \"YOUR_REQUIREMENT_HERE\"\n</code></pre> <p>Save the output to a file:</p> <pre><code>apflow generate task-tree \"YOUR_REQUIREMENT\" --output tasks.json\n</code></pre> <p>Use multi-phase mode for better quality:</p> <pre><code>apflow generate task-tree \"YOUR_REQUIREMENT\" --mode multi_phase\n</code></pre> <p>With custom LLM parameters:</p> <pre><code>apflow generate task-tree \"YOUR_REQUIREMENT\" --temperature 0.9 --max-tokens 6000\n</code></pre>"},{"location":"examples/generate-executor/#python-api-usage","title":"Python API Usage","text":"<pre><code>from apflow.extensions.generate import GenerateExecutor\n\nexecutor = GenerateExecutor(id=\"gen\", name=\"Task Generator\")\n\n# Single-shot mode (default, faster)\nresult = await executor.execute({\n    \"requirement\": \"Fetch user data from API and save to database\",\n    \"user_id\": \"user_123\"\n})\n\n# Multi-phase mode (higher quality)\nresult = await executor.execute({\n    \"requirement\": \"Analyze aipartnerup.com website and generate report\",\n    \"generation_mode\": \"multi_phase\",  # Use multi-phase for complex tasks\n    \"user_id\": \"user_123\"\n})\n\ntasks = result[\"tasks\"]\n</code></pre>"},{"location":"examples/generate-executor/#generation-modes","title":"Generation Modes","text":""},{"location":"examples/generate-executor/#single-shot-mode","title":"Single-Shot Mode","text":"<p>When to use: - Simple, straightforward requirements - Single-executor workflows - Speed is priority - Prototyping and testing</p> <p>Characteristics: - One LLM call generates complete task tree - 2-3x faster than multi-phase - Lower token usage (~2,000 tokens) - Good quality for simple cases</p>"},{"location":"examples/generate-executor/#multi-phase-mode","title":"Multi-Phase Mode","text":"<p>When to use: - Complex multi-step requirements - Multi-executor workflows - Quality is critical - Production deployments</p> <p>Characteristics: - 4 specialized phases (analyze, design, generate, validate) - Higher quality output - Better structure for complex workflows - Higher token usage (~6,000 tokens) - Requires CrewAI: <code>pip install apflow[crewai]</code></p>"},{"location":"examples/generate-executor/#test-cases","title":"Test Cases","text":""},{"location":"examples/generate-executor/#test-case-1-complex-data-pipeline","title":"Test Case 1: Complex Data Pipeline","text":"<p>Command: <pre><code>apflow generate task-tree \"Fetch data from two different APIs in parallel, then merge the results, validate the merged data, and finally save to database\"\n</code></pre></p> <p>Expected Behavior: - Should generate parallel tasks for fetching from two APIs - Should create a merge/aggregate task that depends on both fetch tasks - Should create validation and save tasks in sequence - Should demonstrate understanding of parallel execution patterns</p>"},{"location":"examples/generate-executor/#test-case-2-etl-workflow","title":"Test Case 2: ETL Workflow","text":"<p>Command: <pre><code>apflow generate task-tree \"Extract data from a REST API, transform it by filtering and aggregating, then load it into a database with proper error handling\"\n</code></pre></p> <p>Expected Behavior: - Should create sequential pipeline: Extract \u2192 Transform \u2192 Load - Should use appropriate executors for each step - Should include proper dependencies for execution order</p>"},{"location":"examples/generate-executor/#test-case-3-multi-source-data-collection","title":"Test Case 3: Multi-Source Data Collection","text":"<p>Command: <pre><code>apflow generate task-tree \"Collect system information about CPU and memory in parallel, then run a command to analyze the collected data, and finally aggregate the results\"\n</code></pre></p> <p>Expected Behavior: - Should use system_info_executor for parallel data collection - Should create command_executor for analysis - Should use aggregate_results_executor for final step - Should demonstrate parent_id for organization and dependencies for execution order</p>"},{"location":"examples/generate-executor/#test-case-4-api-integration-with-processing","title":"Test Case 4: API Integration with Processing","text":"<p>Command: <pre><code>apflow generate task-tree \"Call a REST API to get user data, process the response to extract key information using a Python script, validate the processed data, and save results to a file\"\n</code></pre></p> <p>Expected Behavior: - Should create rest_executor for API call - Should create command_executor for processing - Should create validation and file operations - Should show proper dependency chain</p>"},{"location":"examples/generate-executor/#test-case-5-complex-workflow-with-conditional-steps","title":"Test Case 5: Complex Workflow with Conditional Steps","text":"<p>Command: <pre><code>apflow generate task-tree \"Fetch data from API, then process it in two different ways in parallel (filter and aggregate), merge both results, and finally save to database\"\n</code></pre></p> <p>Expected Behavior: - Should demonstrate fan-out pattern (one task spawns multiple) - Should demonstrate fan-in pattern (multiple tasks converge) - Should show proper use of dependencies for parallel execution</p>"},{"location":"examples/generate-executor/#test-case-6-real-world-business-scenario","title":"Test Case 6: Real-World Business Scenario","text":"<p>Command: <pre><code>apflow generate task-tree \"Monitor system resources (CPU, memory, disk) in parallel, analyze the collected metrics, generate a report, and send notification if any metric exceeds threshold\"\n</code></pre></p> <p>Expected Behavior: - Should use system_info_executor multiple times in parallel - Should create analysis and reporting tasks - Should demonstrate complex dependency relationships</p>"},{"location":"examples/generate-executor/#test-case-7-data-processing-pipeline","title":"Test Case 7: Data Processing Pipeline","text":"<p>Command: <pre><code>apflow generate task-tree \"Download data from multiple sources simultaneously, transform each dataset independently, then combine all transformed results into a single output file\"\n</code></pre></p> <p>Expected Behavior: - Should show parallel download tasks - Should show parallel transformation tasks - Should create final aggregation task - Should demonstrate proper dependency management</p>"},{"location":"examples/generate-executor/#test-case-8-api-chain-with-error-handling","title":"Test Case 8: API Chain with Error Handling","text":"<p>Command: <pre><code>apflow generate task-tree \"Call API A to get authentication token, use token to call API B for data, process the data, and if processing fails, call a fallback API\"\n</code></pre></p> <p>Expected Behavior: - Should create sequential API calls with token passing - Should demonstrate optional dependencies for fallback - Should show proper error handling pattern</p>"},{"location":"examples/generate-executor/#test-case-9-hierarchical-data-processing","title":"Test Case 9: Hierarchical Data Processing","text":"<p>Command: <pre><code>apflow generate task-tree \"Fetch data from API, organize it into categories, process each category independently in parallel, then aggregate all category results\"\n</code></pre></p> <p>Expected Behavior: - Should demonstrate hierarchical organization (parent_id) - Should show parallel processing within categories - Should create final aggregation step - Should show both organizational and execution dependencies</p>"},{"location":"examples/generate-executor/#test-case-10-complete-business-workflow","title":"Test Case 10: Complete Business Workflow","text":"<p>Command: <pre><code>apflow generate task-tree \"Create a workflow that fetches customer data from API, validates customer information, processes orders in parallel for each customer, aggregates order results, calculates totals, and generates a final report\"\n</code></pre></p> <p>Expected Behavior: - Should demonstrate complex multi-step workflow - Should show parallel processing pattern - Should create proper dependency chain - Should include all necessary executors</p>"},{"location":"examples/generate-executor/#usage-tips","title":"Usage Tips","text":"<ol> <li>Be Specific: More detailed requirements lead to better task trees</li> <li>Mention Patterns: Use words like \"parallel\", \"sequential\", \"merge\", \"aggregate\" to guide generation</li> <li>Specify Executors: Mention specific operations (API, database, file, command) for better executor selection</li> <li>Describe Flow: Explain the data flow and execution order in your requirement</li> <li>Choose Mode Wisely:</li> <li>Use single-shot (default) for simple tasks</li> <li>Use multi-phase for complex multi-executor workflows</li> <li>Multi-phase provides better structure for production use</li> </ol>"},{"location":"examples/generate-executor/#features","title":"Features","text":"<p>With the enhanced generation system, the executor: - \u2705 Select relevant documentation sections based on requirement keywords - \u2705 Understand business context and map to appropriate executors - \u2705 Generate complete, realistic input parameters - \u2705 Create proper dependency chains for execution order - \u2705 Use parent_id appropriately for organization - \u2705 Follow framework best practices and patterns - \u2705 Validate schema compliance - catches type mismatches and missing fields - \u2705 Enforce root task patterns - ensures aggregator roots for multi-executor trees - \u2705 Auto-fix common errors - automatically corrects multiple roots and invalid parent_ids - \u2705 Dual-mode generation - choose between speed (single-shot) and quality (multi-phase)</p>"},{"location":"examples/generate-executor/#mode-comparison","title":"Mode Comparison","text":"Aspect Single-Shot Multi-Phase Speed ~2-3 seconds ~8-12 seconds Quality (simple) Good Excellent Quality (complex) Fair Excellent Token Usage ~2,000 ~6,000 LLM Calls 1 4 Best For Simple tasks Complex workflows Requirements Core only Requires <code>apflow[crewai]</code>"},{"location":"examples/generate-executor/#advanced-topics","title":"Advanced Topics","text":""},{"location":"examples/generate-executor/#validation-and-auto-fix","title":"Validation and Auto-Fix","text":"<p>The generate executor includes comprehensive validation:</p> <p>Schema Compliance: Validates that task inputs match executor schema definitions <pre><code># This will be caught and reported\n{\n    \"schemas\": {\"method\": \"scrape_executor\"},\n    \"inputs\": {\"website\": \"https://example.com\"}  # Wrong! Should be \"url\"\n}\n</code></pre></p> <p>Root Task Pattern: Ensures multi-executor trees have aggregator roots <pre><code># Bad: scrape_executor with children (will trigger warning)\n# Good: aggregate_results_executor as root with scrape and llm children\n</code></pre></p> <p>Auto-Fix: Automatically corrects common errors: - Multiple root tasks \u2192 wraps in aggregate_results_executor - Invalid parent_ids \u2192 reassigns to actual root</p>"},{"location":"examples/generate-executor/#multi-phase-flow","title":"Multi-Phase Flow","text":"<p>When using <code>generation_mode=\"multi_phase\"</code>, the system executes 4 phases:</p> <ol> <li>Phase 1: Requirement Analysis</li> <li>Input: Natural language requirement</li> <li>Output: Structured analysis with goals and constraints</li> <li> <p>Agent: Requirements Analyst</p> </li> <li> <p>Phase 2: Structure Design</p> </li> <li>Input: Analysis + Executor schemas</li> <li>Output: Task tree skeleton (IDs, names, methods, relationships)</li> <li> <p>Agent: Task Structure Designer</p> </li> <li> <p>Phase 3: Input Generation</p> </li> <li>Input: Task structure + Executor schemas</li> <li>Output: Complete tasks with validated inputs</li> <li> <p>Agent: Task Input Generator</p> </li> <li> <p>Phase 4: Review &amp; Validation</p> </li> <li>Input: Complete tasks</li> <li>Output: Validated and corrected task tree</li> <li>Agent: Task Validator</li> </ol>"},{"location":"examples/generate-executor/#troubleshooting","title":"Troubleshooting","text":"<p>Multi-phase mode falls back to single-shot - Solution: Install CrewAI: <code>pip install apflow[crewai]</code></p> <p>Schema validation fails for custom executor - Cause: Executor doesn't implement <code>get_input_schema()</code> - Solution: Add <code>get_input_schema()</code> method to your executor</p> <p>Generated tree has wrong structure - Try: Use multi-phase mode for better quality - Try: Be more specific in requirement description - Try: Mention executor names explicitly</p>"},{"location":"examples/generate-executor/#further-reading","title":"Further Reading","text":"<ul> <li>Generate Executor Improvements - Detailed technical documentation</li> <li>Task Orchestration Guide - Understanding task trees and dependencies</li> <li>Custom Tasks Guide - Creating custom executors</li> </ul>"},{"location":"examples/real-world/","title":"Real-World Examples","text":"<p>See also: - Basic Task Examples for simple, copy-paste patterns - Task Tree Examples for dependency and execution order patterns</p> <p>Complete, runnable examples for common real-world use cases. These examples demonstrate how to use apflow in production scenarios.</p>"},{"location":"examples/real-world/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Data Processing Pipeline</li> <li>API Integration Workflow</li> <li>Batch Processing with Dependencies</li> <li>Error Handling and Retry</li> <li>Multi-Step Workflow</li> </ol>"},{"location":"examples/real-world/#data-processing-pipeline","title":"Data Processing Pipeline","text":""},{"location":"examples/real-world/#scenario","title":"Scenario","text":"<p>Process data from multiple sources, transform it, and save results.</p>"},{"location":"examples/real-world/#complete-example","title":"Complete Example","text":"<pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Step 1: Fetch data from multiple sources\n    fetch_api = await task_manager.task_repository.create_task(\n        name=\"rest_executor\",  # Use built-in REST executor\n        user_id=\"user123\",\n        priority=1,\n        inputs={\n            \"url\": \"https://api.example.com/data\",\n            \"method\": \"GET\"\n        }\n    )\n\n    fetch_db = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\n            \"command\": \"psql -c 'SELECT * FROM users LIMIT 100'\"\n        }\n    )\n\n    # Step 2: Process data (depends on both fetches)\n    process_task = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=fetch_api.id,\n        dependencies=[\n            {\"id\": fetch_api.id, \"required\": True},\n            {\"id\": fetch_db.id, \"required\": True}\n        ],\n        priority=2,\n        inputs={\n            \"command\": \"python process_data.py\"\n        }\n    )\n\n    # Step 3: Save results\n    save_task = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=fetch_api.id,\n        dependencies=[{\"id\": process_task.id, \"required\": True}],\n        priority=3,\n        inputs={\n            \"command\": \"python save_results.py\"\n        }\n    )\n\n    # Build tree\n    root = TaskTreeNode(fetch_api)\n    root.add_child(TaskTreeNode(fetch_db))\n    root.add_child(TaskTreeNode(process_task))\n    root.add_child(TaskTreeNode(save_task))\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\n    # Check results\n    final_result = await task_manager.task_repository.get_task_by_id(save_task.id)\n    print(f\"Pipeline completed: {final_result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/real-world/#execution-flow","title":"Execution Flow","text":"<pre><code>Fetch API \u2500\u2500\u2510\n            \u251c\u2500\u2500\u2192 Process \u2500\u2500\u2192 Save\nFetch DB \u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/real-world/#api-integration-workflow","title":"API Integration Workflow","text":""},{"location":"examples/real-world/#scenario_1","title":"Scenario","text":"<p>Call multiple APIs, aggregate results, and send notifications.</p>"},{"location":"examples/real-world/#complete-example_1","title":"Complete Example","text":"<pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Call multiple APIs in parallel\n    api1 = await task_manager.task_repository.create_task(\n        name=\"http_request_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\n            \"url\": \"https://api.service1.com/data\",\n            \"method\": \"GET\"\n        }\n    )\n\n    api2 = await task_manager.task_repository.create_task(\n        name=\"http_request_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\n            \"url\": \"https://api.service2.com/data\",\n            \"method\": \"GET\"\n        }\n    )\n\n    api3 = await task_manager.task_repository.create_task(\n        name=\"http_request_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\n            \"url\": \"https://api.service3.com/data\",\n            \"method\": \"GET\"\n        }\n    )\n\n    # Aggregate results\n    aggregate = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=api1.id,\n        dependencies=[\n            {\"id\": api1.id, \"required\": True},\n            {\"id\": api2.id, \"required\": True},\n            {\"id\": api3.id, \"required\": True}\n        ],\n        priority=2,\n        inputs={\n            \"command\": \"python aggregate.py\"\n        }\n    )\n\n    # Send notification\n    notify = await task_manager.task_repository.create_task(\n        name=\"http_request_executor\",\n        user_id=\"user123\",\n        parent_id=api1.id,\n        dependencies=[{\"id\": aggregate.id, \"required\": True}],\n        priority=3,\n        inputs={\n            \"url\": \"https://api.notification.com/send\",\n            \"method\": \"POST\",\n            \"body\": {\"message\": \"Processing complete\"}\n        }\n    )\n\n    # Build tree\n    root = TaskTreeNode(api1)\n    root.add_child(TaskTreeNode(api2))\n    root.add_child(TaskTreeNode(api3))\n    root.add_child(TaskTreeNode(aggregate))\n    root.add_child(TaskTreeNode(notify))\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\n    print(\"API integration workflow completed!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/real-world/#execution-flow_1","title":"Execution Flow","text":"<pre><code>API 1 \u2500\u2500\u2510\nAPI 2 \u2500\u2500\u251c\u2500\u2500\u2192 Aggregate \u2500\u2500\u2192 Notify\nAPI 3 \u2500\u2500\u2518\n</code></pre>"},{"location":"examples/real-world/#batch-processing-with-dependencies","title":"Batch Processing with Dependencies","text":""},{"location":"examples/real-world/#scenario_2","title":"Scenario","text":"<p>Process a batch of items where each item depends on the previous one.</p>"},{"location":"examples/real-world/#complete-example_2","title":"Complete Example","text":"<pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create batch of items\n    items = [\"item1\", \"item2\", \"item3\", \"item4\", \"item5\"]\n    tasks = []\n\n    # Create first task (no dependencies)\n    first_task = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\"command\": f\"process {items[0]}\"}\n    )\n    tasks.append(first_task)\n\n    # Create remaining tasks (each depends on previous)\n    for i in range(1, len(items)):\n        task = await task_manager.task_repository.create_task(\n            name=\"command_executor\",\n            user_id=\"user123\",\n            parent_id=first_task.id,\n            dependencies=[{\"id\": tasks[i-1].id, \"required\": True}],\n            priority=1 + i,\n            inputs={\"command\": f\"process {items[i]}\"}\n        )\n        tasks.append(task)\n\n    # Build sequential chain\n    root = TaskTreeNode(tasks[0])\n    for i in range(1, len(tasks)):\n        root.add_child(TaskTreeNode(tasks[i]))\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\n    # Check final task\n    final_result = await task_manager.task_repository.get_task_by_id(tasks[-1].id)\n    print(f\"Batch processing completed: {final_result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/real-world/#execution-flow_2","title":"Execution Flow","text":"<pre><code>Item1 \u2192 Item2 \u2192 Item3 \u2192 Item4 \u2192 Item5\n</code></pre>"},{"location":"examples/real-world/#error-handling-and-retry","title":"Error Handling and Retry","text":""},{"location":"examples/real-world/#scenario_3","title":"Scenario","text":"<p>Handle failures gracefully with fallback tasks.</p>"},{"location":"examples/real-world/#complete-example_3","title":"Complete Example","text":"<pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Primary task (may fail)\n    primary = await task_manager.task_repository.create_task(\n        name=\"http_request_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\n            \"url\": \"https://unreliable-api.com/data\",\n            \"method\": \"GET\"\n        }\n    )\n\n    # Fallback task (runs even if primary fails)\n    fallback = await task_manager.task_repository.create_task(\n        name=\"http_request_executor\",\n        user_id=\"user123\",\n        parent_id=primary.id,\n        dependencies=[{\"id\": primary.id, \"required\": False}],  # Optional dependency\n        priority=2,\n        inputs={\n            \"url\": \"https://backup-api.com/data\",\n            \"method\": \"GET\"\n        }\n    )\n\n    # Final task (works with either primary or fallback)\n    final = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=primary.id,\n        dependencies=[\n            {\"id\": primary.id, \"required\": False},  # Optional\n            {\"id\": fallback.id, \"required\": False}  # Optional\n        ],\n        priority=3,\n        inputs={\"command\": \"python process_result.py\"}\n    )\n\n    # Build tree\n    root = TaskTreeNode(primary)\n    root.add_child(TaskTreeNode(fallback))\n    root.add_child(TaskTreeNode(final))\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\n    # Check results\n    primary_result = await task_manager.task_repository.get_task_by_id(primary.id)\n    fallback_result = await task_manager.task_repository.get_task_by_id(fallback.id)\n    final_result = await task_manager.task_repository.get_task_by_id(final.id)\n\n    print(f\"Primary: {primary_result.status}\")\n    print(f\"Fallback: {fallback_result.status}\")\n    print(f\"Final: {final_result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/real-world/#execution-flow_3","title":"Execution Flow","text":"<pre><code>Primary \u2500\u2500\u2510\n          \u251c\u2500\u2500\u2192 Final (works with either)\nFallback \u2500\u2518\n</code></pre>"},{"location":"examples/real-world/#multi-step-workflow","title":"Multi-Step Workflow","text":""},{"location":"examples/real-world/#scenario_4","title":"Scenario","text":"<p>Complex workflow with multiple stages and parallel processing.</p>"},{"location":"examples/real-world/#complete-example_4","title":"Complete Example","text":"<pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Stage 1: Data Collection (parallel)\n    collect1 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    collect2 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    collect3 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\"resource\": \"disk\"}\n    )\n\n    # Stage 2: Processing (depends on collection)\n    process1 = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=collect1.id,\n        dependencies=[{\"id\": collect1.id, \"required\": True}],\n        priority=2,\n        inputs={\"command\": \"python process_cpu.py\"}\n    )\n\n    process2 = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=collect2.id,\n        dependencies=[{\"id\": collect2.id, \"required\": True}],\n        priority=2,\n        inputs={\"command\": \"python process_memory.py\"}\n    )\n\n    process3 = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=collect3.id,\n        dependencies=[{\"id\": collect3.id, \"required\": True}],\n        priority=2,\n        inputs={\"command\": \"python process_disk.py\"}\n    )\n\n    # Stage 3: Aggregation (depends on all processing)\n    aggregate = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=collect1.id,\n        dependencies=[\n            {\"id\": process1.id, \"required\": True},\n            {\"id\": process2.id, \"required\": True},\n            {\"id\": process3.id, \"required\": True}\n        ],\n        priority=3,\n        inputs={\"command\": \"python aggregate.py\"}\n    )\n\n    # Stage 4: Finalization (depends on aggregation)\n    finalize = await task_manager.task_repository.create_task(\n        name=\"command_executor\",\n        user_id=\"user123\",\n        parent_id=collect1.id,\n        dependencies=[{\"id\": aggregate.id, \"required\": True}],\n        priority=4,\n        inputs={\"command\": \"python finalize.py\"}\n    )\n\n    # Build tree\n    root = TaskTreeNode(collect1)\n    root.add_child(TaskTreeNode(collect2))\n    root.add_child(TaskTreeNode(collect3))\n    root.add_child(TaskTreeNode(process1))\n    root.add_child(TaskTreeNode(process2))\n    root.add_child(TaskTreeNode(process3))\n    root.add_child(TaskTreeNode(aggregate))\n    root.add_child(TaskTreeNode(finalize))\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\n    # Check final result\n    final_result = await task_manager.task_repository.get_task_by_id(finalize.id)\n    print(f\"Multi-step workflow completed: {final_result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/real-world/#execution-flow_4","title":"Execution Flow","text":"<pre><code>Collect1 \u2500\u2500\u2192 Process1 \u2500\u2500\u2510\nCollect2 \u2500\u2500\u2192 Process2 \u2500\u2500\u251c\u2500\u2500\u2192 Aggregate \u2500\u2500\u2192 Finalize\nCollect3 \u2500\u2500\u2192 Process3 \u2500\u2500\u2518\n</code></pre>"},{"location":"examples/real-world/#best-practices","title":"Best Practices","text":""},{"location":"examples/real-world/#1-use-meaningful-task-names","title":"1. Use Meaningful Task Names","text":"<pre><code># Good\nname=\"fetch_user_data\"\nname=\"process_payment\"\nname=\"send_notification\"\n\n# Bad\nname=\"task1\"\nname=\"task2\"\n</code></pre>"},{"location":"examples/real-world/#2-set-appropriate-priorities","title":"2. Set Appropriate Priorities","text":"<pre><code># Critical tasks first\npriority=0  # Highest priority\n\n# Normal tasks\npriority=2  # Default\n\n# Background tasks\npriority=3  # Lower priority\n</code></pre>"},{"location":"examples/real-world/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<pre><code># Use optional dependencies for fallbacks\ndependencies=[{\"id\": primary.id, \"required\": False}]\n</code></pre>"},{"location":"examples/real-world/#4-keep-trees-manageable","title":"4. Keep Trees Manageable","text":"<ul> <li>3-5 levels deep</li> <li>10-20 tasks per tree</li> <li>Use sub-trees for complex workflows</li> </ul>"},{"location":"examples/real-world/#5-monitor-task-status","title":"5. Monitor Task Status","text":"<pre><code># Check task status after execution\ntask = await task_manager.task_repository.get_task_by_id(task_id)\nif task.status == \"failed\":\n    print(f\"Error: {task.error}\")\n</code></pre>"},{"location":"examples/real-world/#next-steps","title":"Next Steps","text":"<ul> <li>Task Orchestration Guide - Learn more about orchestration patterns</li> <li>Best Practices - Design patterns and optimization</li> <li>Custom Tasks - Create your own executors</li> </ul>"},{"location":"examples/task-tree/","title":"Example 7: Scrape Website Content and Analyze","text":"<p>This example demonstrates how to use <code>scrape_executor</code> to extract the main content and metadata from a website, then analyze it with an LLM.</p> <pre><code>[\n    {\n        \"id\": \"task_1\",\n        \"name\": \"Scrape Website Content\",\n        \"schemas\": {\"method\": \"scrape_executor\"},\n        \"inputs\": {\n            \"url\": \"https://aipartnerup.com\",\n            \"max_chars\": 5000,\n            \"extract_metadata\": true\n        }\n    },\n    {\n        \"id\": \"task_2\",\n        \"name\": \"Analyze Scraped Content\",\n        \"schemas\": {\"method\": \"llm_executor\"},\n        \"parent_id\": \"task_1\",\n        \"dependencies\": [{\"id\": \"task_1\", \"required\": true}],\n        \"inputs\": {\n            \"model\": \"gpt-4\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Analyze the content and provide an evaluation of what the website is about.\"}\n            ]\n        }\n    }\n]\n</code></pre>"},{"location":"examples/task-tree/#task-tree-examples","title":"Task Tree Examples","text":"<p>See also: - Basic Task Examples for simple usage - Real-World Examples for advanced, production scenarios</p> <p>This document provides examples of creating and managing task trees with dependencies and priorities.</p>"},{"location":"examples/task-tree/#important-concepts","title":"Important Concepts","text":"<p>Parent-Child Relationship vs Dependencies:</p> <ul> <li> <p>Parent-Child (<code>parent_id</code>): Used only for organizing the task tree structure. It helps visualize the hierarchy but does NOT affect execution order.</p> </li> <li> <p>Dependencies (<code>dependencies</code>): These determine execution order. A task executes only when all its required dependencies are satisfied. Dependencies control when tasks run, not parent-child relationships.</p> </li> </ul> <p>Key Point: If you want to control execution order, always use <code>dependencies</code>. The <code>parent_id</code> field is purely for organizational purposes.</p>"},{"location":"examples/task-tree/#example-1-simple-sequential-pipeline","title":"Example 1: Simple Sequential Pipeline","text":"<p>Execute tasks one after another:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Step 1: Fetch data\n    fetch_task = await task_manager.task_repository.create_task(\n        name=\"fetch_data\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\"url\": \"https://api.example.com/data\"}\n    )\n\n    # Step 2: Process data (depends on Step 1)\n    process_task = await task_manager.task_repository.create_task(\n        name=\"process_data\",\n        user_id=\"user123\",\n        parent_id=fetch_task.id,\n        dependencies=[{\"id\": fetch_task.id, \"required\": True}],\n        priority=2,\n        inputs={\"operation\": \"analyze\"}\n    )\n\n    # Step 3: Save results (depends on Step 2)\n    save_task = await task_manager.task_repository.create_task(\n        name=\"save_results\",\n        user_id=\"user123\",\n        parent_id=process_task.id,\n        dependencies=[{\"id\": process_task.id, \"required\": True}],\n        priority=3,\n        inputs={\"destination\": \"database\"}\n    )\n\n    # Build tree\n    task_tree = TaskTreeNode(fetch_task)\n    task_tree.add_child(TaskTreeNode(process_task))\n    task_tree.children[0].add_child(TaskTreeNode(save_task))\n\n    # Execute (tasks execute in order: fetch -&gt; process -&gt; save)\n    result = await task_manager.distribute_task_tree(task_tree)\n    print(f\"Execution completed: {result.calculate_status()}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/task-tree/#example-2-parallel-execution","title":"Example 2: Parallel Execution","text":"<p>Execute multiple tasks in parallel, then merge results:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Root task\n    root_task = await task_manager.task_repository.create_task(\n        name=\"root_task\",\n        user_id=\"user123\",\n        priority=1\n    )\n\n    # Task 1: Fetch from API A (parallel)\n    task1 = await task_manager.task_repository.create_task(\n        name=\"fetch_api_a\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        priority=2,\n        inputs={\"url\": \"https://api-a.example.com/data\"}\n    )\n\n    # Task 2: Fetch from API B (parallel with Task 1)\n    task2 = await task_manager.task_repository.create_task(\n        name=\"fetch_api_b\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        priority=2,\n        inputs={\"url\": \"https://api-b.example.com/data\"}\n    )\n\n    # Task 3: Merge results (depends on both Task 1 and Task 2)\n    merge_task = await task_manager.task_repository.create_task(\n        name=\"merge_results\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        dependencies=[\n            {\"id\": task1.id, \"required\": True},\n            {\"id\": task2.id, \"required\": True}\n        ],\n        priority=3,\n        inputs={\"operation\": \"merge\"}\n    )\n\n    # Build tree\n    task_tree = TaskTreeNode(root_task)\n    task_tree.add_child(TaskTreeNode(task1))\n    task_tree.add_child(TaskTreeNode(task2))\n    task_tree.add_child(TaskTreeNode(merge_task))\n\n    # Execute (Task 1 and Task 2 run in parallel, then Task 3)\n    result = await task_manager.distribute_task_tree(task_tree)\n    print(f\"Final status: {result.calculate_status()}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/task-tree/#example-3-priority-based-execution","title":"Example 3: Priority-Based Execution","text":"<p>Use priorities to control execution order:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Root task\n    root_task = await task_manager.task_repository.create_task(\n        name=\"root_task\",\n        user_id=\"user123\",\n        priority=1\n    )\n\n    # Urgent task (priority 0 - executes first)\n    urgent_task = await task_manager.task_repository.create_task(\n        name=\"urgent_task\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        priority=0,  # Highest priority\n        inputs={\"action\": \"urgent_operation\"}\n    )\n\n    # Normal task (priority 2 - executes after urgent)\n    normal_task = await task_manager.task_repository.create_task(\n        name=\"normal_task\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        priority=2,  # Normal priority\n        inputs={\"action\": \"normal_operation\"}\n    )\n\n    # Low priority task (priority 3 - executes last)\n    low_task = await task_manager.task_repository.create_task(\n        name=\"low_task\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        priority=3,  # Lowest priority\n        inputs={\"action\": \"low_priority_operation\"}\n    )\n\n    # Build tree\n    task_tree = TaskTreeNode(root_task)\n    task_tree.add_child(TaskTreeNode(urgent_task))\n    task_tree.add_child(TaskTreeNode(normal_task))\n    task_tree.add_child(TaskTreeNode(low_task))\n\n    # Execute (order: urgent -&gt; normal -&gt; low)\n    result = await task_manager.distribute_task_tree(task_tree)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/task-tree/#example-4-complex-workflow","title":"Example 4: Complex Workflow","text":"<p>Combination of sequential and parallel execution:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Root: Data Collection Phase\n    root_task = await task_manager.task_repository.create_task(\n        name=\"data_collection\",\n        user_id=\"user123\",\n        priority=1\n    )\n\n    # Phase 1: Collect data from multiple sources (parallel)\n    source1 = await task_manager.task_repository.create_task(\n        name=\"collect_source1\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        priority=2,\n        inputs={\"source\": \"source1\"}\n    )\n\n    source2 = await task_manager.task_repository.create_task(\n        name=\"collect_source2\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        priority=2,\n        inputs={\"source\": \"source2\"}\n    )\n\n    # Phase 2: Process collected data (depends on both sources)\n    process_task = await task_manager.task_repository.create_task(\n        name=\"process_data\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        dependencies=[\n            {\"id\": source1.id, \"required\": True},\n            {\"id\": source2.id, \"required\": True}\n        ],\n        priority=3,\n        inputs={\"operation\": \"process\"}\n    )\n\n    # Phase 3: Validate and save (depends on processing)\n    validate_task = await task_manager.task_repository.create_task(\n        name=\"validate_data\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        dependencies=[{\"id\": process_task.id, \"required\": True}],\n        priority=4,\n        inputs={\"validation\": \"strict\"}\n    )\n\n    save_task = await task_manager.task_repository.create_task(\n        name=\"save_data\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        dependencies=[{\"id\": process_task.id, \"required\": True}],\n        priority=4,\n        inputs={\"destination\": \"database\"}\n    )\n\n    # Phase 4: Notify (depends on save, optional dependency on validate)\n    notify_task = await task_manager.task_repository.create_task(\n        name=\"notify\",\n        user_id=\"user123\",\n        parent_id=root_task.id,\n        dependencies=[\n            {\"id\": save_task.id, \"required\": True},\n            {\"id\": validate_task.id, \"required\": False}  # Optional\n        ],\n        priority=5,\n        inputs={\"channel\": \"email\"}\n    )\n\n    # Build tree\n    task_tree = TaskTreeNode(root_task)\n    task_tree.add_child(TaskTreeNode(source1))\n    task_tree.add_child(TaskTreeNode(source2))\n    task_tree.add_child(TaskTreeNode(process_task))\n    task_tree.add_child(TaskTreeNode(validate_task))\n    task_tree.add_child(TaskTreeNode(save_task))\n    task_tree.add_child(TaskTreeNode(notify_task))\n\n    # Execute\n    result = await task_manager.distribute_task_tree(task_tree)\n    print(f\"Workflow completed: {result.calculate_status()}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/task-tree/#example-5-error-handling-with-fallback","title":"Example 5: Error Handling with Fallback","text":"<p>Use optional dependencies for fallback scenarios:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Primary task\n    primary_task = await task_manager.task_repository.create_task(\n        name=\"primary_service\",\n        user_id=\"user123\",\n        priority=1,\n        inputs={\"service\": \"primary\"}\n    )\n\n    # Fallback task (optional dependency - executes even if primary fails)\n    fallback_task = await task_manager.task_repository.create_task(\n        name=\"fallback_service\",\n        user_id=\"user123\",\n        parent_id=primary_task.id,\n        dependencies=[{\"id\": primary_task.id, \"required\": False}],  # Optional\n        priority=2,\n        inputs={\"service\": \"fallback\"}\n    )\n\n    # Final task (depends on either primary or fallback)\n    final_task = await task_manager.task_repository.create_task(\n        name=\"finalize\",\n        user_id=\"user123\",\n        parent_id=primary_task.id,\n        dependencies=[\n            {\"id\": primary_task.id, \"required\": False},\n            {\"id\": fallback_task.id, \"required\": False}\n        ],\n        priority=3,\n        inputs={\"action\": \"finalize\"}\n    )\n\n    # Build tree\n    task_tree = TaskTreeNode(primary_task)\n    task_tree.add_child(TaskTreeNode(fallback_task))\n    task_tree.add_child(TaskTreeNode(final_task))\n\n    # Execute\n    result = await task_manager.distribute_task_tree(task_tree)\n\n    # Check results\n    if primary_task.status == \"failed\":\n        print(\"Primary task failed, using fallback\")\n    if final_task.status == \"completed\":\n        print(\"Final task completed successfully\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/task-tree/#example-6-using-taskcreator","title":"Example 6: Using TaskCreator","text":"<p>Create task tree from array format:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskCreator, create_session\n\nasync def main():\n    db = create_session()\n    task_creator = TaskCreator(db)\n    task_manager = TaskManager(db)\n\n    # Define tasks as array\n    tasks = [\n        {\n            \"id\": \"task_1\",\n            \"name\": \"fetch_data\",\n            \"user_id\": \"user123\",\n            \"priority\": 1,\n            \"inputs\": {\"url\": \"https://api.example.com/data\"}\n        },\n        {\n            \"id\": \"task_2\",\n            \"name\": \"process_data\",\n            \"user_id\": \"user123\",\n            \"parent_id\": \"task_1\",\n            \"dependencies\": [{\"id\": \"task_1\", \"required\": True}],\n            \"priority\": 2,\n            \"inputs\": {\"operation\": \"analyze\"}\n        },\n        {\n            \"id\": \"task_3\",\n            \"name\": \"save_results\",\n            \"user_id\": \"user123\",\n            \"parent_id\": \"task_2\",\n            \"dependencies\": [{\"id\": \"task_2\", \"required\": True}],\n            \"priority\": 3,\n            \"inputs\": {\"destination\": \"database\"}\n        }\n    ]\n\n    # Create task tree from array\n    task_tree = await task_creator.create_task_tree_from_array(tasks)\n\n    # Execute\n    result = await task_manager.distribute_task_tree(task_tree)\n    print(f\"Execution completed: {result.calculate_status()}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/task-tree/#best-practices","title":"Best Practices","text":"<ol> <li>Use meaningful task names: Make task names descriptive</li> <li>Set appropriate priorities: Use consistent priority levels</li> <li>Explicit dependencies: Always specify dependencies explicitly</li> <li>Handle errors: Check task status and handle failures</li> <li>Use parent-child relationships: Create clear hierarchy</li> </ol>"},{"location":"examples/task-tree/#next-steps","title":"Next Steps","text":"<ul> <li>Learn more about Task Orchestration</li> <li>See Custom Tasks for creating custom executors</li> <li>Check Python API Reference for detailed API documentation</li> </ul>"},{"location":"getting-started/","title":"Getting Started with apflow","text":"<p>Welcome to apflow! This guide will help you get started quickly, whether you're new to task orchestration or an experienced developer.</p>"},{"location":"getting-started/#problems-we-solve","title":"Problems We Solve","text":"<p>You might be struggling with these common challenges:</p> <ul> <li> <p>Managing Complex Task Dependencies is Painful: Manually tracking which tasks depend on others, ensuring proper execution order, and handling failures across complex workflows becomes a nightmare. You end up writing custom coordination code, dealing with race conditions, and spending weeks debugging dependency issues.</p> </li> <li> <p>Integrating Multiple Execution Methods Creates Complexity: You need to call HTTP APIs, execute SSH commands, run Docker containers, communicate via gRPC, and coordinate AI agents\u2014but each requires different libraries, error handling, and integration patterns. Managing multiple orchestration systems becomes overwhelming.</p> </li> <li> <p>Combining Traditional Tasks with AI Agents is Challenging: You want to add AI capabilities to existing workflows, but most solutions force you to choose: either traditional task execution OR AI agents. You're stuck with all-or-nothing decisions, requiring complete rewrites to introduce AI gradually.</p> </li> <li> <p>State Persistence and Recovery are Hard to Implement: When workflows fail or get interrupted, you lose progress. Implementing retry logic, checkpointing, and state recovery requires significant custom development. You spend more time building infrastructure than solving business problems.</p> </li> <li> <p>Real-time Monitoring Requires Custom Solutions: You need to show progress to users, but building real-time monitoring with polling, WebSocket connections, or custom streaming solutions takes weeks. Your users wait without feedback, and you struggle to debug long-running workflows.</p> </li> </ul>"},{"location":"getting-started/#why-apflow","title":"Why apflow?","text":"<p>Here's what makes apflow the right choice for your orchestration needs:</p> <ul> <li> <p>One Unified Interface for Everything: Stop managing multiple orchestration systems. One framework handles traditional tasks, HTTP/REST APIs, SSH commands, Docker containers, gRPC services, WebSocket communication, MCP tools, and AI agents\u2014all through the same ExecutableTask interface. No more switching between different libraries and patterns.</p> </li> <li> <p>Start Simple, Scale Up Gradually: Begin with a lightweight, dependency-free core that handles traditional task orchestration. Add AI capabilities, A2A server, CLI tools, or PostgreSQL storage only when you need them. Unlike frameworks that force you to install everything upfront, apflow lets you start minimal and grow incrementally. This modular approach means you only pay for what you use and keep deployments lean.</p> </li> <li> <p>Language-Agnostic Protocol: Built on the AI Partner Up Flow Protocol (APFlow Protocol), ensuring interoperability across Python, Go, Rust, JavaScript, and more. Different language implementations work together seamlessly. The protocol provides complete specifications for building compatible libraries, making it future-proof and vendor-independent.</p> </li> <li> <p>Production-Ready from Day One: Built-in storage (DuckDB or PostgreSQL), real-time streaming, automatic retries, state persistence, and comprehensive monitoring\u2014all included. No need to build these from scratch. The framework handles error recovery, checkpointing, and workflow resumption automatically. Focus on your business logic, not infrastructure.</p> </li> <li> <p>Extensive Executor Ecosystem: Choose from HTTP/REST APIs (with authentication), SSH remote execution, Docker containers, gRPC services, WebSocket communication, MCP integration, and LLM-based task tree generation. All executors support the same interface, making it easy to mix and match execution methods in a single workflow.</p> </li> </ul>"},{"location":"getting-started/#what-happens-when-you-use-apflow","title":"What Happens When You Use apflow?","text":"<p>Here's the real impact of using our framework:</p> <ul> <li> <p>You Build Workflows Faster: Before: Weeks of custom coordination code, dependency management, and error handling. You spend more time building infrastructure than solving business problems. After: Define task trees with dependencies in days, not weeks. The framework handles coordination, error recovery, and state management automatically. Focus on what matters\u2014your business logic.</p> </li> <li> <p>You Integrate Everything Easily: Before: Multiple orchestration systems for HTTP APIs, SSH commands, Docker containers, and AI agents. Each requires different libraries, patterns, and error handling. After: One unified interface for all execution methods. Mix HTTP calls, SSH commands, Docker containers, gRPC services, WebSocket, MCP tools, and AI agents in a single workflow seamlessly.</p> </li> <li> <p>You Add AI Gradually: Before: All-or-nothing decisions. To add AI capabilities, you must rewrite entire workflows or choose between traditional tasks OR AI agents. After: Start with traditional task orchestration, then add AI agents incrementally when ready. No rewrites needed. The framework bridges traditional and AI execution seamlessly.</p> </li> <li> <p>You Monitor in Real-Time: Before: Weeks building custom polling, WebSocket connections, or streaming solutions. Users wait without feedback, and debugging long-running workflows is painful. After: Built-in real-time streaming via A2A Protocol. Monitor progress, task status, and intermediate results instantly. Users see updates in real-time, and you debug workflows easily.</p> </li> <li> <p>You Recover from Failures Automatically: Before: Manual recovery logic, lost progress on interruptions, and weeks implementing retry strategies and checkpointing. After: Automatic retries with exponential backoff, state persistence, and workflow resumption from checkpoints. Failed tasks recover automatically, and interrupted workflows resume seamlessly.</p> </li> <li> <p>You Scale with Confidence: Before: Worrying about resource usage, dependency management at scale, and coordinating hundreds of concurrent workflows manually. After: Production-ready from day one. Built-in storage, streaming architecture, and efficient resource management. Handle hundreds of concurrent workflows with confidence.</p> </li> </ul>"},{"location":"getting-started/#what-is-apflow","title":"What is apflow?","text":"<p>apflow is a Python framework for orchestrating and executing tasks. Think of it as a conductor for your application's tasks - it manages when tasks run, how they depend on each other, and ensures everything executes in the right order.</p>"},{"location":"getting-started/#key-benefits","title":"Key Benefits","text":"<ul> <li>Simple Task Management: Create, organize, and execute tasks with ease</li> <li>Dependency Handling: Tasks automatically wait for their dependencies to complete</li> <li>Flexible Execution: Support for custom tasks, LLM agents (CrewAI), and more</li> <li>Production Ready: Built-in storage, streaming, and API support</li> <li>Extensible: Easy to add custom task types and integrations</li> </ul>"},{"location":"getting-started/#quick-navigation","title":"Quick Navigation","text":""},{"location":"getting-started/#new-to-apflow","title":"\ud83d\ude80 New to apflow?","text":"<p>Start here if you're completely new:</p> <ol> <li>Hello World - Get started in 5 minutes</li> <li>Core Concepts - Learn the fundamental ideas (5 min read)</li> <li>Quick Start Guide - Build your first task (10 min)</li> <li>First Steps Tutorial - Complete beginner tutorial</li> </ol>"},{"location":"getting-started/#already-familiar","title":"\ud83d\udcda Already familiar?","text":"<p>Jump to what you need:</p> <ul> <li>Examples - Copy-paste ready examples</li> <li>Task Orchestration Guide - Deep dive into task management</li> <li>Custom Tasks Guide - Create your own task types</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"getting-started/#what-do-you-want-to-do","title":"\ud83c\udfaf What do you want to do?","text":"<p>I want to...</p> <ul> <li>Execute simple tasks \u2192 Quick Start</li> <li>Build complex workflows \u2192 Task Orchestration Guide</li> <li>Create custom task types \u2192 Custom Tasks Guide</li> <li>Use LLM agents \u2192 CrewAI Examples</li> <li>Understand the architecture \u2192 Architecture Overview</li> </ul>"},{"location":"getting-started/#core-concepts-at-a-glance","title":"Core Concepts at a Glance","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Your Application                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n                          \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              apflow Framework                   \u2502\n\u2502                                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502   Task 1     \u2502  \u2502   Task 2     \u2502  \u2502   Task 3     \u2502 \u2502\n\u2502  \u2502  (Fetch)    \u2502  \u2502  (Process)   \u2502  \u2502  (Save)      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502         \u2502                 \u2502                  \u2502         \u2502\n\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                           \u2502                             \u2502\n\u2502                    Dependencies                          \u2502\n\u2502              (Task 2 waits for Task 1)                  \u2502\n\u2502                                                          \u2502\n\u2502              TaskManager orchestrates                   \u2502\n\u2502              execution order automatically              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/#the-basics","title":"The Basics","text":"<ul> <li>Task: A unit of work (e.g., \"fetch data\", \"process file\", \"send email\")</li> <li>Task Tree: A hierarchical structure organizing related tasks</li> <li>Dependencies: Relationships that control execution order</li> <li>Executor: The code that actually runs a task</li> <li>TaskManager: The orchestrator that manages task execution</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Choose your installation based on what you need:</p> <pre><code># Minimal: Core orchestration only\npip install apflow\n\n# With LLM support (CrewAI)\npip install apflow[crewai]\n\n# With API server\npip install apflow[a2a]\n\n# With CLI tools\npip install apflow[cli]\n\n# Everything\npip install apflow[all]\n</code></pre> <p>See Installation Guide for details.</p>"},{"location":"getting-started/#your-first-5-minutes","title":"Your First 5 Minutes","text":"<p>Here's the fastest way to see apflow in action:</p> <pre><code>from apflow import TaskManager, TaskTreeNode, create_session\nimport asyncio\n\nasync def main():\n    # 1. Create a database session\n    db = create_session()\n\n    # 2. Create a task manager\n    task_manager = TaskManager(db)\n\n    # 3. Create a simple task\n    task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",  # Built-in executor\n        user_id=\"user123\",\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # 4. Build and execute\n    task_tree = TaskTreeNode(task)\n    await task_manager.distribute_task_tree(task_tree)\n\n    # 5. Check the result\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Task completed: {result.status}\")\n    print(f\"Result: {result.result}\")\n\nasyncio.run(main())\n</code></pre> <p>That's it! You just executed your first task. </p> <p>\ud83d\udc49 Next: Read Core Concepts to understand what just happened, or jump to Quick Start for a more detailed walkthrough.</p>"},{"location":"getting-started/#learning-paths","title":"Learning Paths","text":""},{"location":"getting-started/#path-1-quick-learner-30-minutes","title":"Path 1: Quick Learner (30 minutes)","text":"<ol> <li>Hello World (5 min)</li> <li>Core Concepts (5 min)</li> <li>Quick Start (10 min)</li> <li>Basic Examples (10 min)</li> </ol>"},{"location":"getting-started/#path-2-comprehensive-2-hours","title":"Path 2: Comprehensive (2 hours)","text":"<ol> <li>Hello World</li> <li>Core Concepts</li> <li>Quick Start</li> <li>First Steps Tutorial</li> <li>Task Trees Tutorial</li> <li>Dependencies Tutorial</li> </ol>"},{"location":"getting-started/#path-3-professional-developer-4-hours","title":"Path 3: Professional Developer (4+ hours)","text":"<ol> <li>Complete Path 2</li> <li>Custom Tasks Guide</li> <li>Best Practices</li> <li>API Reference</li> <li>Advanced Topics</li> </ol>"},{"location":"getting-started/#common-questions","title":"Common Questions","text":"<p>Q: Do I need to know task orchestration? A: No! Start with Core Concepts - we explain everything from scratch.</p> <p>Q: Can I use this without LLM/AI? A: Yes! The core framework has no AI dependencies. LLM support is optional via <code>[crewai]</code>.</p> <p>Q: Is this production-ready? A: Yes! It includes storage, error handling, streaming, and API support out of the box.</p> <p>Q: How is this different from Celery/Airflow? A: apflow focuses on simplicity and flexibility. It's designed for both simple workflows and complex AI agent orchestration.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>New to task orchestration? \u2192 Start with Core Concepts</li> <li>Ready to code? \u2192 Jump to Quick Start</li> <li>Want examples? \u2192 Check Examples</li> <li>Need help? \u2192 See FAQ</li> </ul> <p>Ready to begin? \u2192 Start with Core Concepts \u2192</p>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":"<p>Understanding these core concepts will help you use apflow effectively. Don't worry - we'll explain everything in simple terms!</p>"},{"location":"getting-started/concepts/#why-these-concepts-matter","title":"Why These Concepts Matter","text":"<p>Before diving into the technical details, let's understand why these concepts exist and what problems they solve:</p> <p>The Problem: When building applications, you often need to coordinate multiple operations that depend on each other. For example: - Fetch data from an API, then process it, then save it - Run multiple tasks in parallel, but wait for all to complete before proceeding - Handle failures gracefully and retry automatically - Track progress and state across long-running operations</p> <p>Without a framework, you'd write custom code to: - Manually coordinate task execution order - Handle dependencies and wait conditions - Implement retry logic and error recovery - Track state and progress - Manage different execution methods (HTTP, SSH, Docker, etc.)</p> <p>With apflow, these concepts provide a unified way to solve all these problems. The framework handles the complexity, so you can focus on your business logic.</p> <p>Now let's learn the core concepts that make this possible!</p>"},{"location":"getting-started/concepts/#what-is-a-task","title":"What is a Task?","text":"<p>A task is a unit of work that needs to be executed. Think of it like a function call, but with additional features like status tracking, dependencies, and persistence.</p>"},{"location":"getting-started/concepts/#real-world-analogy","title":"Real-World Analogy","text":"<p>Imagine you're cooking dinner: - Task: \"Cook pasta\" - Inputs: Pasta, water, salt - Result: Cooked pasta - Status: pending \u2192 in_progress \u2192 completed</p>"},{"location":"getting-started/concepts/#in-code","title":"In Code","text":"<pre><code>task = await task_repository.create_task(\n    name=\"cook_pasta\",           # What to do\n    inputs={\"pasta\": \"spaghetti\", \"water\": \"2L\"},  # What you need\n    user_id=\"chef123\"            # Who's doing it\n)\n</code></pre>"},{"location":"getting-started/concepts/#what-is-task-orchestration","title":"What is Task Orchestration?","text":"<p>Task orchestration is the process of managing multiple tasks - deciding when they run, in what order, and how they relate to each other.</p>"},{"location":"getting-started/concepts/#real-world-analogy_1","title":"Real-World Analogy","text":"<p>Think of a restaurant kitchen: - The chef (TaskManager) coordinates everything - Some dishes must be prepared in order (dependencies) - Some can be prepared simultaneously (parallel tasks) - The chef ensures everything is ready at the right time</p>"},{"location":"getting-started/concepts/#why-it-matters","title":"Why It Matters","text":"<p>Without orchestration, you'd have to manually manage: - \u2705 Which tasks to run - \u2705 When to run them - \u2705 What happens if one fails - \u2705 How to track progress</p> <p>With apflow, the TaskManager handles all of this automatically!</p>"},{"location":"getting-started/concepts/#what-is-a-task-tree","title":"What is a Task Tree?","text":"<p>A task tree is a hierarchical structure that organizes related tasks. It's like a family tree for your tasks.</p>"},{"location":"getting-started/concepts/#visual-example","title":"Visual Example","text":"<pre><code>flowchart TD\n    Root[\"Root Task: Prepare Dinner\"] --&gt; Task1[\"Task 1: Buy Ingredients\"]\n    Root --&gt; Task2[\"Task 2: Cook Main Course\"]\n    Root --&gt; Task3[\"Task 3: Set Table\"]\n\n    Task1 --&gt; Task11[\"Task 1.1: Go to Store\"]\n    Task2 --&gt; Task21[\"Task 2.1: Cook Pasta\"]\n    Task2 --&gt; Task22[\"Task 2.2: Make Sauce\"]\n\n    style Root fill:#e1f5ff\n    style Task1 fill:#fff4e1\n    style Task2 fill:#fff4e1\n    style Task3 fill:#fff4e1\n    style Task11 fill:#e8f5e9\n    style Task21 fill:#e8f5e9\n    style Task22 fill:#e8f5e9</code></pre>"},{"location":"getting-started/concepts/#key-points","title":"Key Points","text":"<ul> <li>Parent-Child Relationship: Used for organization only (like folders)</li> <li>Dependencies: Control execution order (Task 2 waits for Task 1)</li> <li>Root Task: The top-level task that contains everything</li> </ul>"},{"location":"getting-started/concepts/#in-code_1","title":"In Code","text":"<pre><code># Create root task\nroot = await task_repository.create_task(name=\"prepare_dinner\", ...)\n\n# Create child tasks\nbuy_ingredients = await task_repository.create_task(\n    name=\"buy_ingredients\",\n    parent_id=root.id,  # Child of root\n    ...\n)\n\n# Build tree\ntree = TaskTreeNode(root)\ntree.add_child(TaskTreeNode(buy_ingredients))\n</code></pre>"},{"location":"getting-started/concepts/#what-are-dependencies","title":"What are Dependencies?","text":"<p>Dependencies define relationships between tasks. A task with dependencies will wait for those tasks to complete before executing.</p>"},{"location":"getting-started/concepts/#real-world-analogy_2","title":"Real-World Analogy","text":"<p>You can't serve dinner until: 1. \u2705 Ingredients are bought (Task 1) 2. \u2705 Food is cooked (Task 2) 3. \u2705 Table is set (Task 3)</p> <p>Task 4 (Serve Dinner) depends on Tasks 1, 2, and 3.</p>"},{"location":"getting-started/concepts/#visual-example_1","title":"Visual Example","text":"<pre><code>sequenceDiagram\n    participant TaskA as Task A: Fetch Data\n    participant TaskB as Task B: Process Data\n    participant TaskC as Task C: Save Results\n\n    Note over TaskA: No dependencies&lt;br/&gt;Executes first\n    TaskA-&gt;&gt;TaskA: Execute\n    TaskA--&gt;&gt;TaskB: Result available\n\n    Note over TaskB: Depends on Task A&lt;br/&gt;Waits for completion\n    TaskB-&gt;&gt;TaskB: Execute (with Task A result)\n    TaskB--&gt;&gt;TaskC: Result available\n\n    Note over TaskC: Depends on Task B&lt;br/&gt;Waits for completion\n    TaskC-&gt;&gt;TaskC: Execute (with Task B result)\n    TaskC--&gt;&gt;TaskC: Complete</code></pre> <p>Execution Order: A \u2192 B \u2192 C (automatic!)</p>"},{"location":"getting-started/concepts/#in-code_2","title":"In Code","text":"<pre><code># Task B depends on Task A\ntask_b = await task_repository.create_task(\n    name=\"process_data\",\n    dependencies=[\n        {\"id\": task_a.id, \"required\": True}  # Must wait for A\n    ],\n    ...\n)\n</code></pre>"},{"location":"getting-started/concepts/#important-distinction","title":"Important Distinction","text":"<ul> <li>Parent-Child: Organizational (like folders) - doesn't affect execution</li> <li>Dependencies: Execution control - determines when tasks run</li> </ul> <pre><code># Task B is a child of Task A (organization)\n# But Task B depends on Task C (execution order)\ntask_b = await task_repository.create_task(\n    parent_id=task_a.id,  # Organizational: B is child of A\n    dependencies=[{\"id\": task_c.id}],  # Execution: B waits for C\n    ...\n)\n</code></pre>"},{"location":"getting-started/concepts/#what-are-executors","title":"What are Executors?","text":"<p>An executor is the code that actually runs a task. It's like a worker that knows how to do a specific job.</p>"},{"location":"getting-started/concepts/#types-of-executors","title":"Types of Executors","text":"<ol> <li>Built-in Executors: Provided by apflow</li> <li>Core Executors (always available):<ul> <li><code>system_info_executor</code>: Get system information</li> <li><code>command_executor</code>: Run shell commands</li> <li><code>aggregate_results_executor</code>: Aggregate dependency results</li> </ul> </li> <li>Remote Execution Executors:<ul> <li><code>rest_executor</code>: HTTP/REST API calls (requires <code>[http]</code>)</li> <li><code>ssh_executor</code>: Remote command execution via SSH (requires <code>[ssh]</code>)</li> <li><code>grpc_executor</code>: gRPC service calls (requires <code>[grpc]</code>)</li> <li><code>websocket_executor</code>: Bidirectional WebSocket communication</li> <li><code>apflow_api_executor</code>: Call other apflow API instances</li> <li><code>mcp_executor</code>: Model Context Protocol executor (stdio mode: no dependencies, HTTP mode: requires <code>[a2a]</code>)</li> </ul> </li> <li>Protocol Servers:<ul> <li><code>a2a</code>: A2A Protocol Server (default)</li> <li><code>mcp</code>: MCP (Model Context Protocol) Server - exposes task orchestration as MCP tools and resources</li> </ul> </li> <li>Container Executors:<ul> <li><code>docker_executor</code>: Containerized command execution (requires <code>[docker]</code>)</li> </ul> </li> <li>AI Executors (optional):<ul> <li><code>crewai_executor</code>: LLM-based agents (requires <code>[crewai]</code>)</li> <li><code>batch_crewai_executor</code>: Batch execution of multiple crews (requires <code>[crewai]</code>)</li> </ul> </li> <li> <p>Generation Executors:</p> <ul> <li><code>generate_executor</code>: Generate task tree JSON arrays from natural language requirements using LLM (requires <code>openai</code> or <code>anthropic</code> package)</li> </ul> </li> <li> <p>Custom Executors: You create these</p> </li> <li>API calls</li> <li>Data processing</li> <li>File operations</li> <li>Anything you need!</li> </ol>"},{"location":"getting-started/concepts/#real-world-analogy_3","title":"Real-World Analogy","text":"<p>Think of executors as specialized workers: - Plumber (executor) knows how to fix pipes (task type) - Electrician (executor) knows how to fix wiring (task type) - Each worker (executor) has specific skills (code)</p>"},{"location":"getting-started/concepts/#in-code_3","title":"In Code","text":"<pre><code>from apflow import BaseTask, executor_register\n\n@executor_register()\nclass MyCustomExecutor(BaseTask):\n    \"\"\"My custom task executor\"\"\"\n\n    id = \"my_custom_executor\"\n    name = \"My Custom Executor\"\n\n    async def execute(self, inputs):\n        # Your task logic here\n        return {\"result\": \"done\"}\n</code></pre>"},{"location":"getting-started/concepts/#what-is-taskmanager","title":"What is TaskManager?","text":"<p>TaskManager is the orchestrator - it manages task execution, dependencies, and priorities. You don't need to worry about the details; it handles everything automatically.</p>"},{"location":"getting-started/concepts/#what-taskmanager-does","title":"What TaskManager Does","text":"<ol> <li>Checks Dependencies: Ensures tasks wait for their dependencies</li> <li>Schedules Execution: Runs tasks in the right order</li> <li>Handles Failures: Manages errors and retries</li> <li>Tracks Progress: Monitors task status</li> <li>Manages Priorities: Executes high-priority tasks first</li> </ol>"},{"location":"getting-started/concepts/#taskmanager-workflow","title":"TaskManager Workflow","text":"<p>The following diagram shows how TaskManager orchestrates task execution:</p> <pre><code>flowchart TD\n    Start([Task Tree Received]) --&gt; LoadTasks[Load All Tasks]\n    LoadTasks --&gt; CheckDeps[Check Dependencies for Each Task]\n    CheckDeps --&gt; ReadyTasks{Any Tasks&lt;br/&gt;Ready?}\n\n    ReadyTasks --&gt;|No| Wait[Wait for Dependencies]\n    Wait --&gt; CheckDeps\n\n    ReadyTasks --&gt;|Yes| SortPriority[Sort by Priority]\n    SortPriority --&gt; Execute[Execute Highest Priority Task]\n\n    Execute --&gt; UpdateStatus[Update Status: in_progress]\n    UpdateStatus --&gt; RunExecutor[Run Executor]\n    RunExecutor --&gt; Success{Success?}\n\n    Success --&gt;|Yes| Complete[Update Status: completed]\n    Success --&gt;|No| Failed[Update Status: failed]\n\n    Complete --&gt; CheckDependents[Check Dependent Tasks]\n    Failed --&gt; CheckDependents\n\n    CheckDependents --&gt; MoreReady{More Tasks&lt;br/&gt;Ready?}\n    MoreReady --&gt;|Yes| SortPriority\n    MoreReady --&gt;|No| AllDone{All Tasks&lt;br/&gt;Complete?}\n\n    AllDone --&gt;|No| CheckDeps\n    AllDone --&gt;|Yes| End([Execution Complete])\n\n    style Start fill:#e1f5ff\n    style Execute fill:#c8e6c9\n    style Complete fill:#c8e6c9\n    style End fill:#c8e6c9\n    style Failed fill:#ffcdd2\n    style Wait fill:#fff9c4</code></pre>"},{"location":"getting-started/concepts/#real-world-analogy_4","title":"Real-World Analogy","text":"<p>TaskManager is like a project manager: - Knows what needs to be done (tasks) - Knows the order (dependencies) - Assigns work (execution) - Monitors progress (status tracking) - Handles problems (error management)</p>"},{"location":"getting-started/concepts/#in-code_4","title":"In Code","text":"<pre><code># Create TaskManager\ntask_manager = TaskManager(db)\n\n# Give it a task tree\ntask_tree = TaskTreeNode(root_task)\ntask_tree.add_child(TaskTreeNode(child_task))\n\n# TaskManager handles everything automatically\nawait task_manager.distribute_task_tree(task_tree)\n</code></pre>"},{"location":"getting-started/concepts/#task-lifecycle","title":"Task Lifecycle","text":"<p>Tasks go through different states during their lifecycle:</p> <pre><code>pending \u2192 in_progress \u2192 completed\n              \u2502\n              \u2514\u2500\u2500&gt; failed\n              \u2502\n              \u2514\u2500\u2500&gt; cancelled\n</code></pre>"},{"location":"getting-started/concepts/#states-explained","title":"States Explained","text":"<ul> <li>pending: Task is created but not yet executed</li> <li>in_progress: Task is currently running</li> <li>completed: Task finished successfully</li> <li>failed: Task execution failed</li> <li>cancelled: Task was cancelled before completion</li> </ul>"},{"location":"getting-started/concepts/#visual-flow","title":"Visual Flow","text":"<pre><code>Create Task \u2192 pending\n     \u2502\n     \u25bc\nExecute Task \u2192 in_progress\n     \u2502\n     \u251c\u2500\u2500&gt; Success \u2192 completed\n     \u2502\n     \u251c\u2500\u2500&gt; Error \u2192 failed\n     \u2502\n     \u2514\u2500\u2500&gt; Cancelled \u2192 cancelled\n</code></pre>"},{"location":"getting-started/concepts/#priorities","title":"Priorities","text":"<p>Priority controls execution order when multiple tasks are ready to run. Lower numbers = higher priority.</p>"},{"location":"getting-started/concepts/#priority-levels","title":"Priority Levels","text":"<ul> <li>0: Urgent (highest priority)</li> <li>1: High</li> <li>2: Normal (default)</li> <li>3: Low (lowest priority)</li> </ul>"},{"location":"getting-started/concepts/#example","title":"Example","text":"<pre><code># Urgent task runs first\nurgent = await task_repository.create_task(\n    name=\"urgent_task\",\n    priority=0,  # Executes first\n    ...\n)\n\n# Normal task runs later\nnormal = await task_repository.create_task(\n    name=\"normal_task\",\n    priority=2,  # Executes after urgent\n    ...\n)\n</code></pre>"},{"location":"getting-started/concepts/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's how these concepts work together:</p> <pre><code># 1. Create TaskManager (the orchestrator)\ntask_manager = TaskManager(db)\n\n# 2. Create tasks with dependencies\ntask_a = await task_repository.create_task(\n    name=\"fetch_data\",\n    priority=1,\n    ...\n)\n\ntask_b = await task_repository.create_task(\n    name=\"process_data\",\n    parent_id=root.id,  # Organizational: child of root\n    dependencies=[{\"id\": task_a.id}],  # Execution: waits for A\n    priority=2,\n    ...\n)\n\n# 3. Build task tree (organization)\ntree = TaskTreeNode(root)\ntree.add_child(TaskTreeNode(task_a))\ntree.add_child(TaskTreeNode(task_b))\n\n# 4. TaskManager handles execution automatically\n# - Checks dependencies\n# - Executes in order (A, then B)\n# - Tracks status\n# - Handles errors\nawait task_manager.distribute_task_tree(tree)\n</code></pre>"},{"location":"getting-started/concepts/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Task: A unit of work with inputs and results</li> <li>Task Tree: Hierarchical organization of tasks</li> <li>Dependencies: Control execution order (not parent-child!)</li> <li>Executor: The code that runs a task</li> <li>TaskManager: Automatically orchestrates everything</li> <li>Priority: Controls execution order (lower = higher priority)</li> </ol>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<p>Now that you understand the core concepts:</p> <ul> <li>Ready to code? \u2192 Quick Start Guide</li> <li>Want examples? \u2192 Basic Examples</li> <li>Need details? \u2192 Task Orchestration Guide</li> </ul> <p>Confused about something? Check the FAQ or continue to Quick Start to see these concepts in action!</p>"},{"location":"getting-started/concepts/#task-data-fields-inputs-params-and-result","title":"Task Data Fields: inputs, params, and result","text":"<p>To ensure clean, composable, and predictable task orchestration, apflow enforces a strict separation of concerns for task data fields:</p> <ul> <li>inputs: Only business input data for the task. This is the data the executor will process (e.g., text to summarize, file to process, etc.). It should never include configuration, credentials, or executor-specific settings.</li> <li>params: Only executor configuration and setup parameters (e.g., API keys, model names, connection info). These are used to initialize the executor and are not passed as business data.</li> <li>result: Only the pure business output of the task. This is the value that downstream tasks will consume as their <code>inputs</code>. The <code>result</code> should not include logs, token usage, internal metadata, or any executor-specific structure\u2014just the output relevant to the user or next task.</li> </ul> <p>Why this matters: This separation ensures that: - Task data flows are clean and composable in a task tree. - Executors are reusable and predictable. - Downstream tasks can directly use upstream results as their inputs, without worrying about mixed-in configuration or irrelevant metadata.</p> <p>Best Practice: - Executors should only use <code>params</code> for initialization, process <code>inputs</code> as business data, and return a clean <code>result</code>. - Never mix configuration into <code>inputs</code> or <code>result</code>.</p> <p>For detailed field definitions and examples, see the Data Model Protocol documentation.</p>"},{"location":"getting-started/examples/","title":"Examples","text":"<p>This page contains examples and use cases for apflow.</p>"},{"location":"getting-started/examples/#demo-task-initialization","title":"Demo Task Initialization","text":"<p>Note: The built-in examples module has been removed from apflow core library. For demo task initialization, please use the apflow-demo project instead.</p> <p>The apflow-demo project provides: - Complete demo tasks for all executors - Per-user demo task initialization - Demo task validation against executor schemas</p> <p>For more information, see the apflow-demo repository.</p>"},{"location":"getting-started/examples/#executor-metadata-api","title":"Executor Metadata API","text":"<p>apflow provides utilities to query executor metadata for demo task generation:</p> <pre><code>from apflow.core.extensions import (\n    get_executor_metadata,\n    validate_task_format,\n    get_all_executor_metadata\n)\n\n# Get metadata for a specific executor\nmetadata = get_executor_metadata(\"system_info_executor\")\n# Returns: id, name, description, input_schema, examples, tags\n\n# Validate a task against executor schema\ntask = {\n    \"name\": \"CPU Analysis\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"cpu\"}\n}\nis_valid = validate_task_format(task, \"system_info_executor\")\n\n# Get metadata for all executors\nall_metadata = get_all_executor_metadata()\n</code></pre>"},{"location":"getting-started/examples/#basic-examples","title":"Basic Examples","text":"<p>Examples are also available in the test cases:</p> <ul> <li>Integration tests: <code>tests/integration/</code></li> <li>Extension tests: <code>tests/extensions/</code><pre><code>    \"properties\": {\n</code></pre> </li> </ul> <p>Note: Built-in demo tasks have moved to the apflow-demo project. For full demo task initialization and validation, please use that repository.</p> <p>For in-project runnable examples and patterns, see: - Basic Task Examples - Real-World Examples - Task Tree Examples     parent_id=root.id )</p> <p>child2 = await task_manager.task_repository.create_task(     name=\"child2\",     user_id=\"user_123\",     parent_id=root.id,     dependencies=[child1.id]  # child2 depends on child1 )</p>"},{"location":"getting-started/examples/#examples_1","title":"Examples","text":""},{"location":"getting-started/examples/#build-and-execute","title":"Build and execute","text":"<p>tree = TaskTreeNode(root) tree.add_child(TaskTreeNode(child1)) tree.add_child(TaskTreeNode(child2))</p> <p>result = await task_manager.distribute_task_tree(tree) <pre><code>## Example: CrewAI Task with LLM Key\n\n```python\n# Via API with header\nimport httpx\n\nasync with httpx.AsyncClient() as client:\n    response = await client.post(\n        \"http://localhost:8000/tasks\",\n        headers={\n            \"Content-Type\": \"application/json\",\n            \"X-LLM-API-KEY\": \"openai:sk-your-key\"  # Provider-specific format\n        },\n        json={\n            \"jsonrpc\": \"2.0\",\n            \"method\": \"tasks.create\",\n            \"params\": {\n                \"tasks\": [{\n                    \"id\": \"crewai-task\",\n                    \"name\": \"CrewAI Research Task\",\n                    \"user_id\": \"user123\",\n                    \"schemas\": {\"method\": \"crewai_executor\"},\n                    \"params\": {\n                        \"works\": {\n                            \"agents\": {\n                                \"researcher\": {\n                                    \"role\": \"Research Analyst\",\n                                    \"goal\": \"Research and analyze the given topic\",\n                                    \"llm\": \"openai/gpt-4\"\n                                }\n                            },\n                            \"tasks\": {\n                                \"research\": {\n                                    \"description\": \"Research the topic: {topic}\",\n                                    \"agent\": \"researcher\"\n                                }\n                            }\n                        }\n                    },\n                    \"inputs\": {\n                        \"topic\": \"Artificial Intelligence\"\n                    }\n                }]\n            }\n        }\n    )\n</code></pre></p> <p>For more examples, see the test cases in the main repository.</p>"},{"location":"getting-started/hello-world/","title":"Hello World - 5-Minute Quick Start","text":"<p>Get started with apflow in under 5 minutes with this minimal example.</p>"},{"location":"getting-started/hello-world/#installation","title":"Installation","text":"<pre><code>pip install apflow\n</code></pre>"},{"location":"getting-started/hello-world/#your-first-task-python","title":"Your First Task (Python)","text":"<p>Create a file <code>hello.py</code>:</p> <pre><code>from apflow import executor_register, TaskBuilder, execute_tasks\nfrom apflow.extensions import BaseTask\n\n# Define a simple executor\n@executor_register()\nclass HelloWorld(BaseTask):\n    id = \"hello_world\"\n\n    async def execute(self, inputs: dict) -&gt; dict:\n        name = inputs.get(\"name\", \"World\")\n        return {\"message\": f\"Hello, {name}!\"}\n\n# Create and execute a task\nif __name__ == \"__main__\":\n    task = TaskBuilder(\"greet_alice\", \"hello_world\")\\\n        .with_inputs({\"name\": \"Alice\"})\\\n        .build()\n\n    result = execute_tasks([task])\n    print(result[\"message\"])  # Output: Hello, Alice!\n</code></pre> <p>Run it:</p> <pre><code>python hello.py\n</code></pre> <p>Output: <pre><code>Hello, Alice!\n</code></pre></p> <p>That's it! You've created and executed your first apflow task.</p>"},{"location":"getting-started/hello-world/#using-the-cli","title":"Using the CLI","text":"<p>You can also use apflow via CLI without writing code:</p>"},{"location":"getting-started/hello-world/#1-create-a-task-module","title":"1. Create a task module","text":"<p><code>tasks.py</code>: <pre><code>from apflow import executor_register\nfrom apflow.extensions import BaseTask\n\n@executor_register()\nclass Greeter(BaseTask):\n    id = \"greeter\"\n\n    async def execute(self, inputs: dict) -&gt; dict:\n        return {\"message\": f\"Hello, {inputs['name']}!\"}\n</code></pre></p>"},{"location":"getting-started/hello-world/#2-execute-via-cli","title":"2. Execute via CLI","text":"<pre><code># Make sure tasks.py is imported (in same directory or PYTHONPATH)\napflow run greeter --inputs '{\"name\": \"Bob\"}'\n</code></pre> <p>Output: <pre><code>{\n  \"message\": \"Hello, Bob!\"\n}\n</code></pre></p>"},{"location":"getting-started/hello-world/#whats-happening","title":"What's Happening?","text":"<ol> <li>@executor_register() - Registers your task executor with apflow</li> <li>BaseTask - Base class providing the <code>execute()</code> interface</li> <li>TaskBuilder - Fluent API for creating tasks</li> <li>execute_tasks() - Executes one or more tasks and returns results</li> </ol>"},{"location":"getting-started/hello-world/#built-in-executors","title":"Built-in Executors","text":"<p>Don't want to write custom code? Use built-in executors:</p>"},{"location":"getting-started/hello-world/#rest-api-call","title":"REST API Call","text":"<pre><code>from apflow import TaskBuilder, execute_tasks\n\ntask = TaskBuilder(\"fetch_data\", \"rest_executor\")\\\n    .with_inputs({\n        \"url\": \"https://api.github.com/users/octocat\",\n        \"method\": \"GET\"\n    })\\\n    .build()\n\nresult = execute_tasks([task])\nprint(result[\"json\"][\"login\"])  # Output: octocat\n</code></pre>"},{"location":"getting-started/hello-world/#execute-ssh-command","title":"Execute SSH Command","text":"<pre><code>pip install apflow[ssh]\n</code></pre> <pre><code>task = TaskBuilder(\"check_disk\", \"ssh_executor\")\\\n    .with_inputs({\n        \"host\": \"example.com\",\n        \"username\": \"admin\",\n        \"key_file\": \"~/.ssh/id_rsa\",\n        \"command\": \"df -h\"\n    })\\\n    .build()\n\nresult = execute_tasks([task])\nprint(result[\"stdout\"])\n</code></pre>"},{"location":"getting-started/hello-world/#task-dependencies","title":"Task Dependencies","text":"<p>Create workflows with dependencies:</p> <pre><code>from apflow import TaskBuilder, execute_tasks\n\n# Task 1: Fetch user data\nfetch_task = TaskBuilder(\"fetch_user\", \"rest_executor\")\\\n    .with_inputs({\n        \"url\": \"https://api.example.com/user\",\n        \"method\": \"GET\"\n    })\\\n    .build()\n\n# Task 2: Process data (depends on Task 1)\nprocess_task = TaskBuilder(\"process_data\", \"my_processor\")\\\n    .with_inputs({\"user_id\": \"123\"})\\\n    .with_dependencies([fetch_task.id])\\\n    .build()\n\n# Execute: fetch_task runs first, then process_task\nresults = execute_tasks([fetch_task, process_task])\n</code></pre>"},{"location":"getting-started/hello-world/#next-steps","title":"Next Steps","text":"<ul> <li>Executor Selection Guide - Choose the right executor for your use case</li> <li>Complete Quick Start - Comprehensive tutorial with all features</li> <li>Task Dependencies - Learn about complex workflows</li> </ul>"},{"location":"getting-started/hello-world/#common-questions","title":"Common Questions","text":""},{"location":"getting-started/hello-world/#where-is-my-data-stored","title":"Where is my data stored?","text":"<p>By default, apflow uses DuckDB (embedded database) stored at <code>.data/apflow.duckdb</code> in your project directory.</p>"},{"location":"getting-started/hello-world/#how-do-i-see-task-status","title":"How do I see task status?","text":"<pre><code># List all tasks\napflow monitor status --all\n\n# Watch a specific task\napflow monitor watch &lt;task-id&gt;\n</code></pre>"},{"location":"getting-started/hello-world/#can-i-use-postgresql-instead-of-duckdb","title":"Can I use PostgreSQL instead of DuckDB?","text":"<p>Yes! Set the database URL:</p> <pre><code>export APFLOW_DATABASE_URL=\"postgresql://user:pass@localhost/apflow\"\napflow db migrate  # Run migrations\n</code></pre>"},{"location":"getting-started/hello-world/#how-do-i-deploy-to-production","title":"How do I deploy to production?","text":"<p>For production deployments, see: - Production Deployment - Distributed Mode (multi-node orchestration)</p>"},{"location":"getting-started/hello-world/#get-help","title":"Get Help","text":"<ul> <li>Documentation: https://docs.apflow.dev</li> <li>GitHub Issues: Report bugs or request features</li> <li>CLI Help: <code>apflow --help</code></li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Looking for a step-by-step beginner tutorial? See the Quick Start Guide for a hands-on introduction. This page lists all installation options and extras.</p> <p>apflow can be installed with different feature sets depending on your needs.</p>"},{"location":"getting-started/installation/#core-library-minimum","title":"Core Library (Minimum)","text":"<p>The core library provides pure task orchestration without any LLM dependencies:</p> <pre><code>pip install apflow\n</code></pre> <p>Includes: - Task orchestration specifications (TaskManager) - Core interfaces (ExecutableTask, BaseTask, TaskStorage) - Storage (DuckDB default) - NO CrewAI dependency</p> <p>Excludes: - CrewAI support - Batch execution - API server - CLI tools</p>"},{"location":"getting-started/installation/#with-optional-features","title":"With Optional Features","text":""},{"location":"getting-started/installation/#crewai-support","title":"CrewAI Support","text":"<pre><code>pip install apflow[crewai]\n</code></pre> <p>Includes: - CrewaiExecutor for LLM-based agent crews - BatchCrewaiExecutor for atomic batch execution of multiple crews</p>"},{"location":"getting-started/installation/#a2a-protocol-server","title":"A2A Protocol Server","text":"<pre><code>pip install apflow[a2a]\n</code></pre> <p>Includes: - A2A Protocol Server for agent-to-agent communication - HTTP, SSE, and WebSocket support</p> <p>Usage: <pre><code># Run A2A server\npython -m apflow.api.main\n\n# Or use the CLI command\napflow-server\n</code></pre></p>"},{"location":"getting-started/installation/#cli-tools","title":"CLI Tools","text":"<pre><code>pip install apflow[cli]\n</code></pre> <p>Includes: - Command-line interface tools</p> <p>Usage: <pre><code># Run CLI\napflow\n\n# Or use the shorthand\napflow\n</code></pre></p>"},{"location":"getting-started/installation/#postgresql-storage","title":"PostgreSQL Storage","text":"<pre><code>pip install apflow[postgres]\n</code></pre> <p>Includes: - PostgreSQL storage support (for enterprise/distributed scenarios)</p>"},{"location":"getting-started/installation/#ssh-executor","title":"SSH Executor","text":"<pre><code>pip install apflow[ssh]\n</code></pre> <p>Includes: - SSH executor for remote command execution - Execute commands on remote servers via SSH</p>"},{"location":"getting-started/installation/#docker-executor","title":"Docker Executor","text":"<pre><code>pip install apflow[docker]\n</code></pre> <p>Includes: - Docker executor for containerized execution - Execute commands in isolated Docker containers</p>"},{"location":"getting-started/installation/#grpc-executor","title":"gRPC Executor","text":"<pre><code>pip install apflow[grpc]\n</code></pre> <p>Includes: - gRPC executor for gRPC service calls - Call gRPC services and microservices</p>"},{"location":"getting-started/installation/#email-executor","title":"Email Executor","text":"<pre><code>pip install apflow[email]\n</code></pre> <p>Includes: - Email executor for sending emails via Resend API or SMTP - Send transactional and notification emails from task workflows</p>"},{"location":"getting-started/installation/#graphql-api","title":"GraphQL API","text":"<pre><code>pip install apflow[graphql]\n</code></pre> <p>Includes: - GraphQL API server with Strawberry GraphQL - Typed schema with queries, mutations, and subscriptions - GraphiQL interactive playground</p>"},{"location":"getting-started/installation/#everything","title":"Everything","text":"<pre><code>pip install apflow[all]\n</code></pre> <p>Includes: - All optional features (crewai, a2a, cli, postgres, email, ssh, docker, grpc, graphql)</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.10 or higher (3.12+ recommended)</li> <li>DuckDB: Included by default (no setup required)</li> <li>PostgreSQL: Optional, for distributed/production scenarios</li> </ul>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development, install with development dependencies:</p> <pre><code># Clone the repository\ngit clone https://github.com/aipartnerup/apflow.git\ncd apflow\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install in development mode with all features\npip install -e \".[all,dev]\"\n</code></pre>"},{"location":"getting-started/installation/#verification","title":"Verification","text":"<p>After installation, verify the installation:</p> <pre><code>import apflow\nprint(apflow.__version__)\n</code></pre> <p>Or using the CLI (if installed with <code>[cli]</code>):</p> <pre><code>apflow --version\n</code></pre>"},{"location":"getting-started/quick-start/","title":"Quick Start Guide","text":"<p>Need more installation options? See the Installation Guide for advanced and optional feature setups. This page is for new users to get started quickly.</p> <p>Get started with apflow in 10 minutes. This guide will walk you through creating and executing your first task.</p>"},{"location":"getting-started/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher (3.12+ recommended)</li> <li>Basic command-line knowledge (for CLI usage)</li> <li>Basic Python knowledge (for creating custom executors)</li> </ul>"},{"location":"getting-started/quick-start/#step-0-installation","title":"Step 0: Installation","text":""},{"location":"getting-started/quick-start/#minimal-installation-core-only","title":"Minimal Installation (Core Only)","text":"<pre><code>pip install apflow\n</code></pre> <p>This installs the core orchestration framework with no optional dependencies.</p> <p>What you get: - Task orchestration engine (TaskManager) - Built-in executors (system_info_executor, command_executor, llm_executor) - Storage (DuckDB - no setup needed!)</p>"},{"location":"getting-started/quick-start/#full-installation-all-features","title":"Full Installation (All Features)","text":"<pre><code>pip install apflow[all]\n</code></pre> <p>This includes everything: - Core orchestration framework - CrewAI support for LLM tasks - A2A Protocol Server - CLI tools - PostgreSQL storage support</p> <p>For this tutorial:  - Step 1 (CLI): Minimal installation is enough! - Step 2 (API): Install with <code>[cli]</code> or <code>[all]</code> to get CLI tools for starting the API server - Step 5 (Custom Executors): Minimal installation is enough for creating executors</p>"},{"location":"getting-started/quick-start/#step-1-your-first-task-using-cli","title":"Step 1: Your First Task (Using CLI)","text":"<p>Let's start with the simplest possible example - using the CLI to execute a built-in executor. No Python code needed!</p>"},{"location":"getting-started/quick-start/#what-well-do","title":"What We'll Do","text":"<p>We'll execute a task that gets system information (CPU, memory, disk) using the built-in <code>system_info_executor</code> via the command line.</p>"},{"location":"getting-started/quick-start/#execute-your-first-task","title":"Execute Your First Task","text":"<p>Open your terminal and run:</p> <pre><code>apflow run flow system_info_executor --inputs '{\"resource\": \"cpu\"}'\n</code></pre> <p>Expected Output: <pre><code>{\n  \"status\": \"completed\",\n  \"progress\": 1.0,\n  \"root_task_id\": \"abc-123-def-456\",\n  \"task_count\": 1,\n  \"result\": {\n    \"system\": \"Darwin\",\n    \"cores\": 8,\n    \"cpu_count\": 8,\n    ...\n  }\n}\n</code></pre></p>"},{"location":"getting-started/quick-start/#try-different-resources","title":"Try Different Resources","text":"<p>You can get different system information by changing the <code>resource</code> parameter:</p> <pre><code># Get memory information\napflow run flow system_info_executor --inputs '{\"resource\": \"memory\"}'\n\n# Get disk information\napflow run flow system_info_executor --inputs '{\"resource\": \"disk\"}'\n\n# Get all system resources\napflow run flow system_info_executor --inputs '{\"resource\": \"all\"}'\n</code></pre>"},{"location":"getting-started/quick-start/#what-just-happened","title":"What Just Happened?","text":"<ol> <li>CLI parsed the command: It identified the executor (<code>system_info_executor</code>) and inputs</li> <li>Task was created: A task was automatically created in the database</li> <li>Executor was found: The system automatically found the built-in <code>system_info_executor</code></li> <li>Task executed: The executor ran and collected CPU information</li> <li>Result returned: The result was displayed in JSON format</li> </ol> <p>That's it! You just executed your first task with apflow! \ud83c\udf89</p>"},{"location":"getting-started/quick-start/#understanding-the-command","title":"Understanding the Command","text":"<p>Let's break down the command:</p> <pre><code>apflow run flow system_info_executor --inputs '{\"resource\": \"cpu\"}'\n</code></pre> <ul> <li><code>apflow</code>: The CLI command (short for <code>apflow</code>)</li> <li><code>run flow</code>: Execute a task flow</li> <li><code>system_info_executor</code>: The executor ID (built-in executor)</li> <li><code>--inputs</code>: Task input parameters (JSON format)</li> <li><code>{\"resource\": \"cpu\"}</code>: Input data - get CPU information</li> </ul> <p>Note: This is the \"legacy mode\" for backward compatibility. For more complex scenarios, you can use the standard mode with task arrays (see Step 4).</p>"},{"location":"getting-started/quick-start/#step-2-using-the-api-server","title":"Step 2: Using the API Server","text":"<p>The API server provides an alternative way to execute tasks via HTTP. This is useful for remote access, integration with other systems, or when you prefer HTTP over CLI.</p>"},{"location":"getting-started/quick-start/#start-the-api-server","title":"Start the API Server","text":"<p>In one terminal, start the API server:</p> <pre><code>apflow serve\n</code></pre> <p>The server will start on <code>http://localhost:8000</code> by default.</p>"},{"location":"getting-started/quick-start/#execute-a-task-via-api","title":"Execute a Task via API","text":"<p>In another terminal (or use the same one), execute a task via HTTP:</p> <pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [\n        {\n          \"id\": \"task1\",\n          \"name\": \"system_info_executor\",\n          \"user_id\": \"user123\",\n          \"schemas\": {\n            \"method\": \"system_info_executor\"\n          },\n          \"inputs\": {\n            \"resource\": \"cpu\"\n          }\n        }\n      ]\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre> <p>Expected Response: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"id\": \"request-123\",\n  \"result\": {\n    \"status\": \"completed\",\n    \"progress\": 1.0,\n    \"root_task_id\": \"abc-123-def-456\",\n    \"task_count\": 1,\n    \"result\": {\n      \"system\": \"Darwin\",\n      \"cores\": 8,\n      ...\n    }\n  }\n}\n</code></pre></p>"},{"location":"getting-started/quick-start/#understanding-the-api-request","title":"Understanding the API Request","text":"<p>The API uses the A2A Protocol (JSON-RPC 2.0 format):</p> <ul> <li>method: <code>tasks.execute</code> - Execute a task tree</li> <li>params.tasks: Array of task objects</li> <li>params.tasks[].schemas.method: Executor ID (must match executor <code>id</code>)</li> <li>params.tasks[].inputs: Task input parameters</li> </ul>"},{"location":"getting-started/quick-start/#cli-vs-api-when-to-use-which","title":"CLI vs API: When to Use Which?","text":"<p>Use CLI when: - Local development and testing - Quick one-off tasks - Scripts and automation - No need for remote access</p> <p>Use API when: - Remote access needed - Integration with other systems - Multi-user scenarios - Production deployments - A2A Protocol integration</p> <p>Both share the same database, so you can: - Execute via CLI, query via API - Execute via API, query via CLI - Mix and match as needed!</p>"},{"location":"getting-started/quick-start/#task-statistics","title":"Task Statistics","text":"<p>You can quickly get an overview of all tasks in the database: <pre><code>apflow tasks count\n</code></pre></p>"},{"location":"getting-started/quick-start/#step-3-understanding-task-execution","title":"Step 3: Understanding Task Execution","text":"<p>Let's break down what happened in more detail:</p>"},{"location":"getting-started/quick-start/#the-components","title":"The Components","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              User Interface                     \u2502\n\u2502  (CLI or API)                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            TaskExecutor                         \u2502\n\u2502  - Creates tasks                                 \u2502\n\u2502  - Manages execution                            \u2502\n\u2502  - Tracks status                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Task (in database)                      \u2502\n\u2502  - name: \"system_info_executor\"                 \u2502\n\u2502  - inputs: {\"resource\": \"cpu\"}                   \u2502\n\u2502  - status: pending \u2192 in_progress \u2192 completed    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Executor (system_info_executor)            \u2502\n\u2502  - Runs the actual code                         \u2502\n\u2502  - Returns the result                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/quick-start/#key-concepts","title":"Key Concepts","text":"<ul> <li>Task: A unit of work (what you want to do)</li> <li>Executor: The code that does the work (how it's done)</li> <li>TaskExecutor: Coordinates everything (the conductor)</li> <li>CLI/API: User interfaces to interact with the system</li> <li>Task Tree: Organizes tasks (even single tasks need a tree)</li> </ul>"},{"location":"getting-started/quick-start/#advanced-direct-taskmanager-usage","title":"Advanced: Direct TaskManager Usage","text":"<p>For advanced use cases, you can use <code>TaskManager</code> directly in Python code:</p> <pre><code>from apflow import TaskManager, TaskTreeNode, create_session\n\ndb = create_session()\ntask_manager = TaskManager(db)\n# ... create and execute tasks\n</code></pre> <p>Note: This is for advanced users. Most use cases work better with CLI or API.</p>"},{"location":"getting-started/quick-start/#step-4-task-dependencies","title":"Step 4: Task Dependencies","text":"<p>Now let's create multiple tasks where one depends on another. This is where apflow really shines!</p>"},{"location":"getting-started/quick-start/#example-sequential-tasks-via-cli","title":"Example: Sequential Tasks via CLI","text":"<p>Create a file <code>tasks.json</code> with a task array:</p> <pre><code>[\n  {\n    \"id\": \"cpu_task\",\n    \"name\": \"Get CPU Info\",\n    \"user_id\": \"user123\",\n    \"schemas\": {\n      \"method\": \"system_info_executor\"\n    },\n    \"inputs\": {\n      \"resource\": \"cpu\"\n    },\n    \"priority\": 1,\n    \"status\": \"pending\"\n  },\n  {\n    \"id\": \"memory_task\",\n    \"name\": \"Get Memory Info\",\n    \"user_id\": \"user123\",\n    \"parent_id\": \"cpu_task\",\n    \"schemas\": {\n      \"method\": \"system_info_executor\"\n    },\n    \"inputs\": {\n      \"resource\": \"memory\"\n    },\n    \"dependencies\": [\n      {\n        \"id\": \"cpu_task\",\n        \"required\": true\n      }\n    ],\n    \"priority\": 2,\n    \"status\": \"pending\"\n  }\n]\n</code></pre> <p>Execute via CLI:</p> <pre><code>apflow run flow --tasks-file tasks.json\n</code></pre> <p>What happens: 1. <code>cpu_task</code> executes first 2. System waits for <code>cpu_task</code> to complete 3. <code>memory_task</code> executes after <code>cpu_task</code> completes</p>"},{"location":"getting-started/quick-start/#example-sequential-tasks-via-api","title":"Example: Sequential Tasks via API","text":"<p>Start the API server (if not already running):</p> <pre><code>apflow serve\n</code></pre> <p>Execute via API:</p> <pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [\n        {\n          \"id\": \"cpu_task\",\n          \"name\": \"Get CPU Info\",\n          \"user_id\": \"user123\",\n          \"schemas\": {\"method\": \"system_info_executor\"},\n          \"inputs\": {\"resource\": \"cpu\"},\n          \"priority\": 1,\n          \"status\": \"pending\"\n        },\n        {\n          \"id\": \"memory_task\",\n          \"name\": \"Get Memory Info\",\n          \"user_id\": \"user123\",\n          \"parent_id\": \"cpu_task\",\n          \"schemas\": {\"method\": \"system_info_executor\"},\n          \"inputs\": {\"resource\": \"memory\"},\n          \"dependencies\": [{\"id\": \"cpu_task\", \"required\": true}],\n          \"priority\": 2,\n          \"status\": \"pending\"\n        }\n      ]\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre>"},{"location":"getting-started/quick-start/#understanding-dependencies","title":"Understanding Dependencies","text":"<p>Key Point: <code>dependencies</code> control execution order, not <code>parent_id</code>!</p> <ul> <li>parent_id: Organizational (like folders) - doesn't affect when tasks run</li> <li>dependencies: Execution control - determines when tasks run</li> </ul> <p>In the example above: - <code>memory_task</code> is a child of <code>cpu_task</code> (organization via <code>parent_id</code>) - <code>memory_task</code> depends on <code>cpu_task</code> (execution order via <code>dependencies</code>)</p>"},{"location":"getting-started/quick-start/#try-it-yourself","title":"Try It Yourself","text":"<p>Create three tasks in sequence (CPU \u2192 Memory \u2192 Disk):</p> <p>tasks.json: <pre><code>[\n  {\n    \"id\": \"cpu_task\",\n    \"name\": \"Get CPU Info\",\n    \"user_id\": \"user123\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"cpu\"},\n    \"status\": \"pending\"\n  },\n  {\n    \"id\": \"memory_task\",\n    \"name\": \"Get Memory Info\",\n    \"user_id\": \"user123\",\n    \"parent_id\": \"cpu_task\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"memory\"},\n    \"dependencies\": [{\"id\": \"cpu_task\", \"required\": true}],\n    \"status\": \"pending\"\n  },\n  {\n    \"id\": \"disk_task\",\n    \"name\": \"Get Disk Info\",\n    \"user_id\": \"user123\",\n    \"parent_id\": \"memory_task\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"disk\"},\n    \"dependencies\": [{\"id\": \"memory_task\", \"required\": true}],\n    \"status\": \"pending\"\n  }\n]\n</code></pre></p> <p>Execute: <pre><code>apflow run flow --tasks-file tasks.json\n</code></pre></p> <p>Execution Order: CPU \u2192 Memory \u2192 Disk (automatic!)</p>"},{"location":"getting-started/quick-start/#step-5-creating-your-own-executor","title":"Step 5: Creating Your Own Executor","text":"<p>Now let's create a custom executor. This is where you add your own business logic!</p>"},{"location":"getting-started/quick-start/#create-the-executor","title":"Create the Executor","text":"<p>Create a file <code>my_executor.py</code>:</p> <pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any, Literal\nfrom pydantic import BaseModel, Field\n\n# Define input schema as a Pydantic model\nclass GreetingInputSchema(BaseModel):\n    \"\"\"Input schema for greeting task\"\"\"\n    name: str = Field(description=\"Name of the person to greet\")\n    language: Literal[\"en\", \"es\", \"fr\", \"zh\"] = Field(\n        default=\"en\", description=\"Language for the greeting\"\n    )\n\n@executor_register()\nclass GreetingTask(BaseTask):\n    \"\"\"A simple task that creates personalized greetings\"\"\"\n\n    id = \"greeting_task\"\n    name = \"Greeting Task\"\n    description = \"Creates a personalized greeting message\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = GreetingInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute the task\"\"\"\n        name = inputs.get(\"name\", \"Guest\")\n        language = inputs.get(\"language\", \"en\")\n\n        greetings = {\n            \"en\": f\"Hello, {name}!\",\n            \"es\": f\"\u00a1Hola, {name}!\",\n            \"fr\": f\"Bonjour, {name}!\"\n        }\n\n        return {\n            \"greeting\": greetings.get(language, greetings[\"en\"]),\n            \"name\": name,\n            \"language\": language\n        }\n</code></pre>"},{"location":"getting-started/quick-start/#use-your-custom-executor-via-cli","title":"Use Your Custom Executor via CLI","text":"<p>To use your custom executor, you need to import it first. Create a simple Python script that imports the executor and then uses CLI:</p> <p>Option A: Import in a wrapper script</p> <p>Create <code>run_greeting.py</code>:</p> <pre><code># Import to register the executor\nfrom my_executor import GreetingTask\n\n# Now you can use CLI\nimport subprocess\nimport sys\n\n# Run via CLI\nsubprocess.run([\n    \"apflow\", \"run\", \"flow\", \"greeting_task\",\n    \"--inputs\", '{\"name\": \"Alice\", \"language\": \"en\"}'\n])\n</code></pre> <p>Run it: <pre><code>python run_greeting.py\n</code></pre></p> <p>Option B: Direct CLI (after importing in Python session)</p> <p>If you're in a Python environment where the executor is already imported:</p> <pre><code>apflow run flow greeting_task --inputs '{\"name\": \"Alice\", \"language\": \"en\"}'\n</code></pre> <p>Option C: Use task array format</p> <p>Create <code>greeting_task.json</code>:</p> <pre><code>[\n  {\n    \"id\": \"greeting1\",\n    \"name\": \"Greet Alice\",\n    \"user_id\": \"user123\",\n    \"schemas\": {\n      \"method\": \"greeting_task\"\n    },\n    \"inputs\": {\n            \"name\": \"Alice\",\n            \"language\": \"en\"\n    },\n    \"status\": \"pending\"\n  }\n]\n</code></pre> <p>Run: <pre><code># Make sure executor is imported first (in Python)\npython -c \"from my_executor import GreetingTask\"\n\n# Then use CLI\napflow run flow --tasks-file greeting_task.json\n</code></pre></p>"},{"location":"getting-started/quick-start/#use-your-custom-executor-via-api","title":"Use Your Custom Executor via API","text":"<p>Start the API server (make sure executor is imported):</p> <pre><code># In a Python script that imports your executor\npython -c \"from my_executor import GreetingTask; import apflow.api.main; apflow.api.main.main()\"\n</code></pre> <p>Or create <code>api_server.py</code>:</p> <pre><code>from my_executor import GreetingTask  # Import to register\nfrom apflow.api.main import main\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Then execute via API:</p> <pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [\n        {\n          \"id\": \"greeting1\",\n          \"name\": \"Greet Alice\",\n          \"user_id\": \"user123\",\n          \"schemas\": {\"method\": \"greeting_task\"},\n          \"inputs\": {\"name\": \"Alice\", \"language\": \"en\"},\n          \"status\": \"pending\"\n        }\n      ]\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre>"},{"location":"getting-started/quick-start/#understanding-custom-executors","title":"Understanding Custom Executors","text":"<p>What you need to implement:</p> <ol> <li>id: Unique identifier (used in <code>schemas.method</code> when creating tasks)</li> <li>name: Display name</li> <li>description: What the task does</li> <li>execute(): The actual work (async function)</li> <li>get_input_schema(): Input parameter definition (JSON Schema)</li> </ol> <p>The <code>@executor_register()</code> decorator automatically registers your executor when imported!</p> <p>Important: The executor must be imported before it can be used. This happens automatically when: - You import it in your Python script - You import it before starting the API server - The extension system loads it (for built-in executors)</p>"},{"location":"getting-started/quick-start/#step-6-next-steps","title":"Step 6: Next Steps","text":"<p>Congratulations! You've learned the basics. Here's what to explore next:</p>"},{"location":"getting-started/quick-start/#immediate-next-steps","title":"Immediate Next Steps","text":"<ol> <li>Core Concepts - Deep dive into the concepts you just used</li> <li>First Steps Tutorial - More detailed beginner tutorial</li> <li>Basic Examples - Copy-paste ready examples</li> </ol>"},{"location":"getting-started/quick-start/#learn-more","title":"Learn More","text":"<ul> <li>Task Orchestration Guide - Master task trees and dependencies</li> <li>Custom Tasks Guide - Create more complex executors</li> <li>Best Practices - Learn from the experts</li> </ul>"},{"location":"getting-started/quick-start/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Task Trees Tutorial - Build complex workflows</li> <li>Dependencies Tutorial - Master dependency management</li> </ul>"},{"location":"getting-started/quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start/#pattern-1-simple-task-no-dependencies","title":"Pattern 1: Simple Task (No Dependencies)","text":"<p>Via CLI (Legacy Mode): <pre><code>apflow run flow executor_id --inputs '{\"key\": \"value\"}'\n</code></pre></p> <p>Via CLI (Standard Mode - Task Array): <pre><code>apflow run flow --tasks '[{\"id\": \"task1\", \"name\": \"Task 1\", \"user_id\": \"user123\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}, \"status\": \"pending\"}]'\n</code></pre></p> <p>Via API: <pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [{\n        \"id\": \"task1\",\n        \"name\": \"Task 1\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\"method\": \"system_info_executor\"},\n        \"inputs\": {\"resource\": \"cpu\"},\n        \"status\": \"pending\"\n      }]\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre></p>"},{"location":"getting-started/quick-start/#pattern-2-sequential-tasks-with-dependencies","title":"Pattern 2: Sequential Tasks (With Dependencies)","text":"<p>Via CLI (Task Array File):</p> <p>Create <code>tasks.json</code>: <pre><code>[\n  {\n    \"id\": \"task1\",\n    \"name\": \"Get System Info\",\n    \"user_id\": \"user123\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"cpu\"},\n    \"status\": \"pending\"\n  },\n  {\n    \"id\": \"task2\",\n    \"name\": \"Process Data\",\n    \"user_id\": \"user123\",\n    \"parent_id\": \"task1\",\n    \"schemas\": {\"method\": \"command_executor\"},\n    \"inputs\": {\"command\": \"echo 'Processing system info'\"},\n    \"dependencies\": [{\"id\": \"task1\", \"required\": true}],\n    \"status\": \"pending\"\n  }\n]\n</code></pre></p> <p>Execute: <pre><code>apflow run flow --tasks-file tasks.json\n</code></pre></p> <p>Via API: <pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [\n        {\n          \"id\": \"task1\",\n          \"name\": \"Get System Info\",\n          \"user_id\": \"user123\",\n          \"schemas\": {\"method\": \"system_info_executor\"},\n          \"inputs\": {\"resource\": \"cpu\"},\n          \"status\": \"pending\"\n        },\n        {\n          \"id\": \"task2\",\n          \"name\": \"Process Data\",\n          \"user_id\": \"user123\",\n          \"parent_id\": \"task1\",\n          \"schemas\": {\"method\": \"command_executor\"},\n          \"inputs\": {\"command\": \"echo 'Processing system info'\"},\n          \"dependencies\": [{\"id\": \"task1\", \"required\": true}],\n          \"status\": \"pending\"\n        }\n      ]\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre></p> <p>Execution Order: Task2 waits for Task1 automatically!</p>"},{"location":"getting-started/quick-start/#pattern-3-parallel-tasks-no-dependencies","title":"Pattern 3: Parallel Tasks (No Dependencies)","text":"<p>Via CLI (Task Array File):</p> <p>Create <code>parallel_tasks.json</code>: <pre><code>[\n  {\n    \"id\": \"task1\",\n    \"name\": \"Get CPU Info\",\n    \"user_id\": \"user123\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"cpu\"},\n    \"status\": \"pending\"\n  },\n  {\n    \"id\": \"task2\",\n    \"name\": \"Get Memory Info\",\n    \"user_id\": \"user123\",\n    \"schemas\": {\"method\": \"system_info_executor\"},\n    \"inputs\": {\"resource\": \"memory\"},\n    \"status\": \"pending\"\n  }\n]\n</code></pre></p> <p>Execute: <pre><code>apflow run flow --tasks-file parallel_tasks.json\n</code></pre></p> <p>Note: Both tasks run in parallel since there are no dependencies between them!</p>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#problem-task-executor-not-found","title":"Problem: Task Executor Not Found","text":"<p>Error: <code>Task executor not found: executor_id</code></p> <p>Solutions:</p> <ol> <li>For built-in executors: Built-in executors are automatically available. If you get this error:</li> <li>Make sure you've installed the required extension: <code>pip install apflow[extension_name]</code></li> <li> <p>Check that the executor ID is correct (e.g., <code>system_info_executor</code>, not <code>system_info</code>)</p> </li> <li> <p>For custom executors: Make sure you:</p> </li> <li>Used <code>@executor_register()</code> decorator</li> <li>Imported the executor class before using it</li> <li>The <code>schemas.method</code> field matches the executor <code>id</code></li> <li>For CLI: Import executor in a Python script before running CLI command</li> <li>For API: Import executor before starting the API server</li> </ol> <p>Example (Custom Executor): <pre><code># my_script.py\nfrom my_executor import GreetingTask  # Import to register\n\n# Now you can use CLI or API\n</code></pre></p>"},{"location":"getting-started/quick-start/#problem-task-stays-in-pending-status","title":"Problem: Task Stays in \"pending\" Status","text":"<p>Possible causes: - Dependencies not satisfied (check if dependency tasks are completed) - Executor not found (see above) - Task not executed (make sure you called the CLI command or API endpoint)</p> <p>Solution - Check Task Status:</p> <p>Via CLI: <pre><code># Check status of a specific task\napflow tasks status &lt;task_id&gt;\n\n# List all running tasks\napflow tasks list\n\n# Get full task details\napflow tasks get &lt;task_id&gt;\n</code></pre></p> <p>Via API: <pre><code># Check task status\ncurl http://localhost:8000/api/tasks/&lt;task_id&gt;/status\n\n# Get task details\ncurl http://localhost:8000/api/tasks/&lt;task_id&gt;\n</code></pre></p> <p>Check for errors: - Look at the <code>error</code> field in the task result - Check CLI output or API response for error messages - Review task logs if available</p>"},{"location":"getting-started/quick-start/#problem-cli-command-not-found","title":"Problem: CLI Command Not Found","text":"<p>Error: <code>command not found: apflow</code> or <code>command not found: apflow</code></p> <p>Solution: <pre><code># Install with CLI support\npip install apflow[cli]\n\n# Or install everything\npip install apflow[all]\n\n# Verify installation\napflow --version\n# Or\napflow --version\n</code></pre></p>"},{"location":"getting-started/quick-start/#problem-api-server-wont-start","title":"Problem: API Server Won't Start","text":"<p>Error: Port already in use or server won't start</p> <p>Solutions: <pre><code># Use a different port\napflow serve --port 8080\n</code></pre></p>"},{"location":"getting-started/quick-start/#check-if-port-is-in-use","title":"Check if port is in use","text":"<p>lsof -i :8000</p>"},{"location":"getting-started/quick-start/#kill-process-using-the-port-if-needed","title":"Kill process using the port (if needed)","text":"<p>kill -9  <pre><code>### Problem: Database Error\n\n**Error:** Database connection issues\n\n**Solution:**\n- **DuckDB (default)**: No setup needed! It just works. Database file is created automatically at `~/.aipartnerup/data/apflow.duckdb`\n- **PostgreSQL**: Set environment variable:\n  ```bash\n  export DATABASE_URL=\"postgresql+asyncpg://user:password@localhost/dbname\"\n  ```\n- **Check database connection:**\n  ```bash\n  # For DuckDB, check if file exists\n  ls ~/.aipartnerup/data/apflow.duckdb\n\n  # For PostgreSQL, test connection\n  psql $DATABASE_URL -c \"SELECT 1;\"\n  ```\n\n### Problem: Custom Executor Not Found When Using CLI\n\n**Error:** Custom executor not registered when running CLI command\n\n**Solution:**\nCustom executors must be imported before they can be used. For CLI usage:\n\n**Option 1: Import in a wrapper script**\n```python\n# run_my_task.py\nfrom my_executor import MyExecutor  # Import to register\nimport subprocess\n\nsubprocess.run([\"apflow\", \"run\", \"flow\", \"my_executor\", \"--inputs\", '{\"key\": \"value\"}'])\n</code></pre> <p>Option 2: Use Python to import, then CLI <pre><code># Import executor first\npython -c \"from my_executor import MyExecutor\"\n\n# Then use CLI (in same shell session)\napflow run flow my_executor --inputs '{\"key\": \"value\"}'\n</code></pre></p> <p>Option 3: Use task array with Python wrapper Create a Python script that imports the executor and then calls the CLI with task array format.</p>"},{"location":"getting-started/quick-start/#problem-import-error","title":"Problem: Import Error","text":"<p>Error: <code>ModuleNotFoundError: No module named 'apflow'</code></p> <p>Solution: <pre><code>pip install apflow\n\n# Or with specific features\npip install apflow[cli]\npip install apflow[all]\n</code></pre></p>"},{"location":"getting-started/quick-start/#try-it-yourself_1","title":"Try It Yourself","text":""},{"location":"getting-started/quick-start/#exercise-1-multiple-system-checks","title":"Exercise 1: Multiple System Checks","text":"<p>Create tasks to check CPU, memory, and disk using CLI or API, then view the aggregated results.</p> <p>Hint: Use the <code>system_info_executor</code> with different <code>resource</code> values, or create a task array with multiple tasks.</p>"},{"location":"getting-started/quick-start/#exercise-2-custom-greeting-in-multiple-languages","title":"Exercise 2: Custom Greeting in Multiple Languages","text":"<p>Use the <code>GreetingTask</code> example to create greetings in different languages via CLI or API, then combine them.</p> <p>Hint: Create multiple tasks with different <code>language</code> inputs, or create a task array with sequential dependencies.</p>"},{"location":"getting-started/quick-start/#exercise-3-sequential-processing","title":"Exercise 3: Sequential Processing","text":"<p>Create a pipeline: fetch data \u2192 process data \u2192 save results (each step depends on the previous).</p> <p>Hint: Use task dependencies to ensure proper execution order. You can use built-in executors or create custom ones for each step.</p>"},{"location":"getting-started/quick-start/#getting-help","title":"Getting Help","text":"<ul> <li>Stuck? Check the FAQ</li> <li>Need examples? See Basic Examples</li> <li>Want to understand concepts? Read Core Concepts</li> <li>Found a bug? Report it on GitHub</li> <li>Have questions? Ask on Discussions</li> </ul> <p>Ready for more? \u2192 Learn Core Concepts \u2192 or Try the First Steps Tutorial \u2192</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/","title":"Tutorial 1: First Steps","text":"<p>This is a complete beginner-friendly tutorial. If you're new to apflow, start here!</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this tutorial, you'll be able to: - \u2705 Install and set up apflow - \u2705 Create and execute your first task - \u2705 Understand the basic workflow - \u2705 Use built-in executors - \u2705 Check task status and results</p> <p>Time required: 15-20 minutes</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ installed</li> <li>Basic Python knowledge (variables, functions, async/await basics)</li> <li>A text editor or IDE</li> </ul>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#part-1-installation-and-setup","title":"Part 1: Installation and Setup","text":""},{"location":"getting-started/tutorials/tutorial-01-first-steps/#step-1-install-apflow","title":"Step 1: Install apflow","text":"<pre><code>pip install apflow\n</code></pre> <p>That's it! No database setup needed - DuckDB works out of the box.</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#step-2-verify-installation","title":"Step 2: Verify Installation","text":"<pre><code>import apflow\nprint(apflow.__version__)\n</code></pre> <p>If you see a version number, you're good to go!</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#part-2-your-very-first-task","title":"Part 2: Your Very First Task","text":"<p>Let's create the simplest possible task to see apflow in action.</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#the-goal","title":"The Goal","text":"<p>Get CPU information from your system using the built-in <code>system_info_executor</code>.</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#complete-code","title":"Complete Code","text":"<p>Create a file <code>tutorial_01.py</code>:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    print(\"\ud83d\ude80 Starting apflow tutorial...\")\n\n    # Step 1: Create database session\n    # DuckDB is used by default - no configuration needed!\n    print(\"\ud83d\udce6 Creating database session...\")\n    db = create_session()\n\n    # Step 2: Create TaskManager\n    # This is the orchestrator that manages everything\n    print(\"\ud83c\udfaf Creating TaskManager...\")\n    task_manager = TaskManager(db)\n\n    # Step 3: Create a task\n    # We're using the built-in system_info_executor\n    # It's already available - no custom code needed!\n    print(\"\ud83d\udcdd Creating task...\")\n    task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",  # Built-in executor ID\n        user_id=\"tutorial_user\",      # Your identifier\n        priority=2,                   # Normal priority\n        inputs={\"resource\": \"cpu\"}   # Get CPU information\n    )\n\n    print(f\"\u2705 Task created with ID: {task.id}\")\n    print(f\"\ud83d\udcca Task status: {task.status}\")\n\n    # Step 4: Build task tree\n    # Even a single task needs to be in a tree structure\n    print(\"\ud83c\udf33 Building task tree...\")\n    task_tree = TaskTreeNode(task)\n\n    # Step 5: Execute the task\n    # TaskManager handles everything:\n    # - Finds the executor\n    # - Executes the task\n    # - Updates status\n    # - Saves results\n    print(\"\u26a1 Executing task...\")\n    await task_manager.distribute_task_tree(task_tree)\n\n    # Step 6: Get the result\n    # Reload the task to see updated status and result\n    print(\"\ud83d\udce5 Fetching results...\")\n    completed_task = await task_manager.task_repository.get_task_by_id(task.id)\n\n    print(f\"\\n\u2705 Task completed!\")\n    print(f\"\ud83d\udcca Final status: {completed_task.status}\")\n    print(f\"\ud83d\udcbe Result: {completed_task.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#run-it","title":"Run It","text":"<pre><code>python tutorial_01.py\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#expected-output","title":"Expected Output","text":"<pre><code>\ud83d\ude80 Starting apflow tutorial...\n\ud83d\udce6 Creating database session...\n\ud83c\udfaf Creating TaskManager...\n\ud83d\udcdd Creating task...\n\u2705 Task created with ID: &lt;some-uuid&gt;\n\ud83d\udcca Task status: pending\n\ud83c\udf33 Building task tree...\n\u26a1 Executing task...\n\ud83d\udce5 Fetching results...\n\n\u2705 Task completed!\n\ud83d\udcca Final status: completed\n\ud83d\udcbe Result: {'system': 'Darwin', 'cores': 8, 'cpu_count': 8, ...}\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#what-happened","title":"What Happened?","text":"<ol> <li>Created a task: We told apflow \"get CPU info\"</li> <li>TaskManager found the executor: It automatically found <code>system_info_executor</code></li> <li>Task executed: The executor ran and collected CPU information</li> <li>Result saved: The result was stored in the database</li> </ol> <p>Congratulations! You just executed your first task! \ud83c\udf89</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#part-3-understanding-the-workflow","title":"Part 3: Understanding the Workflow","text":"<p>Let's break down what happened step by step:</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#the-workflow","title":"The Workflow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. Create Task                         \u2502\n\u2502     - Define what you want to do        \u2502\n\u2502     - Specify inputs                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. Build Task Tree                     \u2502\n\u2502     - Organize tasks in a tree          \u2502\n\u2502     - Even single tasks need a tree     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. Execute with TaskManager            \u2502\n\u2502     - TaskManager finds the executor    \u2502\n\u2502     - Executor runs the task            \u2502\n\u2502     - Status updates automatically      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2502\n              \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  4. Get Results                         \u2502\n\u2502     - Reload task from database          \u2502\n\u2502     - Check status and result           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#key-components-explained","title":"Key Components Explained","text":"<p>TaskManager: The orchestrator - Manages task execution - Finds executors - Tracks status - Handles errors</p> <p>Task: What you want to do - Has a <code>name</code> (executor ID) - Has <code>inputs</code> (parameters) - Has a <code>status</code> (pending \u2192 in_progress \u2192 completed)</p> <p>Executor: The code that does the work - <code>system_info_executor</code> is built-in - You can create custom executors too!</p> <p>Task Tree: Organization structure - Even single tasks need a tree - Multiple tasks form a hierarchy</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#part-4-experimenting","title":"Part 4: Experimenting","text":"<p>Let's try different things to understand better:</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#experiment-1-get-different-information","title":"Experiment 1: Get Different Information","text":"<p>Modify the <code>inputs</code> to get different system information:</p> <pre><code># Get memory information\ninputs={\"resource\": \"memory\"}\n\n# Get disk information\ninputs={\"resource\": \"disk\"}\n\n# Get all system resources\ninputs={\"resource\": \"all\"}\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#experiment-2-check-task-status","title":"Experiment 2: Check Task Status","text":"<p>Add status checking during execution:</p> <pre><code># After creating task\nprint(f\"Initial status: {task.status}\")  # Should be \"pending\"\n\n# After execution\ncompleted_task = await task_manager.task_repository.get_task_by_id(task.id)\nprint(f\"Final status: {completed_task.status}\")  # Should be \"completed\"\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#experiment-3-handle-errors","title":"Experiment 3: Handle Errors","text":"<p>Try with invalid input:</p> <pre><code># This might fail\ninputs={\"resource\": \"invalid_resource\"}\n</code></pre> <p>Check the error:</p> <pre><code>failed_task = await task_manager.task_repository.get_task_by_id(task.id)\nif failed_task.status == \"failed\":\n    print(f\"Error: {failed_task.error}\")\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#part-5-multiple-tasks","title":"Part 5: Multiple Tasks","text":"<p>Now let's create multiple tasks to see how they work together:</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#example-get-all-system-resources","title":"Example: Get All System Resources","text":"<p>Create a file <code>tutorial_01_multiple.py</code>:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create multiple tasks\n    cpu_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    memory_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    disk_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"disk\"}\n    )\n\n    # Build task tree (all tasks are children of a root)\n    # For now, we'll use cpu_task as root\n    root = TaskTreeNode(cpu_task)\n    root.add_child(TaskTreeNode(memory_task))\n    root.add_child(TaskTreeNode(disk_task))\n\n    # Execute all tasks\n    # Since they have no dependencies, they can run in parallel!\n    await task_manager.distribute_task_tree(root)\n\n    # Get all results\n    cpu_result = await task_manager.task_repository.get_task_by_id(cpu_task.id)\n    memory_result = await task_manager.task_repository.get_task_by_id(memory_task.id)\n    disk_result = await task_manager.task_repository.get_task_by_id(disk_task.id)\n\n    print(f\"\u2705 CPU: {cpu_result.status}\")\n    print(f\"\u2705 Memory: {memory_result.status}\")\n    print(f\"\u2705 Disk: {disk_result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#understanding-parallel-execution","title":"Understanding Parallel Execution","text":"<p>In this example: - All three tasks have no dependencies - They can run in parallel (at the same time) - TaskManager handles this automatically!</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#part-6-whats-next","title":"Part 6: What's Next?","text":"<p>You've learned the basics! Here's what to explore:</p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#immediate-next-steps","title":"Immediate Next Steps","text":"<ol> <li>Core Concepts - Understand the concepts deeply</li> <li>Quick Start Guide - More examples and patterns</li> <li>Basic Examples - Copy-paste ready examples</li> </ol>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#learn-more","title":"Learn More","text":"<ul> <li>Task Trees Tutorial - Build complex task hierarchies</li> <li>Dependencies Tutorial - Control execution order</li> <li>Custom Tasks Guide - Create your own executors</li> </ul>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#common-questions","title":"Common Questions","text":"<p>Q: Why do I need a task tree for a single task? A: The framework is designed for complex workflows. Even single tasks use the tree structure for consistency.</p> <p>Q: Can I skip creating a task tree? A: No, but you can use <code>TaskCreator</code> to build trees from arrays automatically (we'll cover this in later tutorials).</p> <p>Q: What if my task fails? A: Check <code>task.status</code> - it will be \"failed\". Check <code>task.error</code> for the error message.</p> <p>Q: How do I know which executors are available? A: Built-in executors are automatically registered. Check the API Reference or use the registry: <pre><code>from apflow.core.extensions import get_registry\nregistry = get_registry()\nexecutors = registry.list_by_category(ExtensionCategory.EXECUTOR)\nprint(executors)\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-01-first-steps/#summary","title":"Summary","text":"<p>In this tutorial, you learned: - \u2705 How to install apflow - \u2705 How to create and execute tasks - \u2705 How to use built-in executors - \u2705 How to check task status and results - \u2705 How to create multiple tasks</p> <p>Next: Tutorial 2: Task Trees \u2192 or Core Concepts \u2192</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/","title":"Tutorial 2: Building Task Trees","text":"<p>Learn how to build complex task trees with multiple tasks, dependencies, and priorities.</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this tutorial, you'll be able to: - \u2705 Build task trees with multiple tasks - \u2705 Understand parent-child relationships - \u2705 Create hierarchical task structures - \u2705 Organize complex workflows</p> <p>Time required: 20-30 minutes</p> <p>Prerequisites:  - Completed Tutorial 1: First Steps - Understand basic task execution</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-1-understanding-task-trees","title":"Part 1: Understanding Task Trees","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#what-is-a-task-tree","title":"What is a Task Tree?","text":"<p>A task tree is a hierarchical structure that organizes tasks. Think of it like a family tree: - Root task: The top-level task (like a family ancestor) - Child tasks: Tasks that belong to a parent (like children) - Grandchild tasks: Tasks that belong to a child (like grandchildren)</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#visual-representation","title":"Visual Representation","text":"<pre><code>Root Task\n\u2502\n\u251c\u2500\u2500 Child Task 1\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 Grandchild Task 1.1\n\u2502\n\u251c\u2500\u2500 Child Task 2\n\u2502\n\u2514\u2500\u2500 Child Task 3\n    \u2502\n    \u251c\u2500\u2500 Grandchild Task 3.1\n    \u2514\u2500\u2500 Grandchild Task 3.2\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#key-concept-parent-child-is-organizational","title":"Key Concept: Parent-Child is Organizational","text":"<p>Important: Parent-child relationships (<code>parent_id</code>) are for organization only. They don't control execution order!</p> <ul> <li>Parent-Child: Like folders - helps organize tasks</li> <li>Dependencies: Control when tasks run (we'll cover this in Tutorial 3)</li> </ul>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-2-building-your-first-task-tree","title":"Part 2: Building Your First Task Tree","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#example-simple-two-level-tree","title":"Example: Simple Two-Level Tree","text":"<p>Let's create a simple tree with a root and one child:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Step 1: Create root task\n    root_task = await task_manager.task_repository.create_task(\n        name=\"root_task\",\n        user_id=\"tutorial_user\",\n        priority=1\n    )\n\n    # Step 2: Create child task\n    child_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=root_task.id,  # This makes it a child\n        priority=2,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Step 3: Build task tree\n    root = TaskTreeNode(root_task)\n    root.add_child(TaskTreeNode(child_task))\n\n    # Step 4: Execute\n    await task_manager.distribute_task_tree(root)\n\n    # Step 5: Check results\n    child_result = await task_manager.task_repository.get_task_by_id(child_task.id)\n    print(f\"Child task status: {child_result.status}\")\n    print(f\"Child task result: {child_result.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#understanding-the-code","title":"Understanding the Code","text":"<ol> <li>Root task: The top-level task (organizational parent)</li> <li>Child task: Has <code>parent_id=root_task.id</code> (organizational child)</li> <li>TaskTreeNode: Wraps tasks in tree structure</li> <li>add_child(): Adds a child node to the tree</li> </ol> <p>Visual Structure: <pre><code>Root Task\n\u2502\n\u2514\u2500\u2500 Child Task (gets CPU info)\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-3-multiple-children","title":"Part 3: Multiple Children","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#example-root-with-multiple-children","title":"Example: Root with Multiple Children","text":"<p>Create a root task with multiple children:</p> <pre><code>async def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create root\n    root_task = await task_manager.task_repository.create_task(\n        name=\"root_task\",\n        user_id=\"tutorial_user\",\n        priority=1\n    )\n\n    # Create multiple children\n    child1 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=root_task.id,  # Child of root\n        priority=2,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    child2 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=root_task.id,  # Also child of root\n        priority=2,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    child3 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=root_task.id,  # Also child of root\n        priority=2,\n        inputs={\"resource\": \"disk\"}\n    )\n\n    # Build tree\n    root = TaskTreeNode(root_task)\n    root.add_child(TaskTreeNode(child1))\n    root.add_child(TaskTreeNode(child2))\n    root.add_child(TaskTreeNode(child3))\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\n    # Check all results\n    for child in [child1, child2, child3]:\n        result = await task_manager.task_repository.get_task_by_id(child.id)\n        print(f\"{child.id}: {result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Visual Structure: <pre><code>Root Task\n\u2502\n\u251c\u2500\u2500 Child 1 (CPU)\n\u251c\u2500\u2500 Child 2 (Memory)\n\u2514\u2500\u2500 Child 3 (Disk)\n</code></pre></p> <p>Note: All three children can run in parallel (no dependencies between them)!</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-4-multi-level-trees","title":"Part 4: Multi-Level Trees","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#example-three-level-tree","title":"Example: Three-Level Tree","text":"<p>Create a tree with root, children, and grandchildren:</p> <pre><code>async def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Level 1: Root\n    root_task = await task_manager.task_repository.create_task(\n        name=\"root_task\",\n        user_id=\"tutorial_user\",\n        priority=1\n    )\n\n    # Level 2: Children\n    child1 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=root_task.id,\n        priority=2,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    child2 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=root_task.id,\n        priority=2,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    # Level 3: Grandchildren\n    grandchild1 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=child1.id,  # Child of child1\n        priority=3,\n        inputs={\"resource\": \"all\"}\n    )\n\n    grandchild2 = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=child2.id,  # Child of child2\n        priority=3,\n        inputs={\"resource\": \"all\"}\n    )\n\n    # Build tree\n    root = TaskTreeNode(root_task)\n    child1_node = TaskTreeNode(child1)\n    child2_node = TaskTreeNode(child2)\n\n    child1_node.add_child(TaskTreeNode(grandchild1))\n    child2_node.add_child(TaskTreeNode(grandchild2))\n\n    root.add_child(child1_node)\n    root.add_child(child2_node)\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\n    # Check results\n    print(\"Tree execution completed!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Visual Structure: <pre><code>Root Task\n\u2502\n\u251c\u2500\u2500 Child 1 (CPU)\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 Grandchild 1 (All resources)\n\u2502\n\u2514\u2500\u2500 Child 2 (Memory)\n    \u2502\n    \u2514\u2500\u2500 Grandchild 2 (All resources)\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-5-building-trees-programmatically","title":"Part 5: Building Trees Programmatically","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#helper-function","title":"Helper Function","text":"<p>Create a helper function to build trees more easily:</p> <pre><code>def build_tree_from_tasks(task_manager, tasks_config):\n    \"\"\"\n    Build a task tree from configuration\n\n    tasks_config: List of dicts with task info\n    \"\"\"\n    # Create all tasks first\n    created_tasks = {}\n    for config in tasks_config:\n        task = await task_manager.task_repository.create_task(\n            name=config[\"name\"],\n            user_id=config[\"user_id\"],\n            parent_id=config.get(\"parent_id\"),\n            priority=config.get(\"priority\", 2),\n            inputs=config.get(\"inputs\", {})\n        )\n        created_tasks[config[\"id\"]] = task\n\n    # Build tree\n    root_id = None\n    nodes = {}\n\n    # Find root (task with no parent)\n    for task_id, task in created_tasks.items():\n        if task.parent_id is None:\n            root_id = task_id\n            break\n\n    # Create nodes\n    for task_id, task in created_tasks.items():\n        nodes[task_id] = TaskTreeNode(task)\n\n    # Add children\n    for task_id, task in created_tasks.items():\n        if task.parent_id:\n            parent_node = nodes[task.parent_id]\n            parent_node.add_child(nodes[task_id])\n\n    return nodes[root_id]\n\n# Use it\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    tasks_config = [\n        {\n            \"id\": \"root\",\n            \"name\": \"root_task\",\n            \"user_id\": \"tutorial_user\",\n            \"priority\": 1\n        },\n        {\n            \"id\": \"child1\",\n            \"name\": \"system_info_executor\",\n            \"user_id\": \"tutorial_user\",\n            \"parent_id\": \"root\",\n            \"priority\": 2,\n            \"inputs\": {\"resource\": \"cpu\"}\n        },\n        {\n            \"id\": \"child2\",\n            \"name\": \"system_info_executor\",\n            \"user_id\": \"tutorial_user\",\n            \"parent_id\": \"root\",\n            \"priority\": 2,\n            \"inputs\": {\"resource\": \"memory\"}\n        }\n    ]\n\n    # Build tree\n    root = await build_tree_from_tasks(task_manager, tasks_config)\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-6-common-tree-patterns","title":"Part 6: Common Tree Patterns","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#pattern-1-flat-tree-all-children-of-root","title":"Pattern 1: Flat Tree (All Children of Root)","text":"<pre><code># All tasks are direct children of root\nroot = TaskTreeNode(root_task)\nroot.add_child(TaskTreeNode(child1))\nroot.add_child(TaskTreeNode(child2))\nroot.add_child(TaskTreeNode(child3))\n</code></pre> <p>Use Case: Parallel processing of independent tasks</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#pattern-2-deep-tree-many-levels","title":"Pattern 2: Deep Tree (Many Levels)","text":"<pre><code># Tree with many levels\nroot = TaskTreeNode(root_task)\nlevel1 = TaskTreeNode(child1)\nlevel2 = TaskTreeNode(grandchild1)\nlevel3 = TaskTreeNode(great_grandchild1)\n\nlevel2.add_child(level3)\nlevel1.add_child(level2)\nroot.add_child(level1)\n</code></pre> <p>Use Case: Hierarchical data processing</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#pattern-3-balanced-tree","title":"Pattern 3: Balanced Tree","text":"<pre><code># Tree where each node has similar number of children\nroot = TaskTreeNode(root_task)\nfor i in range(3):\n    child = TaskTreeNode(children[i])\n    for j in range(2):\n        grandchild = TaskTreeNode(grandchildren[i*2 + j])\n        child.add_child(grandchild)\n    root.add_child(child)\n</code></pre> <p>Use Case: Balanced workload distribution</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-7-tree-operations","title":"Part 7: Tree Operations","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#calculate-progress","title":"Calculate Progress","text":"<p>Get overall progress of the tree:</p> <pre><code># After execution\nprogress = root.calculate_progress()\nprint(f\"Overall progress: {progress * 100}%\")\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#calculate-status","title":"Calculate Status","text":"<p>Get overall status of the tree:</p> <pre><code>status = root.calculate_status()\nprint(f\"Overall status: {status}\")\n# Returns: \"completed\", \"failed\", \"in_progress\", or \"pending\"\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#traverse-tree","title":"Traverse Tree","text":"<p>Visit all nodes in the tree:</p> <pre><code>def traverse_tree(node, level=0):\n    \"\"\"Traverse tree and print structure\"\"\"\n    indent = \"  \" * level\n    print(f\"{indent}- {node.task.name} (status: {node.task.status})\")\n\n    for child in node.children:\n        traverse_tree(child, level + 1)\n\n# Use it\ntraverse_tree(root)\n</code></pre> <p>Output: <pre><code>- root_task (status: completed)\n  - child1 (status: completed)\n    - grandchild1 (status: completed)\n  - child2 (status: completed)\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-8-best-practices","title":"Part 8: Best Practices","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#1-keep-trees-manageable","title":"1. Keep Trees Manageable","text":"<p>Good: 3-5 levels deep, 10-20 tasks Bad: 10+ levels deep, 100+ tasks</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#2-use-meaningful-names","title":"2. Use Meaningful Names","text":"<pre><code># Good\nname=\"fetch_user_data\"\nname=\"process_payment\"\nname=\"send_notification\"\n\n# Bad\nname=\"task1\"\nname=\"task2\"\nname=\"x\"\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#3-organize-by-function","title":"3. Organize by Function","text":"<p>Group related tasks under the same parent:</p> <pre><code># Data collection group\ndata_collection_root = create_task(name=\"data_collection_root\")\nfetch_api = create_task(parent_id=data_collection_root.id, ...)\nfetch_db = create_task(parent_id=data_collection_root.id, ...)\n\n# Processing group\nprocessing_root = create_task(name=\"processing_root\")\nprocess_data = create_task(parent_id=processing_root.id, ...)\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#4-document-complex-trees","title":"4. Document Complex Trees","text":"<p>For complex trees, add comments:</p> <pre><code># Tree structure:\n# Root\n# \u251c\u2500\u2500 Data Collection\n# \u2502   \u251c\u2500\u2500 Fetch from API\n# \u2502   \u2514\u2500\u2500 Fetch from DB\n# \u2514\u2500\u2500 Processing\n#     \u251c\u2500\u2500 Process Data\n#     \u2514\u2500\u2500 Save Results\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-9-common-mistakes","title":"Part 9: Common Mistakes","text":""},{"location":"getting-started/tutorials/tutorial-02-task-trees/#mistake-1-forgetting-to-add-children","title":"Mistake 1: Forgetting to Add Children","text":"<pre><code># Wrong: Created tasks but didn't add to tree\nroot = TaskTreeNode(root_task)\nchild = TaskTreeNode(child_task)\n# Forgot: root.add_child(child)\nawait task_manager.distribute_task_tree(root)  # Child won't execute!\n</code></pre> <p>Fix: Always add children to the tree</p>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#mistake-2-confusing-parent-child-with-dependencies","title":"Mistake 2: Confusing Parent-Child with Dependencies","text":"<pre><code># Wrong: Thinking parent-child controls execution order\nchild = create_task(parent_id=parent.id)  # This doesn't make child wait for parent!\n\n# Right: Use dependencies for execution order\nchild = create_task(\n    parent_id=parent.id,  # Organizational\n    dependencies=[{\"id\": parent.id, \"required\": True}]  # Execution order\n)\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#mistake-3-creating-orphan-tasks","title":"Mistake 3: Creating Orphan Tasks","text":"<pre><code># Wrong: Task with parent_id that doesn't exist\nchild = create_task(parent_id=\"nonexistent_id\")  # Will cause errors!\n\n# Right: Create parent first\nparent = create_task(...)\nchild = create_task(parent_id=parent.id)\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#part-10-next-steps","title":"Part 10: Next Steps","text":"<p>You've learned how to build task trees! Next:</p> <ol> <li>Tutorial 3: Dependencies - Learn how dependencies control execution order</li> <li>Task Orchestration Guide - Deep dive into orchestration</li> <li>Best Practices - Learn from experts</li> </ol>"},{"location":"getting-started/tutorials/tutorial-02-task-trees/#summary","title":"Summary","text":"<p>In this tutorial, you learned: - \u2705 What task trees are and why they're useful - \u2705 How to build simple and complex trees - \u2705 How parent-child relationships work (organizational) - \u2705 Common tree patterns and best practices - \u2705 How to traverse and inspect trees</p> <p>Key Takeaway: Parent-child relationships organize tasks, but dependencies control execution order!</p> <p>Next: Tutorial 3: Dependencies \u2192</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/","title":"Tutorial 3: Working with Dependencies","text":"<p>Master dependencies - the mechanism that controls when tasks execute. This is one of the most important concepts in apflow!</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this tutorial, you'll be able to: - \u2705 Understand how dependencies control execution order - \u2705 Create sequential task pipelines - \u2705 Handle multiple dependencies - \u2705 Use optional dependencies for fallbacks - \u2705 Build complex dependency graphs</p> <p>Time required: 25-35 minutes</p> <p>Prerequisites:  - Completed Tutorial 1: First Steps - Completed Tutorial 2: Task Trees - Understand task trees and parent-child relationships</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-1-understanding-dependencies","title":"Part 1: Understanding Dependencies","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#what-are-dependencies","title":"What are Dependencies?","text":"<p>Dependencies control when tasks execute. A task with dependencies will wait for its dependencies to complete before executing.</p> <p>Key Point: Dependencies are different from parent-child relationships! - Parent-Child (<code>parent_id</code>): Organizational (like folders) - Dependencies (<code>dependencies</code>): Execution control (when tasks run)</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#visual-example","title":"Visual Example","text":"<pre><code>Task A (no dependencies) \u2192 runs first\n    \u2193\nTask B (depends on A) \u2192 waits for A, then runs\n    \u2193\nTask C (depends on B) \u2192 waits for B, then runs\n</code></pre> <p>Execution Order: A \u2192 B \u2192 C (automatic!)</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-2-your-first-dependency","title":"Part 2: Your First Dependency","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#example-sequential-tasks","title":"Example: Sequential Tasks","text":"<p>Create two tasks where the second depends on the first:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Task 1: Get CPU info (no dependencies)\n    cpu_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Task 2: Get memory info (depends on CPU task)\n    memory_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=cpu_task.id,  # Organizational\n        dependencies=[{\"id\": cpu_task.id, \"required\": True}],  # Execution: waits for CPU\n        priority=2,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    # Build tree\n    root = TaskTreeNode(cpu_task)\n    root.add_child(TaskTreeNode(memory_task))\n\n    # Execute\n    # TaskManager will:\n    # 1. Execute cpu_task first (no dependencies)\n    # 2. Wait for cpu_task to complete\n    # 3. Then execute memory_task (dependency satisfied)\n    await task_manager.distribute_task_tree(root)\n\n    # Check results\n    cpu_result = await task_manager.task_repository.get_task_by_id(cpu_task.id)\n    memory_result = await task_manager.task_repository.get_task_by_id(memory_task.id)\n\n    print(f\"CPU task: {cpu_result.status}\")\n    print(f\"Memory task: {memory_result.status}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#understanding-the-code","title":"Understanding the Code","text":"<p>Key Components: 1. <code>dependencies</code>: List of dependency dictionaries 2. <code>{\"id\": cpu_task.id, \"required\": True}</code>: Dependency specification    - <code>id</code>: The task ID to wait for    - <code>required</code>: Whether the dependency must succeed (True) or can fail (False)</p> <p>Execution Flow: <pre><code>CPU Task (runs first, no dependencies)\n    \u2193 (waits for completion)\nMemory Task (runs after CPU completes)\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-3-sequential-pipeline","title":"Part 3: Sequential Pipeline","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#example-three-step-pipeline","title":"Example: Three-Step Pipeline","text":"<p>Create a pipeline: Fetch \u2192 Process \u2192 Save</p> <pre><code>async def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Step 1: Fetch data\n    fetch_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Step 2: Process data (depends on fetch)\n    process_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=fetch_task.id,\n        dependencies=[{\"id\": fetch_task.id, \"required\": True}],\n        priority=2,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    # Step 3: Save results (depends on process)\n    save_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=fetch_task.id,\n        dependencies=[{\"id\": process_task.id, \"required\": True}],\n        priority=3,\n        inputs={\"resource\": \"disk\"}\n    )\n\n    # Build pipeline\n    root = TaskTreeNode(fetch_task)\n    root.add_child(TaskTreeNode(process_task))\n    root.add_child(TaskTreeNode(save_task))\n\n    # Execute\n    # Order: Fetch \u2192 Process \u2192 Save (automatic!)\n    await task_manager.distribute_task_tree(root)\n\n    print(\"Pipeline completed!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Execution Flow: <pre><code>Fetch \u2192 Process \u2192 Save\n</code></pre></p> <p>Key Point: Each task waits for the previous one to complete!</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-4-multiple-dependencies","title":"Part 4: Multiple Dependencies","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#example-task-depends-on-multiple-tasks","title":"Example: Task Depends on Multiple Tasks","text":"<p>Create a task that depends on multiple other tasks:</p> <pre><code>async def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Task 1: Get CPU info\n    cpu_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Task 2: Get memory info\n    memory_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    # Task 3: Get disk info\n    disk_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"disk\"}\n    )\n\n    # Task 4: Aggregate all (depends on ALL three)\n    aggregate_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=cpu_task.id,\n        dependencies=[\n            {\"id\": cpu_task.id, \"required\": True},\n            {\"id\": memory_task.id, \"required\": True},\n            {\"id\": disk_task.id, \"required\": True}\n        ],\n        priority=2,\n        inputs={\"resource\": \"all\"}\n    )\n\n    # Build tree\n    root = TaskTreeNode(cpu_task)\n    root.add_child(TaskTreeNode(memory_task))\n    root.add_child(TaskTreeNode(disk_task))\n    root.add_child(TaskTreeNode(aggregate_task))\n\n    # Execute\n    # Tasks 1, 2, 3 run in parallel (no dependencies)\n    # Task 4 waits for ALL of them to complete\n    await task_manager.distribute_task_tree(root)\n\n    print(\"All tasks completed!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Execution Flow: <pre><code>CPU Task \u2500\u2500\u2510\n           \u2502\nMemory Task\u251c\u2500\u2500\u2192 Aggregate Task (waits for all three)\n           \u2502\nDisk Task \u2500\u2500\u2518\n</code></pre></p> <p>Key Point: Task 4 only executes after all three dependencies complete!</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-5-optional-dependencies","title":"Part 5: Optional Dependencies","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#example-fallback-pattern","title":"Example: Fallback Pattern","text":"<p>Use optional dependencies for fallback scenarios:</p> <pre><code>async def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Primary task\n    primary_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Fallback task (runs even if primary fails)\n    fallback_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=primary_task.id,\n        dependencies=[{\"id\": primary_task.id, \"required\": False}],  # Optional!\n        priority=2,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    # Final task (works with either primary or fallback)\n    final_task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=primary_task.id,\n        dependencies=[\n            {\"id\": primary_task.id, \"required\": False},  # Optional\n            {\"id\": fallback_task.id, \"required\": False}  # Optional\n        ],\n        priority=3,\n        inputs={\"resource\": \"disk\"}\n    )\n\n    # Build tree\n    root = TaskTreeNode(primary_task)\n    root.add_child(TaskTreeNode(fallback_task))\n    root.add_child(TaskTreeNode(final_task))\n\n    # Execute\n    # Even if primary_task fails, fallback_task and final_task will still run\n    await task_manager.distribute_task_tree(root)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Key Point: <code>\"required\": False</code> means the task can execute even if the dependency fails!</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-6-complex-dependency-graphs","title":"Part 6: Complex Dependency Graphs","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#example-complex-workflow","title":"Example: Complex Workflow","text":"<p>Create a complex workflow with multiple dependency paths:</p> <pre><code>async def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create all tasks\n    tasks = {}\n\n    # Level 1: Independent tasks\n    tasks[\"fetch1\"] = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    tasks[\"fetch2\"] = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        priority=1,\n        inputs={\"resource\": \"memory\"}\n    )\n\n    # Level 2: Tasks that depend on Level 1\n    tasks[\"process1\"] = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=tasks[\"fetch1\"].id,\n        dependencies=[{\"id\": tasks[\"fetch1\"].id, \"required\": True}],\n        priority=2,\n        inputs={\"resource\": \"disk\"}\n    )\n\n    tasks[\"process2\"] = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=tasks[\"fetch2\"].id,\n        dependencies=[{\"id\": tasks[\"fetch2\"].id, \"required\": True}],\n        priority=2,\n        inputs={\"resource\": \"all\"}\n    )\n\n    # Level 3: Final task depends on both Level 2 tasks\n    tasks[\"final\"] = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",\n        user_id=\"tutorial_user\",\n        parent_id=tasks[\"fetch1\"].id,\n        dependencies=[\n            {\"id\": tasks[\"process1\"].id, \"required\": True},\n            {\"id\": tasks[\"process2\"].id, \"required\": True}\n        ],\n        priority=3,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Build tree\n    root = TaskTreeNode(tasks[\"fetch1\"])\n    root.add_child(TaskTreeNode(tasks[\"fetch2\"]))\n    root.add_child(TaskTreeNode(tasks[\"process1\"]))\n    root.add_child(TaskTreeNode(tasks[\"process2\"]))\n    root.add_child(TaskTreeNode(tasks[\"final\"]))\n\n    # Execute\n    await task_manager.distribute_task_tree(root)\n\n    print(\"Complex workflow completed!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Execution Flow: <pre><code>Fetch1 \u2500\u2500\u2192 Process1 \u2500\u2500\u2510\n                       \u251c\u2500\u2500\u2192 Final\nFetch2 \u2500\u2500\u2192 Process2 \u2500\u2500\u2518\n</code></pre></p> <p>Execution Order: 1. Fetch1 and Fetch2 run in parallel 2. Process1 waits for Fetch1, Process2 waits for Fetch2 3. Final waits for both Process1 and Process2</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-7-dependencies-vs-priorities","title":"Part 7: Dependencies vs Priorities","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#important-dependencies-override-priorities","title":"Important: Dependencies Override Priorities","text":"<p>Key Rule: Dependencies take precedence over priorities!</p> <pre><code># Task 1: Priority 2, no dependencies\ntask1 = create_task(name=\"task1\", priority=2, ...)\n\n# Task 2: Priority 0 (higher), but depends on Task 1\ntask2 = create_task(\n    name=\"task2\",\n    priority=0,  # Higher priority\n    dependencies=[{\"id\": task1.id, \"required\": True}],  # But depends on Task 1!\n    ...\n)\n</code></pre> <p>Execution Order: Task 1 runs first (even though Task 2 has higher priority), then Task 2</p> <p>Why: Dependencies control execution order, priorities only matter when tasks are ready to run!</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-8-common-patterns","title":"Part 8: Common Patterns","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#pattern-1-sequential-pipeline","title":"Pattern 1: Sequential Pipeline","text":"<pre><code># Tasks execute one after another\ntask1 \u2192 task2 \u2192 task3\n</code></pre> <p>Implementation: <pre><code>task2 = create_task(dependencies=[{\"id\": task1.id, \"required\": True}])\ntask3 = create_task(dependencies=[{\"id\": task2.id, \"required\": True}])\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#pattern-2-fan-in-converge","title":"Pattern 2: Fan-In (Converge)","text":"<pre><code># Multiple tasks converge to one\ntask1 \u2500\u2500\u2510\ntask2 \u2500\u2500\u251c\u2500\u2500\u2192 final_task\ntask3 \u2500\u2500\u2518\n</code></pre> <p>Implementation: <pre><code>final = create_task(\n    dependencies=[\n        {\"id\": task1.id, \"required\": True},\n        {\"id\": task2.id, \"required\": True},\n        {\"id\": task3.id, \"required\": True}\n    ]\n)\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#pattern-3-fan-out-diverge","title":"Pattern 3: Fan-Out (Diverge)","text":"<pre><code># One task spawns multiple dependent tasks\nroot_task \u2500\u2500\u2192 task1\n           \u2514\u2500\u2500\u2192 task2\n           \u2514\u2500\u2500\u2192 task3\n</code></pre> <p>Implementation: <pre><code>task1 = create_task(dependencies=[{\"id\": root_task.id, \"required\": True}])\ntask2 = create_task(dependencies=[{\"id\": root_task.id, \"required\": True}])\ntask3 = create_task(dependencies=[{\"id\": root_task.id, \"required\": True}])\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#pattern-4-conditional-execution","title":"Pattern 4: Conditional Execution","text":"<pre><code># Fallback pattern\nprimary \u2500\u2500\u2510\n          \u251c\u2500\u2500\u2192 final (works with either)\nfallback \u2500\u2518\n</code></pre> <p>Implementation: <pre><code>final = create_task(\n    dependencies=[\n        {\"id\": primary.id, \"required\": False},  # Optional\n        {\"id\": fallback.id, \"required\": False}   # Optional\n    ]\n)\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-9-best-practices","title":"Part 9: Best Practices","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#1-always-specify-dependencies-explicitly","title":"1. Always Specify Dependencies Explicitly","text":"<p>Good: <pre><code>task2 = create_task(\n    dependencies=[{\"id\": task1.id, \"required\": True}]  # Explicit\n)\n</code></pre></p> <p>Bad: <pre><code># Relying on implicit order - don't do this!\ntask2 = create_task(...)  # No dependency, but hoping task1 runs first\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#2-use-required-dependencies-by-default","title":"2. Use Required Dependencies by Default","text":"<p>Good: <pre><code>dependencies=[{\"id\": task1.id, \"required\": True}]  # Default\n</code></pre></p> <p>Only use optional when needed: <pre><code>dependencies=[{\"id\": task1.id, \"required\": False}]  # Only for fallbacks\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#3-keep-dependency-chains-manageable","title":"3. Keep Dependency Chains Manageable","text":"<p>Good: 3-5 levels deep Bad: 10+ levels deep (hard to debug)</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#4-document-complex-dependencies","title":"4. Document Complex Dependencies","text":"<pre><code># Dependency structure:\n# Task1 (no dependencies)\n#   \u2193\n# Task2 (depends on Task1)\n#   \u2193\n# Task3 (depends on Task2)\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-10-common-mistakes","title":"Part 10: Common Mistakes","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#mistake-1-circular-dependencies","title":"Mistake 1: Circular Dependencies","text":"<pre><code># Wrong: Circular dependency\ntask1 = create_task(dependencies=[{\"id\": task2.id}])\ntask2 = create_task(dependencies=[{\"id\": task1.id}])  # Circular!\n\n# This will cause infinite waiting!\n</code></pre> <p>Fix: Avoid circular dependencies. Use a directed acyclic graph (DAG).</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#mistake-2-missing-dependency","title":"Mistake 2: Missing Dependency","text":"<pre><code># Wrong: Task depends on task that doesn't exist\ntask2 = create_task(dependencies=[{\"id\": \"nonexistent_task\", \"required\": True}])\n\n# This will cause errors!\n</code></pre> <p>Fix: Always ensure dependency tasks exist and are in the same tree.</p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#mistake-3-confusing-parent-child-with-dependencies","title":"Mistake 3: Confusing Parent-Child with Dependencies","text":"<pre><code># Wrong: Thinking parent-child controls execution\nchild = create_task(parent_id=parent.id)  # This doesn't make child wait!\n\n# Right: Use dependencies\nchild = create_task(\n    parent_id=parent.id,  # Organizational\n    dependencies=[{\"id\": parent.id, \"required\": True}]  # Execution order\n)\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-11-debugging-dependencies","title":"Part 11: Debugging Dependencies","text":""},{"location":"getting-started/tutorials/tutorial-03-dependencies/#check-dependency-status","title":"Check Dependency Status","text":"<pre><code># After execution, check if dependencies were satisfied\ntask = await task_manager.task_repository.get_task_by_id(task_id)\n\nprint(f\"Task status: {task.status}\")\nprint(f\"Dependencies: {task.dependencies}\")\n\n# Check each dependency\nfor dep in task.dependencies:\n    dep_task = await task_manager.task_repository.get_task_by_id(dep[\"id\"])\n    print(f\"Dependency {dep['id']}: {dep_task.status}\")\n    if dep_task.status == \"failed\":\n        print(f\"  Error: {dep_task.error}\")\n</code></pre>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#why-is-my-task-stuck-in-pending","title":"Why is My Task Stuck in \"pending\"?","text":"<p>Common causes: 1. Dependencies not completed 2. Dependency task failed (if required=True) 3. Dependency task not in tree 4. Circular dependency</p> <p>Debug: <pre><code># Check task status\ntask = await task_manager.task_repository.get_task_by_id(task_id)\nprint(f\"Status: {task.status}\")\n\n# Check dependencies\nfor dep in task.dependencies:\n    dep_task = await task_manager.task_repository.get_task_by_id(dep[\"id\"])\n    print(f\"Dependency {dep['id']}: {dep_task.status}\")\n</code></pre></p>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#part-12-next-steps","title":"Part 12: Next Steps","text":"<p>You've mastered dependencies! Next:</p> <ol> <li>Task Orchestration Guide - Deep dive into orchestration</li> <li>Best Practices - Learn from experts</li> <li>Basic Examples - See more patterns</li> </ol>"},{"location":"getting-started/tutorials/tutorial-03-dependencies/#summary","title":"Summary","text":"<p>In this tutorial, you learned: - \u2705 How dependencies control execution order - \u2705 How to create sequential pipelines - \u2705 How to handle multiple dependencies - \u2705 How to use optional dependencies for fallbacks - \u2705 Common patterns and best practices - \u2705 How to debug dependency issues</p> <p>Key Takeaways: - Dependencies control when tasks run - Parent-child is for organization only - Dependencies override priorities - Always specify dependencies explicitly</p> <p>Next: Task Orchestration Guide \u2192</p>"},{"location":"guides/api-server/","title":"API Server Usage Guide","text":"<p>This guide explains how to use the apflow API server for remote task execution and integration.</p>"},{"location":"guides/api-server/#overview","title":"Overview","text":"<p>The API server provides: - A2A Protocol Server: Standard agent-to-agent communication protocol (default) - MCP Server: Model Context Protocol server exposing task orchestration as MCP tools and resources - HTTP API: RESTful endpoints for task management - Real-time Streaming: Progress updates via SSE/WebSocket - Multi-user Support: User isolation and authentication</p>"},{"location":"guides/api-server/#starting-the-api-server","title":"Starting the API Server","text":""},{"location":"guides/api-server/#basic-startup","title":"Basic Startup","text":"<pre><code># Start server on default port (8000) with A2A Protocol (default)\napflow serve\n\n# Or use the server command\napflow-server\n\n# Or use Python module\npython -m apflow.api.main\n</code></pre>"},{"location":"guides/api-server/#protocol-selection","title":"Protocol Selection","text":"<p>You can choose which protocol to use via the <code>APFLOW_API_PROTOCOL</code> environment variable:</p> <pre><code># A2A Protocol Server (default)\nexport APFLOW_API_PROTOCOL=a2a\npython -m apflow.api.main\n\n# MCP Server\nexport APFLOW_API_PROTOCOL=mcp\npython -m apflow.api.main\n\n# GraphQL Server\nexport APFLOW_API_PROTOCOL=graphql\npython -m apflow.api.main\n</code></pre> <p>Supported Protocols: - <code>a2a</code> (default): A2A Protocol Server for agent-to-agent communication - <code>mcp</code>: MCP (Model Context Protocol) Server exposing task orchestration as MCP tools and resources - <code>graphql</code>: GraphQL API with typed schema, subscriptions, and GraphiQL playground (see GraphQL API Reference)</p>"},{"location":"guides/api-server/#advanced-options","title":"Advanced Options","text":"<pre><code># Custom host and port\napflow serve --host 0.0.0.0 --port 8080\n\n# Enable auto-reload (development)\napflow serve --reload\n\n# Multiple workers (production)\napflow serve --workers 4\n\n# Custom configuration\napflow serve --config config.yaml\n</code></pre>"},{"location":"guides/api-server/#api-endpoints","title":"API Endpoints","text":""},{"location":"guides/api-server/#protocol-selection_1","title":"Protocol Selection","text":"<p>The API server supports multiple protocols:</p> <ol> <li>A2A Protocol (default): Standard agent-to-agent communication protocol</li> <li>MCP Protocol: Model Context Protocol for tool and resource access</li> </ol>"},{"location":"guides/api-server/#a2a-protocol-endpoints","title":"A2A Protocol Endpoints","text":"<p>The API server implements the A2A (Agent-to-Agent) Protocol standard when <code>APFLOW_API_PROTOCOL=a2a</code> (default).</p>"},{"location":"guides/api-server/#get-agent-card","title":"Get Agent Card","text":"<pre><code>curl http://localhost:8000/.well-known/agent-card\n</code></pre> <p>Returns agent capabilities and available skills.</p>"},{"location":"guides/api-server/#execute-task-tree-a2a-protocol","title":"Execute Task Tree (A2A Protocol)","text":"<pre><code>curl -X POST http://localhost:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [\n        {\n          \"id\": \"task1\",\n          \"name\": \"my_task\",\n          \"user_id\": \"user123\",\n          \"inputs\": {\"key\": \"value\"}\n        }\n      ]\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre> <p>Note: The method <code>execute_task_tree</code> is still supported for backward compatibility, but <code>tasks.execute</code> is the recommended standard method name.</p>"},{"location":"guides/api-server/#task-management-via-api","title":"Task Management via API","text":"<p>The API follows a three-layer architecture:</p> <ol> <li>A2A Protocol (<code>POST /</code>) \u2014 Agent-level actions only:</li> <li><code>tasks.execute</code> (or <code>execute_task_tree</code> for backward compatibility): Execute task tree</li> <li><code>tasks.generate</code>: Generate task tree from natural language using LLM</li> <li> <p><code>tasks.cancel</code>: Cancel running task</p> </li> <li> <p>Native API (<code>POST /tasks</code>) \u2014 All task operations via JSON-RPC:</p> </li> <li>CRUD: <code>tasks.create</code>, <code>tasks.get</code>, <code>tasks.update</code>, <code>tasks.delete</code></li> <li>Query: <code>tasks.detail</code>, <code>tasks.tree</code>, <code>tasks.list</code>, <code>tasks.children</code></li> <li>Monitoring: <code>tasks.running.list</code>, <code>tasks.running.status</code>, <code>tasks.running.count</code></li> <li> <p>Actions: <code>tasks.execute</code>, <code>tasks.generate</code>, <code>tasks.cancel</code>, <code>tasks.clone</code></p> </li> <li> <p>Method Discovery (<code>GET /tasks/methods</code>) \u2014 Returns all available methods grouped by category with input schemas. Useful for programmatic discovery and client code generation.</p> </li> </ol> <p>Note: CRUD operations (create, get, update, delete, list, etc.) are not supported via A2A <code>POST /</code>. Use <code>POST /tasks</code> instead. A2A <code>POST /</code> is reserved for agent-level actions that benefit from streaming and push notifications.</p>"},{"location":"guides/api-server/#task-management-endpoints-json-rpc","title":"Task Management Endpoints (JSON-RPC)","text":""},{"location":"guides/api-server/#create-tasks","title":"Create Tasks","text":"<pre><code>curl -X POST http://localhost:8000/tasks \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.create\",\n    \"params\": {\n      \"tasks\": [...]\n    },\n    \"id\": \"request-123\"\n  }'\n</code></pre>"},{"location":"guides/api-server/#get-task-status","title":"Get Task Status","text":"<pre><code>curl http://localhost:8000/tasks/{task_id}/status\n</code></pre>"},{"location":"guides/api-server/#list-tasks","title":"List Tasks","text":"<pre><code>curl http://localhost:8000/tasks?user_id=user123\n</code></pre>"},{"location":"guides/api-server/#streaming-support","title":"Streaming Support","text":""},{"location":"guides/api-server/#server-sent-events-sse","title":"Server-Sent Events (SSE)","text":"<p>Use <code>tasks.execute</code> with <code>use_streaming=true</code> to receive real-time updates via SSE:</p> <pre><code>curl -N -H \"Content-Type: application/json\" \\\n  -d '{\"jsonrpc\": \"2.0\", \"method\": \"tasks.execute\", \"params\": {\"task_id\": \"task-123\", \"use_streaming\": true}, \"id\": 1}' \\\n  http://localhost:8000/tasks\n</code></pre> <p>The response will be a Server-Sent Events stream with real-time progress updates.</p>"},{"location":"guides/api-server/#websocket","title":"WebSocket","text":"<p>Connect via WebSocket for bidirectional communication:</p> <pre><code>const ws = new WebSocket('ws://localhost:8000/ws');\nws.onmessage = (event) =&gt; {\n  const update = JSON.parse(event.data);\n  console.log('Progress:', update.progress);\n};\n</code></pre>"},{"location":"guides/api-server/#client-integration","title":"Client Integration","text":""},{"location":"guides/api-server/#python-client-example","title":"Python Client Example","text":"<pre><code>import httpx\nimport json\n\n# Execute task via API\nasync with httpx.AsyncClient() as client:\n    response = await client.post(\n        \"http://localhost:8000/\",\n        json={\n            \"jsonrpc\": \"2.0\",\n            \"method\": \"tasks.execute\",\n            \"params\": {\n                \"tasks\": [\n                    {\n                        \"id\": \"task1\",\n                        \"name\": \"my_task\",\n                        \"user_id\": \"user123\",\n                        \"inputs\": {\"key\": \"value\"}\n                    }\n                ]\n            },\n            \"id\": \"request-123\"\n        }\n    )\n    result = response.json()\n    print(result)\n</code></pre>"},{"location":"guides/api-server/#javascript-client-example","title":"JavaScript Client Example","text":"<pre><code>// Execute task via API\nconst response = await fetch('http://localhost:8000/', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n  },\n  body: JSON.stringify({\n    jsonrpc: '2.0',\n    method: 'tasks.execute',\n    params: {\n      tasks: [\n        {\n          id: 'task1',\n          name: 'my_task',\n          user_id: 'user123',\n          inputs: { key: 'value' }\n        }\n      ]\n    },\n    id: 'request-123'\n  })\n});\n\nconst result = await response.json();\nconsole.log(result);\n</code></pre>"},{"location":"guides/api-server/#authentication","title":"Authentication","text":""},{"location":"guides/api-server/#jwt-authentication-optional","title":"JWT Authentication (Optional)","text":"<p>The API supports JWT authentication via headers or cookies. You can generate tokens using the <code>generate_token()</code> function:</p> <pre><code>from apflow.api.a2a.server import generate_token\n\n# Generate JWT token\npayload = {\"user_id\": \"user123\", \"roles\": [\"admin\"]}\nsecret_key = \"your-secret-key\"\ntoken = generate_token(payload, secret_key, expires_in_days=30)\n</code></pre> <p>Using Token in Requests:</p> <pre><code># Method 1: Authorization header (recommended)\ncurl -X POST http://localhost:8000/ \\\n  -H \"Authorization: Bearer {token}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{...}'\n\n# Method 2: Cookie (for browser-based clients)\ncurl -X POST http://localhost:8000/ \\\n  -H \"Cookie: Authorization={token}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{...}'\n</code></pre> <p>Note: Authorization header takes priority over cookie if both are present.</p>"},{"location":"guides/api-server/#llm-api-key-management","title":"LLM API Key Management","text":"<p>The API server supports dynamic LLM API key injection for CrewAI tasks. Keys can be provided via request headers or user configuration.</p>"},{"location":"guides/api-server/#request-header-demoone-time-usage","title":"Request Header (Demo/One-time Usage)","text":"<p>For demo or one-time usage, you can provide LLM API keys via the <code>X-LLM-API-KEY</code> header:</p> <pre><code># Simple format (auto-detects provider from model)\ncurl -X POST http://localhost:8000/tasks \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-LLM-API-KEY: sk-your-openai-key\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.create\",\n    \"params\": {\n      \"tasks\": [{\n        \"id\": \"task1\",\n        \"name\": \"CrewAI Task\",\n        \"schemas\": {\"method\": \"crewai_executor\"},\n        \"params\": {\n          \"works\": {\n            \"agents\": {\n              \"researcher\": {\n                \"role\": \"Research Analyst\",\n                \"llm\": \"openai/gpt-4\"\n              }\n            }\n          }\n        }\n      }]\n    }\n  }'\n\n# Provider-specific format (explicit provider)\ncurl -X POST http://localhost:8000/tasks \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-LLM-API-KEY: openai:sk-your-openai-key\" \\\n  -d '{...}'\n\n# Anthropic example\ncurl -X POST http://localhost:8000/tasks \\\n  -H \"Content-Type: application/json\" \\\n  -H \"X-LLM-API-KEY: anthropic:sk-ant-your-key\" \\\n  -d '{...}'\n</code></pre> <p>Header Format: - Simple: <code>X-LLM-API-KEY: &lt;api-key&gt;</code> (provider auto-detected from model name) - Provider-specific: <code>X-LLM-API-KEY: &lt;provider&gt;:&lt;api-key&gt;</code> (e.g., <code>openai:sk-xxx</code>, <code>anthropic:sk-ant-xxx</code>)</p> <p>Supported Providers: - <code>openai</code> - OpenAI (GPT models) - <code>anthropic</code> - Anthropic (Claude models) - <code>google</code> / <code>gemini</code> - Google (Gemini models) - <code>mistral</code> - Mistral AI - <code>groq</code> - Groq - And more (see LLM Key Injector documentation)</p> <p>Priority: 1. Request header (<code>X-LLM-API-KEY</code>) - highest priority 2. User config (if <code>llm-key-config</code> extension is installed) 3. Environment variables (automatically read by CrewAI/LiteLLM)</p>"},{"location":"guides/api-server/#user-configuration-multi-user-scenarios","title":"User Configuration (Multi-user Scenarios)","text":"<p>For production multi-user scenarios, use the <code>llm-key-config</code> extension:</p> <pre><code># Install extension\npip install apflow[llm-key-config]\n</code></pre> <p>Then configure keys programmatically:</p> <pre><code>from apflow.extensions.llm_key_config import LLMKeyConfigManager\n\n# Set user's LLM key\nconfig_manager = LLMKeyConfigManager()\nconfig_manager.set_key(user_id=\"user123\", api_key=\"sk-xxx\", provider=\"openai\")\n\n# Set provider-specific keys\nconfig_manager.set_key(user_id=\"user123\", api_key=\"sk-xxx\", provider=\"openai\")\nconfig_manager.set_key(user_id=\"user123\", api_key=\"sk-ant-xxx\", provider=\"anthropic\")\n</code></pre> <p>Note: Keys are stored in memory (not in database). For production multi-server scenarios, consider using Redis.</p>"},{"location":"guides/api-server/#environment-variables-fallback","title":"Environment Variables (Fallback)","text":"<p>If no header or user config is provided, CrewAI/LiteLLM will automatically use provider-specific environment variables:</p> <pre><code>export OPENAI_API_KEY=\"sk-xxx\"\nexport ANTHROPIC_API_KEY=\"sk-ant-xxx\"\nexport GOOGLE_API_KEY=\"xxx\"\n</code></pre>"},{"location":"guides/api-server/#configuration","title":"Configuration","text":""},{"location":"guides/api-server/#environment-variables","title":"Environment Variables","text":"<pre><code># Server configuration\nexport APFLOW_API_HOST=0.0.0.0\nexport APFLOW_API_PORT=8000\n\n# Database configuration\nexport APFLOW_DATABASE_URL=postgresql://user:pass@localhost/db\n\n# Authentication\nexport APFLOW_JWT_SECRET=your-secret-key\n</code></pre>"},{"location":"guides/api-server/#configuration-file","title":"Configuration File","text":"<p>Create <code>config.yaml</code>:</p> <pre><code>server:\n  host: 0.0.0.0\n  port: 8000\n  workers: 4\n\ndatabase:\n  url: postgresql://user:pass@localhost/db\n\nauth:\n  enabled: true\n  jwt_secret: your-secret-key\n</code></pre>"},{"location":"guides/api-server/#distributed-cluster-mode","title":"Distributed Cluster Mode","text":"<p>The API server supports distributed cluster mode for high availability and horizontal scaling. When enabled, a <code>DistributedRuntime</code> is automatically initialized during server startup, providing lease-based task assignment, leader election, and multi-node coordination \u2014 all through PostgreSQL (no external services required).</p>"},{"location":"guides/api-server/#enabling-cluster-mode","title":"Enabling Cluster Mode","text":"<p>Set <code>APFLOW_CLUSTER_ENABLED=true</code> and provide a PostgreSQL <code>DATABASE_URL</code>:</p> <pre><code>export APFLOW_CLUSTER_ENABLED=true\nexport APFLOW_DATABASE_URL=postgresql+asyncpg://user:pass@db-host/apflow\nexport APFLOW_NODE_ROLE=auto    # auto | leader | worker | observer\nexport APFLOW_NODE_ID=node-1    # optional, auto-generated if not set\n\napflow serve --host 0.0.0.0 --port 8000\n</code></pre> <p>On startup, <code>create_runnable_app()</code> calls <code>_init_distributed_runtime()</code> which:</p> <ol> <li>Loads <code>DistributedConfig</code> from environment variables</li> <li>Creates a sync SQLAlchemy session factory for distributed coordination</li> <li>Instantiates <code>DistributedRuntime</code> and injects it into <code>TaskExecutor</code></li> <li>Wraps the ASGI app with a lifespan manager that starts the runtime on server startup and stops it on shutdown</li> </ol> <p>If cluster mode is not enabled, or no PostgreSQL URL is configured, the server continues as a normal single-node instance with no behavioral change.</p>"},{"location":"guides/api-server/#multi-node-example","title":"Multi-Node Example","text":"<pre><code># Node A \u2014 will attempt leader election\nAPFLOW_CLUSTER_ENABLED=true APFLOW_NODE_ROLE=auto APFLOW_NODE_ID=node-a \\\n  APFLOW_DATABASE_URL=postgresql+asyncpg://user:pass@db-host/apflow \\\n  apflow serve --port 8000\n\n# Node B \u2014 worker only\nAPFLOW_CLUSTER_ENABLED=true APFLOW_NODE_ROLE=worker APFLOW_NODE_ID=node-b \\\n  APFLOW_DATABASE_URL=postgresql+asyncpg://user:pass@db-host/apflow \\\n  apflow serve --port 8001\n</code></pre> <p>For full configuration reference, deployment patterns, and troubleshooting, see the Distributed Cluster Guide.</p>"},{"location":"guides/api-server/#production-deployment","title":"Production Deployment","text":""},{"location":"guides/api-server/#using-uvicorn-directly","title":"Using Uvicorn Directly","text":"<pre><code>uvicorn apflow.api.main:app \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --workers 4\n</code></pre>"},{"location":"guides/api-server/#using-docker","title":"Using Docker","text":"<pre><code>FROM python:3.12\nWORKDIR /app\nCOPY . .\nRUN pip install apflow[a2a]\nCMD [\"apflow-server\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"guides/api-server/#using-systemd","title":"Using Systemd","text":"<p>Create <code>/etc/systemd/system/apflow.service</code>:</p> <pre><code>[Unit]\nDescription=apflow API Server\nAfter=network.target\n\n[Service]\nType=simple\nUser=apflow\nWorkingDirectory=/opt/apflow\nExecStart=/usr/local/bin/apflow-server --host 0.0.0.0 --port 8000\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"guides/api-server/#monitoring","title":"Monitoring","text":""},{"location":"guides/api-server/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8000/health\n</code></pre>"},{"location":"guides/api-server/#metrics","title":"Metrics","text":"<pre><code>curl http://localhost:8000/metrics\n</code></pre>"},{"location":"guides/api-server/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/api-server/#server-wont-start","title":"Server Won't Start","text":"<ul> <li>Check if port is already in use</li> <li>Verify database connection</li> <li>Check logs for errors</li> </ul>"},{"location":"guides/api-server/#tasks-not-executing","title":"Tasks Not Executing","text":"<ul> <li>Verify executor is registered</li> <li>Check task name matches executor ID</li> <li>Review server logs</li> </ul>"},{"location":"guides/api-server/#connection-issues","title":"Connection Issues","text":"<ul> <li>Verify firewall settings</li> <li>Check network connectivity</li> <li>Ensure server is accessible</li> </ul>"},{"location":"guides/api-server/#mcp-server","title":"MCP Server","text":"<p>When started with <code>APFLOW_API_PROTOCOL=mcp</code>, the API server exposes task orchestration capabilities as MCP tools and resources.</p>"},{"location":"guides/api-server/#mcp-tools","title":"MCP Tools","text":"<p>The MCP server provides 8 tools for task orchestration:</p> <ul> <li><code>execute_task</code> - Execute tasks or task trees</li> <li><code>create_task</code> - Create new tasks or task trees</li> <li><code>get_task</code> - Get task details by ID</li> <li><code>update_task</code> - Update existing tasks</li> <li><code>delete_task</code> - Delete tasks (if all pending)</li> <li><code>list_tasks</code> - List tasks with filtering</li> <li><code>get_task_status</code> - Get status of running tasks</li> <li><code>cancel_task</code> - Cancel running tasks</li> </ul>"},{"location":"guides/api-server/#mcp-resources","title":"MCP Resources","text":"<p>The MCP server provides 2 resource types:</p> <ul> <li><code>task://{task_id}</code> - Access individual task data</li> <li><code>tasks://</code> - Access task list with query parameters (e.g., <code>tasks://?status=running&amp;limit=10</code>)</li> </ul>"},{"location":"guides/api-server/#mcp-endpoints","title":"MCP Endpoints","text":"<p>HTTP Mode: <pre><code># POST /mcp - JSON-RPC endpoint\ncurl -X POST http://localhost:8000/mcp \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 1,\n    \"method\": \"tools/list\",\n    \"params\": {}\n  }'\n</code></pre></p> <p>stdio Mode: <pre><code># Run as standalone process\npython -m apflow.api.mcp.server\n</code></pre></p>"},{"location":"guides/api-server/#mcp-usage-example","title":"MCP Usage Example","text":"<pre><code># List available tools\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 1,\n  \"method\": \"tools/list\",\n  \"params\": {}\n}\n\n# Call a tool\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 2,\n  \"method\": \"tools/call\",\n  \"params\": {\n    \"name\": \"execute_task\",\n    \"arguments\": {\n      \"task_id\": \"task-123\"\n    }\n  }\n}\n\n# Read a resource\n{\n  \"jsonrpc\": \"2.0\",\n  \"id\": 3,\n  \"method\": \"resources/read\",\n  \"params\": {\n    \"uri\": \"task://task-123\"\n  }\n}\n</code></pre>"},{"location":"guides/api-server/#programmatic-usage","title":"Programmatic Usage","text":"<p>For library usage in external projects (e.g., apflow-demo), you can import functions from the modular API components:</p>"},{"location":"guides/api-server/#extension-management","title":"Extension Management","text":"<pre><code>from apflow.api.extensions import initialize_extensions\n\n# Initialize extensions before creating the app\ninitialize_extensions(\n    include_stdio=True,\n    include_crewai=True,\n    # ... other extension flags\n)\n</code></pre>"},{"location":"guides/api-server/#protocol-management","title":"Protocol Management","text":"<pre><code>from apflow.api.protocols import (\n    get_protocol_from_env,\n    check_protocol_dependency,\n    get_supported_protocols,\n)\n\n# Get protocol from environment or default\nprotocol = get_protocol_from_env()\n\n# Check if protocol dependencies are installed\ncheck_protocol_dependency(protocol)\n\n# Get list of supported protocols\nprotocols = get_supported_protocols()  # ['a2a', 'mcp']\n</code></pre>"},{"location":"guides/api-server/#application-creation","title":"Application Creation","text":"<pre><code>from apflow.api.app import (\n    create_app_by_protocol,\n    create_a2a_server,\n    create_mcp_server,\n)\n\n# Create app with automatic extension initialization\napp = create_app_by_protocol(\n    protocol=\"a2a\",\n    auto_initialize_extensions=True,\n    task_routes_class=MyCustomTaskRoutes,  # Optional: custom TaskRoutes\n)\n\n# Or create specific server directly\na2a_app = create_a2a_server(\n    jwt_secret_key=\"your-secret\",\n    base_url=\"http://localhost:8000\",\n    auto_initialize_extensions=True,  # New: auto-initialize extensions\n    task_routes_class=MyCustomTaskRoutes,  # Optional: custom TaskRoutes\n)\n</code></pre>"},{"location":"guides/api-server/#custom-taskroutes","title":"Custom TaskRoutes","text":"<p>You can extend <code>TaskRoutes</code> functionality without monkey patching:</p> <pre><code>from apflow.api.routes.tasks import TaskRoutes\n\nclass MyCustomTaskRoutes(TaskRoutes):\n    async def handle_task_create(self, params, request, request_id):\n        # Add custom logic before parent implementation\n        result = await super().handle_task_create(params, request, request_id)\n        # Add custom logic after parent implementation\n        return result\n\n# Use custom TaskRoutes when creating the app\napp = create_app_by_protocol(\n    protocol=\"a2a\",\n    task_routes_class=MyCustomTaskRoutes,\n)\n</code></pre>"},{"location":"guides/api-server/#next-steps","title":"Next Steps","text":"<ul> <li>See HTTP API Reference for complete A2A endpoint documentation</li> <li>See GraphQL API Reference for GraphQL schema and examples</li> <li>Check Examples for integration examples</li> <li>See Custom Tasks Guide for MCP executor usage</li> <li>See Distributed Cluster Guide for multi-node deployment</li> </ul>"},{"location":"guides/best-practices/","title":"Best Practices Guide","text":"<p>Learn from the experts. This guide covers design patterns, optimization techniques, and best practices for building robust applications with apflow.</p>"},{"location":"guides/best-practices/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Task Design</li> <li>Orchestration Patterns</li> <li>Error Handling</li> <li>Performance Optimization</li> <li>Code Organization</li> <li>Testing Strategies</li> <li>Production Readiness</li> </ol>"},{"location":"guides/best-practices/#understanding-lifecycles","title":"Understanding Lifecycles","text":"<p>Important: Before diving into best practices, understand the execution model:</p> <ul> <li>Task Tree Execution Lifecycle: How tasks are created, distributed, executed, and completed</li> <li>DB Session Context Hook Lifecycle: How hooks access the database and share context</li> </ul> <p>See Task Tree Execution Lifecycle for comprehensive details on: - Session scope and lifetime (spans entire task tree) - Hook context setup and cleanup (guaranteed by finally blocks) - Execution order and concurrency guarantees - Error handling and resource cleanup patterns</p>"},{"location":"guides/best-practices/#task-design","title":"Task Design","text":""},{"location":"guides/best-practices/#1-single-responsibility-principle","title":"1. Single Responsibility Principle","text":"<p>Each task should do one thing well.</p> <p>Good: <pre><code>@executor_register()\nclass FetchUserData(BaseTask):\n    \"\"\"Fetches user data from API\"\"\"\n    # Only fetches data\n\n@executor_register()\nclass ProcessUserData(BaseTask):\n    \"\"\"Processes user data\"\"\"\n    # Only processes data\n\n@executor_register()\nclass SaveUserData(BaseTask):\n    \"\"\"Saves user data to database\"\"\"\n    # Only saves data\n</code></pre></p> <p>Bad: <pre><code>@executor_register()\nclass DoEverythingWithUserData(BaseTask):\n    \"\"\"Fetches, processes, saves, and sends notifications\"\"\"\n    # Does too much!\n</code></pre></p> <p>Benefits: - Easier to test - Easier to reuse - Easier to debug - Easier to maintain</p>"},{"location":"guides/best-practices/#2-idempotent-tasks","title":"2. Idempotent Tasks","text":"<p>Tasks should be idempotent - running them multiple times should produce the same result.</p> <p>Good: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    user_id = inputs.get(\"user_id\")\n\n    # Check if already processed\n    existing = await self._check_if_processed(user_id)\n    if existing:\n        return {\"status\": \"completed\", \"result\": existing, \"cached\": True}\n\n    # Process\n    result = await self._process(user_id)\n    await self._save_result(user_id, result)\n\n    return {\"status\": \"completed\", \"result\": result}\n</code></pre></p> <p>Benefits: - Safe to retry - Can handle duplicate requests - Better error recovery</p>"},{"location":"guides/best-practices/#3-validate-inputs-early","title":"3. Validate Inputs Early","text":"<p>Validate all inputs at the start of execution.</p> <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    # Validate immediately\n    url = inputs.get(\"url\")\n    if not url:\n        return {\n            \"status\": \"failed\",\n            \"error\": \"URL is required\",\n            \"error_type\": \"validation_error\"\n        }\n\n    if not isinstance(url, str):\n        return {\n            \"status\": \"failed\",\n            \"error\": \"URL must be a string\",\n            \"error_type\": \"type_error\"\n        }\n\n    if not url.startswith((\"http://\", \"https://\")):\n        return {\n            \"status\": \"failed\",\n            \"error\": \"URL must start with http:// or https://\",\n            \"error_type\": \"validation_error\"\n        }\n\n    # Continue with execution\n    ...\n</code></pre> <p>Benefits: - Fails fast - Clear error messages - Saves resources</p>"},{"location":"guides/best-practices/#4-return-consistent-results","title":"4. Return Consistent Results","text":"<p>Always return results in a consistent format.</p> <p>Good: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    try:\n        result = await self._process(inputs)\n        return {\n            \"status\": \"completed\",\n            \"result\": result,\n            \"metadata\": {\n                \"processed_at\": datetime.now().isoformat(),\n                \"input_count\": len(inputs)\n            }\n        }\n    except Exception as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": type(e).__name__\n        }\n</code></pre></p> <p>Bad: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    # Sometimes returns just the result\n    return result\n\n    # Sometimes returns wrapped\n    return {\"data\": result}\n\n    # Sometimes returns different format\n    return {\"output\": result, \"success\": True}\n</code></pre></p>"},{"location":"guides/best-practices/#5-use-async-properly","title":"5. Use Async Properly","text":"<p>Always use async/await for I/O operations.</p> <p>Good: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    # Async HTTP request\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            data = await response.json()\n\n    # Async file operation\n    async with aiofiles.open(file_path, 'r') as f:\n        content = await f.read()\n\n    return {\"status\": \"completed\", \"data\": data, \"content\": content}\n</code></pre></p> <p>Bad: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    # Blocking HTTP request\n    import requests\n    response = requests.get(url)  # Blocks the event loop!\n\n    # Blocking file operation\n    with open(file_path, 'r') as f:\n        content = f.read()  # Blocks!\n\n    return {\"status\": \"completed\"}\n</code></pre></p>"},{"location":"guides/best-practices/#orchestration-patterns","title":"Orchestration Patterns","text":""},{"location":"guides/best-practices/#1-use-dependencies-not-parent-child-for-execution-order","title":"1. Use Dependencies, Not Parent-Child for Execution Order","text":"<p>Remember: <code>parent_id</code> is organizational, <code>dependencies</code> control execution.</p> <p>Good: <pre><code># Task 1\nfetch = create_task(name=\"fetch_data\", ...)\n\n# Task 2 depends on Task 1 (execution order)\nprocess = create_task(\n    name=\"process_data\",\n    parent_id=fetch.id,  # Organizational\n    dependencies=[{\"id\": fetch.id, \"required\": True}],  # Execution order\n    ...\n)\n</code></pre></p> <p>Bad: <pre><code># Relying on parent-child for execution order\nprocess = create_task(\n    name=\"process_data\",\n    parent_id=fetch.id,  # This doesn't guarantee execution order!\n    ...\n)\n</code></pre></p>"},{"location":"guides/best-practices/#2-keep-task-trees-manageable","title":"2. Keep Task Trees Manageable","text":"<p>Break complex workflows into smaller, manageable trees.</p> <p>Good: <pre><code># Pipeline 1: Data collection\ncollect_tree = build_collection_tree()\n\n# Pipeline 2: Data processing\nprocess_tree = build_processing_tree()\n\n# Pipeline 3: Data storage\nstore_tree = build_storage_tree()\n\n# Execute sequentially\nawait task_manager.distribute_task_tree(collect_tree)\nawait task_manager.distribute_task_tree(process_tree)\nawait task_manager.distribute_task_tree(store_tree)\n</code></pre></p> <p>Bad: <pre><code># One massive tree with hundreds of tasks\nmega_tree = build_mega_tree_with_500_tasks()\nawait task_manager.distribute_task_tree(mega_tree)  # Hard to manage!\n</code></pre></p>"},{"location":"guides/best-practices/#3-use-parallel-execution-when-possible","title":"3. Use Parallel Execution When Possible","text":"<p>Tasks without dependencies can run in parallel.</p> <p>Good: <pre><code># All three tasks can run in parallel\ntask1 = create_task(name=\"fetch_data_1\", ...)  # No dependencies\ntask2 = create_task(name=\"fetch_data_2\", ...)  # No dependencies\ntask3 = create_task(name=\"fetch_data_3\", ...)  # No dependencies\n\n# Build tree\nroot = TaskTreeNode(root_task)\nroot.add_child(TaskTreeNode(task1))\nroot.add_child(TaskTreeNode(task2))\nroot.add_child(TaskTreeNode(task3))\n\n# All run in parallel!\nawait task_manager.distribute_task_tree(root)\n</code></pre></p> <p>Performance Benefit: - 3 tasks in parallel = ~3x faster than sequential - Great for independent operations</p>"},{"location":"guides/best-practices/#4-use-optional-dependencies-for-fallbacks","title":"4. Use Optional Dependencies for Fallbacks","text":"<p>Use optional dependencies for fallback scenarios.</p> <pre><code># Primary task\nprimary = create_task(\n    name=\"primary_fetch\",\n    ...\n)\n\n# Fallback task (runs even if primary fails)\nfallback = create_task(\n    name=\"fallback_fetch\",\n    dependencies=[{\"id\": primary.id, \"required\": False}],  # Optional\n    ...\n)\n\n# Final task (runs after either primary or fallback)\nfinal = create_task(\n    name=\"process_result\",\n    dependencies=[\n        {\"id\": primary.id, \"required\": False},\n        {\"id\": fallback.id, \"required\": False}\n    ],\n    ...\n)\n</code></pre>"},{"location":"guides/best-practices/#5-set-appropriate-priorities","title":"5. Set Appropriate Priorities","text":"<p>Use priorities consistently and sparingly.</p> <pre><code># Priority convention\nURGENT = 0      # Critical/emergency only\nHIGH = 1        # High priority business tasks\nNORMAL = 2      # Default for most tasks\nLOW = 3         # Background/low priority\n\n# Payment processing (critical)\npayment = create_task(name=\"process_payment\", priority=URGENT)\n\n# Data processing (normal)\ndata = create_task(name=\"process_data\", priority=NORMAL)\n\n# Cleanup (low priority)\ncleanup = create_task(name=\"cleanup\", priority=LOW)\n</code></pre>"},{"location":"guides/best-practices/#hooks-best-practices","title":"Hooks Best Practices","text":""},{"location":"guides/best-practices/#1-use-hooks-for-cross-cutting-concerns","title":"1. Use Hooks for Cross-Cutting Concerns","text":"<p>Good use cases: - Validation and transformation of inputs - Logging and monitoring - Authentication and authorization checks - Metrics collection - Notification sending</p> <pre><code>from apflow import register_pre_hook, register_post_hook\n\n@register_pre_hook\nasync def validate_and_enrich(task):\n    \"\"\"Validate inputs and add metadata\"\"\"\n    # Validate\n    if task.inputs and \"user_id\" in task.inputs:\n        if not task.inputs[\"user_id\"]:\n            raise ValueError(\"user_id is required\")\n\n    # Enrich with metadata\n    task.inputs[\"_hook_timestamp\"] = datetime.now().isoformat()\n    task.inputs[\"_environment\"] = os.getenv(\"ENV\", \"production\")\n\n@register_post_hook\nasync def log_and_metric(task, inputs, result):\n    \"\"\"Log execution and collect metrics\"\"\"\n    duration = (datetime.now() - task.created_at).total_seconds()\n    logger.info(f\"Task {task.id} completed in {duration}s\")\n    metrics.record(\"task.duration\", duration, tags={\"type\": task.type})\n</code></pre>"},{"location":"guides/best-practices/#2-modify-task-fields-using-hook-repository","title":"2. Modify Task Fields Using Hook Repository","text":"<p>For fields other than inputs, use <code>get_hook_repository()</code>:</p> <pre><code>from apflow import register_pre_hook, get_hook_repository\n\n@register_pre_hook\nasync def adjust_priority_by_load(task):\n    \"\"\"Adjust task priority based on system load\"\"\"\n    repo = get_hook_repository()\n    if not repo:\n        return\n\n    # Query current system load\n    pending_count = len(await repo.get_tasks_by_status(\"pending\"))\n\n    # Adjust priority if system is overloaded\n    if pending_count &gt; 100:\n        await repo.update_task(task.id, priority=task.priority + 1)\n</code></pre> <p>Remember: - <code>task.inputs</code> modifications are auto-persisted (no explicit save needed) - Other fields require explicit repository method calls - All hooks share the same database session - Changes made by one hook are visible to subsequent hooks</p>"},{"location":"guides/best-practices/#3-keep-hooks-fast-and-lightweight","title":"3. Keep Hooks Fast and Lightweight","text":"<p>Bad: <pre><code>@register_pre_hook\nasync def slow_hook(task):\n    # Don't do heavy computation in hooks!\n    await expensive_api_call()  # \u274c\n    time.sleep(5)  # \u274c\n    complex_calculation()  # \u274c\n</code></pre></p> <p>Good: <pre><code>@register_pre_hook\nasync def fast_hook(task):\n    # Quick validation and transformation only\n    if task.inputs:\n        task.inputs[\"validated\"] = True\n    # Heavy work should be in separate tasks\n</code></pre></p>"},{"location":"guides/best-practices/#4-handle-hook-failures-gracefully","title":"4. Handle Hook Failures Gracefully","text":"<p>Hooks should not crash the entire execution:</p> <pre><code>@register_pre_hook\nasync def safe_hook(task):\n    \"\"\"Hook with proper error handling\"\"\"\n    try:\n        # Your hook logic\n        await validate_something(task)\n    except Exception as e:\n        # Log error but don't fail the task\n        logger.error(f\"Hook failed for task {task.id}: {e}\")\n        # Optionally add error flag to inputs\n        if task.inputs is None:\n            task.inputs = {}\n        task.inputs[\"_hook_error\"] = str(e)\n</code></pre> <p>Note: The framework already catches hook exceptions and logs them without failing task execution. But adding your own error handling provides more control.</p>"},{"location":"guides/best-practices/#error-handling","title":"Error Handling","text":""},{"location":"guides/best-practices/#1-return-errors-dont-just-raise","title":"1. Return Errors, Don't Just Raise","text":"<p>Return error information in results for better control.</p> <p>Good: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    try:\n        result = await self._process(inputs)\n        return {\"status\": \"completed\", \"result\": result}\n    except ValueError as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": \"validation_error\",\n            \"field\": \"input_data\"\n        }\n    except TimeoutError as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": \"timeout_error\",\n            \"retryable\": True\n        }\n    except Exception as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": \"execution_error\"\n        }\n</code></pre></p> <p>Benefits: - More control over error format - Can include additional context - Easier to handle programmatically</p>"},{"location":"guides/best-practices/#2-handle-dependency-failures","title":"2. Handle Dependency Failures","text":"<p>Check dependency status before using results.</p> <pre><code># After execution, check dependencies\ntask = await task_manager.task_repository.get_task_by_id(task_id)\n\nif task.status == \"failed\":\n    # Check if dependencies failed\n    for dep in task.dependencies:\n        dep_task = await task_manager.task_repository.get_task_by_id(dep[\"id\"])\n        if dep_task.status == \"failed\":\n            print(f\"Dependency {dep['id']} failed: {dep_task.error}\")\n            # Handle dependency failure\n</code></pre>"},{"location":"guides/best-practices/#3-use-optional-dependencies-for-resilience","title":"3. Use Optional Dependencies for Resilience","text":"<p>Use optional dependencies for non-critical paths.</p> <pre><code># Critical path\ncritical = create_task(name=\"critical_task\", ...)\n\n# Optional enhancement (nice to have, but not required)\noptional = create_task(\n    name=\"optional_enhancement\",\n    dependencies=[{\"id\": critical.id, \"required\": False}],  # Optional\n    ...\n)\n\n# Final task (works with or without optional)\nfinal = create_task(\n    name=\"final_task\",\n    dependencies=[\n        {\"id\": critical.id, \"required\": True},  # Required\n        {\"id\": optional.id, \"required\": False}  # Optional\n    ],\n    ...\n)\n</code></pre>"},{"location":"guides/best-practices/#4-implement-retry-logic","title":"4. Implement Retry Logic","text":"<p>For transient failures, implement retry logic in executors.</p> <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    max_retries = inputs.get(\"max_retries\", 3)\n    retry_delay = inputs.get(\"retry_delay\", 1.0)\n\n    for attempt in range(max_retries):\n        try:\n            result = await self._process(inputs)\n            return {\"status\": \"completed\", \"result\": result, \"attempts\": attempt + 1}\n        except TransientError as e:\n            if attempt &lt; max_retries - 1:\n                await asyncio.sleep(retry_delay * (attempt + 1))  # Exponential backoff\n                continue\n            else:\n                return {\n                    \"status\": \"failed\",\n                    \"error\": str(e),\n                    \"error_type\": \"transient_error\",\n                    \"attempts\": max_retries\n                }\n</code></pre>"},{"location":"guides/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/best-practices/#1-use-parallel-execution","title":"1. Use Parallel Execution","text":"<p>Run independent tasks in parallel.</p> <pre><code># Sequential (slow)\ntask1 = create_task(...)\ntask2 = create_task(...)  # Waits for task1\ntask3 = create_task(...)  # Waits for task2\n# Total time: time1 + time2 + time3\n\n# Parallel (fast)\ntask1 = create_task(...)  # No dependencies\ntask2 = create_task(...)  # No dependencies\ntask3 = create_task(...)  # No dependencies\n# Total time: max(time1, time2, time3)\n</code></pre>"},{"location":"guides/best-practices/#2-batch-operations","title":"2. Batch Operations","text":"<p>Batch similar operations together.</p> <p>Good: <pre><code>@executor_register()\nclass BatchProcessor(BaseTask):\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        items = inputs.get(\"items\", [])\n\n        # Process all items in parallel\n        results = await asyncio.gather(*[\n            self._process_item(item) for item in items\n        ])\n\n        return {\"status\": \"completed\", \"results\": results}\n</code></pre></p> <p>Bad: <pre><code># Processing items one by one\nfor item in items:\n    result = await process_item(item)  # Sequential, slow!\n</code></pre></p>"},{"location":"guides/best-practices/#3-cache-results","title":"3. Cache Results","text":"<p>Cache expensive operations.</p> <pre><code>class CachedExecutor(BaseTask):\n    _cache = {}\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        cache_key = self._get_cache_key(inputs)\n\n        # Check cache\n        if cache_key in self._cache:\n            return {\n                \"status\": \"completed\",\n                \"result\": self._cache[cache_key],\n                \"cached\": True\n            }\n\n        # Compute\n        result = await self._expensive_operation(inputs)\n\n        # Cache\n        self._cache[cache_key] = result\n\n        return {\"status\": \"completed\", \"result\": result, \"cached\": False}\n</code></pre>"},{"location":"guides/best-practices/#4-optimize-database-queries","title":"4. Optimize Database Queries","text":"<p>Use efficient database queries in executors.</p> <pre><code># Good: Single query with filtering\nasync def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    user_ids = inputs.get(\"user_ids\", [])\n\n    # Single query with WHERE IN\n    query = \"SELECT * FROM users WHERE id IN :user_ids\"\n    results = await db.fetch(query, user_ids=user_ids)\n\n    return {\"status\": \"completed\", \"users\": results}\n\n# Bad: Multiple queries in loop\nasync def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    user_ids = inputs.get(\"user_ids\", [])\n    results = []\n\n    # Multiple queries (slow!)\n    for user_id in user_ids:\n        user = await db.fetch_one(\"SELECT * FROM users WHERE id = :id\", id=user_id)\n        results.append(user)\n\n    return {\"status\": \"completed\", \"users\": results}\n</code></pre>"},{"location":"guides/best-practices/#code-organization","title":"Code Organization","text":""},{"location":"guides/best-practices/#1-organize-executors-by-domain","title":"1. Organize Executors by Domain","text":"<p>Group related executors together.</p> <pre><code>my_project/\n\u251c\u2500\u2500 executors/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 fetch.py      # Data fetching executors\n\u2502   \u2502   \u2514\u2500\u2500 process.py    # Data processing executors\n\u2502   \u251c\u2500\u2500 api/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 http.py       # HTTP API executors\n\u2502   \u2514\u2500\u2500 storage/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 database.py   # Database executors\n</code></pre>"},{"location":"guides/best-practices/#2-use-shared-utilities","title":"2. Use Shared Utilities","text":"<p>Extract common functionality into utilities.</p> <pre><code># utils/validation.py\ndef validate_url(url: str) -&gt; bool:\n    \"\"\"Validate URL format\"\"\"\n    return url.startswith((\"http://\", \"https://\"))\n\n# executors/api.py\nfrom utils.validation import validate_url\n\n@executor_register()\nclass APICallExecutor(BaseTask):\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        url = inputs.get(\"url\")\n        if not validate_url(url):\n            return {\"status\": \"failed\", \"error\": \"Invalid URL\"}\n        # ...\n</code></pre>"},{"location":"guides/best-practices/#3-use-configuration","title":"3. Use Configuration","text":"<p>Externalize configuration.</p> <pre><code># config.py\nimport os\n\nAPI_TIMEOUT = int(os.getenv(\"API_TIMEOUT\", \"30\"))\nMAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", \"3\"))\n\n# executors/api.py\nfrom config import API_TIMEOUT, MAX_RETRIES\n\n@executor_register()\nclass APICallExecutor(BaseTask):\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        timeout = inputs.get(\"timeout\", API_TIMEOUT)\n        # ...\n</code></pre>"},{"location":"guides/best-practices/#4-document-your-code","title":"4. Document Your Code","text":"<p>Add clear documentation.</p> <pre><code>@executor_register()\nclass UserDataFetcher(BaseTask):\n    \"\"\"\n    Fetches user data from the API.\n\n    This executor retrieves user information from the external API\n    and returns it in a standardized format.\n\n    Args (inputs):\n        user_id (str): The ID of the user to fetch\n        include_profile (bool): Whether to include profile data (default: False)\n\n    Returns:\n        dict: User data with status and result\n\n    Example:\n        task = create_task(\n            name=\"user_data_fetcher\",\n            inputs={\"user_id\": \"123\", \"include_profile\": True}\n        )\n    \"\"\"\n    id = \"user_data_fetcher\"\n    # ...\n</code></pre>"},{"location":"guides/best-practices/#testing-strategies","title":"Testing Strategies","text":""},{"location":"guides/best-practices/#1-unit-test-executors","title":"1. Unit Test Executors","text":"<p>Test executors in isolation.</p> <pre><code>import pytest\nfrom my_executors import UserDataFetcher\n\n@pytest.mark.asyncio\nasync def test_user_data_fetcher():\n    executor = UserDataFetcher()\n\n    # Test with valid input\n    result = await executor.execute({\"user_id\": \"123\"})\n    assert result[\"status\"] == \"completed\"\n    assert \"user_id\" in result[\"result\"]\n\n    # Test with invalid input\n    result = await executor.execute({})\n    assert result[\"status\"] == \"failed\"\n    assert \"error\" in result\n</code></pre>"},{"location":"guides/best-practices/#2-integration-test-task-trees","title":"2. Integration Test Task Trees","text":"<p>Test complete workflows.</p> <pre><code>@pytest.mark.asyncio\nasync def test_user_data_pipeline():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create pipeline\n    fetch = await task_manager.task_repository.create_task(\n        name=\"user_data_fetcher\",\n        user_id=\"test_user\",\n        inputs={\"user_id\": \"123\"}\n    )\n\n    process = await task_manager.task_repository.create_task(\n        name=\"user_data_processor\",\n        user_id=\"test_user\",\n        dependencies=[{\"id\": fetch.id, \"required\": True}],\n        inputs={}\n    )\n\n    # Execute\n    root = TaskTreeNode(fetch)\n    root.add_child(TaskTreeNode(process))\n    await task_manager.distribute_task_tree(root)\n\n    # Verify\n    fetch_result = await task_manager.task_repository.get_task_by_id(fetch.id)\n    process_result = await task_manager.task_repository.get_task_by_id(process.id)\n\n    assert fetch_result.status == \"completed\"\n    assert process_result.status == \"completed\"\n</code></pre>"},{"location":"guides/best-practices/#3-mock-external-dependencies","title":"3. Mock External Dependencies","text":"<p>Mock external services in tests.</p> <pre><code>from unittest.mock import AsyncMock, patch\n\n@pytest.mark.asyncio\nasync def test_api_executor_with_mock():\n    executor = APICallExecutor()\n\n    with patch('aiohttp.ClientSession') as mock_session:\n        mock_response = AsyncMock()\n        mock_response.json = AsyncMock(return_value={\"data\": \"test\"})\n        mock_response.status = 200\n\n        mock_session.return_value.__aenter__.return_value.request.return_value.__aenter__.return_value = mock_response\n\n        result = await executor.execute({\"url\": \"https://api.example.com\"})\n\n        assert result[\"status\"] == \"completed\"\n        assert result[\"data\"] == {\"data\": \"test\"}\n</code></pre>"},{"location":"guides/best-practices/#production-readiness","title":"Production Readiness","text":""},{"location":"guides/best-practices/#1-add-logging","title":"1. Add Logging","text":"<p>Log important events.</p> <pre><code>from apflow.core.utils.logger import get_logger\n\nlogger = get_logger(__name__)\n\n@executor_register()\nclass LoggedExecutor(BaseTask):\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        logger.info(f\"Starting execution with inputs: {inputs}\")\n\n        try:\n            result = await self._process(inputs)\n            logger.info(f\"Execution completed successfully\")\n            return {\"status\": \"completed\", \"result\": result}\n        except Exception as e:\n            logger.error(f\"Execution failed: {e}\", exc_info=True)\n            return {\"status\": \"failed\", \"error\": str(e)}\n</code></pre>"},{"location":"guides/best-practices/#2-add-monitoring","title":"2. Add Monitoring","text":"<p>Monitor task execution.</p> <pre><code>import time\n\n@executor_register()\nclass MonitoredExecutor(BaseTask):\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        start_time = time.time()\n\n        try:\n            result = await self._process(inputs)\n            duration = time.time() - start_time\n\n            # Log metrics\n            logger.info(f\"Execution completed in {duration:.2f}s\")\n\n            return {\n                \"status\": \"completed\",\n                \"result\": result,\n                \"duration\": duration\n            }\n        except Exception as e:\n            duration = time.time() - start_time\n            logger.error(f\"Execution failed after {duration:.2f}s: {e}\")\n            return {\"status\": \"failed\", \"error\": str(e)}\n</code></pre>"},{"location":"guides/best-practices/#3-handle-timeouts","title":"3. Handle Timeouts","text":"<p>Set appropriate timeouts.</p> <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    timeout = inputs.get(\"timeout\", 30)\n\n    try:\n        result = await asyncio.wait_for(\n            self._process(inputs),\n            timeout=timeout\n        )\n        return {\"status\": \"completed\", \"result\": result}\n    except asyncio.TimeoutError:\n        return {\n            \"status\": \"failed\",\n            \"error\": f\"Operation timed out after {timeout}s\",\n            \"error_type\": \"timeout\"\n        }\n</code></pre>"},{"location":"guides/best-practices/#4-validate-production-configuration","title":"4. Validate Production Configuration","text":"<p>Validate configuration at startup.</p> <pre><code>def validate_config():\n    \"\"\"Validate production configuration\"\"\"\n    required_vars = [\"API_KEY\", \"DATABASE_URL\"]\n    missing = [var for var in required_vars if not os.getenv(var)]\n\n    if missing:\n        raise ValueError(f\"Missing required environment variables: {missing}\")\n\n# Call at startup\nvalidate_config()\n</code></pre>"},{"location":"guides/best-practices/#generate-executor-best-practices","title":"Generate Executor Best Practices","text":""},{"location":"guides/best-practices/#when-to-use-generation-modes","title":"When to Use Generation Modes","text":"<p>Use Single-Shot Mode (default) when: - Building prototypes or testing workflows - Requirements are simple and straightforward - Single executor or simple sequential workflows - Speed is more important than structure quality</p> <p>Use Multi-Phase Mode when: - Building production workflows - Requirements are complex with multiple steps - Multi-executor workflows (scrape + analyze, fetch + process + save) - Structure quality is critical - You need aggregator-root patterns enforced</p>"},{"location":"guides/best-practices/#example-good-requirements","title":"Example: Good Requirements","text":"<p>Good requirements are specific and mention patterns:</p> <pre><code># Good: Specific with clear pattern\nrequirement = \"Scrape data from https://api.example.com, analyze the content using LLM, then aggregate both results into a final report\"\n\n# Good: Mentions execution pattern\nrequirement = \"Fetch user data and order data in parallel, then merge the results and save to database\"\n\n# Bad: Too vague\nrequirement = \"Get some data and do something with it\"\n</code></pre>"},{"location":"guides/best-practices/#schema-definition-for-custom-executors","title":"Schema Definition for Custom Executors","text":"<p>Define input schemas as Pydantic BaseModel classes with <code>ClassVar</code>:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import ClassVar, Literal\n\nclass MyCustomInputSchema(BaseModel):\n    \"\"\"Input schema for my custom executor\"\"\"\n    url: str = Field(description=\"API endpoint to call\", pattern=r\"^https?://\")\n    method: Literal[\"GET\", \"POST\"] = Field(default=\"GET\", description=\"HTTP method\")\n\n@executor_register()\nclass MyCustomExecutor(BaseTask):\n    inputs_schema: ClassVar[type[BaseModel]] = MyCustomInputSchema\n    # get_input_schema() is automatically provided by BaseTask\n</code></pre> <p>Benefits: - Type-safe schema definition with IDE autocomplete - Automatic JSON Schema generation via <code>get_input_schema()</code> - Pydantic validation at runtime - Single source of truth for field definitions - Better error messages - Documentation for LLM</p>"},{"location":"guides/best-practices/#task-tree-structure","title":"Task Tree Structure","text":"<p>Follow aggregator-root pattern for multi-executor workflows:</p> <pre><code># Good: Aggregator root pattern\n[\n    {\n        \"id\": \"root\",\n        \"schemas\": {\"method\": \"aggregate_results_executor\"},  # Aggregator root\n        \"dependencies\": [{\"id\": \"fetch\"}, {\"id\": \"process\"}]\n    },\n    {\n        \"id\": \"fetch\",\n        \"parent_id\": \"root\",\n        \"schemas\": {\"method\": \"fetch_executor\"}\n    },\n    {\n        \"id\": \"process\", \n        \"parent_id\": \"root\",\n        \"dependencies\": [{\"id\": \"fetch\"}],\n        \"schemas\": {\"method\": \"llm_executor\"}\n    }\n]\n\n# Bad: Non-aggregator root with children\n[\n    {\n        \"id\": \"root\",\n        \"schemas\": {\"method\": \"fetch_executor\"}  # Wrong! Has children but isn't aggregator\n    },\n    {\n        \"id\": \"process\",\n        \"parent_id\": \"root\",\n        \"schemas\": {\"method\": \"llm_executor\"}\n    }\n]\n</code></pre> <p>Why aggregator-root matters: - User sees all child task statuses - Better failure visibility - Proper result aggregation - Framework best practice</p>"},{"location":"guides/best-practices/#summary","title":"Summary","text":"<p>Key Takeaways:</p> <ol> <li>Design: Single responsibility, idempotent, validate early</li> <li>Orchestration: Use dependencies for execution order, parallelize when possible</li> <li>Errors: Return errors with context, handle dependencies gracefully</li> <li>Performance: Parallelize, batch, cache, optimize queries</li> <li>Code: Organize by domain, use utilities, document well</li> <li>Testing: Unit test executors, integration test workflows, mock dependencies</li> <li>Production: Log, monitor, timeout, validate config</li> <li>Generate Executor: Use multi-phase for complex workflows, implement <code>get_input_schema()</code>, follow aggregator-root pattern</li> </ol>"},{"location":"guides/best-practices/#next-steps","title":"Next Steps","text":"<ul> <li>Task Orchestration Guide - Learn orchestration patterns</li> <li>Custom Tasks Guide - Create custom executors</li> <li>Generate Executor Guide - Task tree generation</li> <li>Generate Executor Improvements - Technical details</li> <li>Examples - See practical examples</li> <li>API Reference - Complete API documentation</li> </ul> <p>Want to contribute? Check the Contributing Guide</p>"},{"location":"guides/cli/","title":"CLI Usage Guide","text":""},{"location":"guides/cli/#overview","title":"Overview","text":"<p>The CLI can work in two modes: 1. Standalone Mode: Direct database access (default, no API server needed) 2. API Gateway Mode: Route commands through API server (when configured)</p> <p>When API server is configured via <code>apflow config</code>, CLI commands automatically use the API gateway, ensuring data consistency and enabling distributed deployments.</p>"},{"location":"guides/cli/#documentation","title":"Documentation","text":"<p>For detailed documentation, see:</p> <ul> <li>Configuration - Managing CLI and API configuration</li> <li>Commands - Complete reference of all CLI commands  </li> <li>API Gateway Integration - Using CLI with API server</li> <li>Examples - Practical usage examples</li> <li>CLI Directory Overview - Complete CLI documentation structure</li> </ul>"},{"location":"guides/cli/#quick-start","title":"Quick Start","text":""},{"location":"guides/cli/#installation","title":"Installation","text":"<pre><code># Install with CLI support\npip install -e \".[cli]\"\n\n# Or install everything\npip install -e \".[all]\"\n</code></pre>"},{"location":"guides/cli/#basic-usage-no-api-server-required","title":"Basic Usage (No API Server Required)","text":"<pre><code># Execute a task\napflow run flow --tasks '[{\"id\": \"task1\", \"name\": \"Task 1\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}}]'\n\n# Query task status\napflow tasks status task-123\n\n# List tasks from database\napflow tasks list\n</code></pre>"},{"location":"guides/cli/#configure-api-server","title":"Configure API Server","text":"<pre><code># Quick setup (recommended)\napflow config init-server\n\n# Or manual setup\napflow config set api-server http://localhost:8000\napflow config gen-token --role admin --save api_auth_token\n</code></pre>"},{"location":"guides/cli/#architecture-cli-vs-api","title":"Architecture: CLI vs API","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Shared Database                          \u2502\n\u2502  (DuckDB default, or PostgreSQL if configured)             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2                              \u25b2\n         \u2502                              \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   CLI   \u2502                  \u2502    API    \u2502\n    \u2502         \u2502                  \u2502  Server   \u2502\n    \u2502 Direct  \u2502\u25c4\u2500\u2500\u2500 or \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  (HTTP)   \u2502\n    \u2502 Access  \u2502    API Gateway   \u2502           \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Two Modes: - Standalone: CLI \u2192 Direct DB Access (default) - API Gateway: CLI \u2192 API Server \u2192 DB (when configured)</p> <p>Key Points: - CLI can work independently without API server - When API configured, CLI uses API gateway (solves DuckDB concurrency) - Shared data: Both CLI and API read/write to the same database - Configuration stored in multi-location system (project-local or user-global)</p>"},{"location":"guides/cli/#common-questions","title":"Common Questions","text":""},{"location":"guides/cli/#q-do-i-need-to-start-api-server-to-use-cli","title":"Q: Do I need to start API server to use CLI?","text":"<p>A: No. CLI works independently. It directly accesses the database, so no API server is required.</p>"},{"location":"guides/cli/#q-can-cli-and-api-run-at-the-same-time","title":"Q: Can CLI and API run at the same time?","text":"<p>A: Yes. They share the same database, so you can: - Execute tasks via CLI - Query tasks via API - Or vice versa</p>"},{"location":"guides/cli/#q-how-does-cli-query-task-status","title":"Q: How does CLI query task status?","text":"<p>A: Direct database access. CLI uses <code>TaskRepository</code> to query the database directly, not through API.</p>"},{"location":"guides/cli/#q-can-i-use-cli-to-query-tasks-created-by-api","title":"Q: Can I use CLI to query tasks created by API?","text":"<p>A: Yes. Since they share the same database, CLI can query any task created by API, and vice versa.</p>"},{"location":"guides/cli/#q-whats-the-difference-between-cli-and-api","title":"Q: What's the difference between CLI and API?","text":"Feature CLI API Execution Direct via TaskExecutor Via HTTP/A2A protocol Query Direct database access Via HTTP endpoints Setup No server needed Requires server Remote Access No (local only) Yes (HTTP) A2A Protocol No Yes Speed Fast (direct DB) Slightly slower (HTTP overhead) Use Case Local dev, scripts Production, remote access"},{"location":"guides/cli/#best-practices","title":"Best Practices","text":""},{"location":"guides/cli/#1-development-workflow","title":"1. Development Workflow","text":"<pre><code># Use CLI for quick testing\napflow run flow --tasks '[{\"id\": \"task1\", \"name\": \"Task 1\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}}]'\n\n# Use API server for integration testing\napflow serve --reload\n# Then test API endpoints\n</code></pre>"},{"location":"guides/cli/#2-production-deployment","title":"2. Production Deployment","text":"<pre><code># Option A: CLI only (for automation/scripts)\n# No server needed, just use CLI commands\n\n# Option B: API server (for remote access)\n# Start with A2A protocol (default)\napflow daemon start --port 8000\n# Or start with MCP protocol\napflow daemon start --port 8000 --protocol mcp\n# Then use HTTP API, A2A protocol, or MCP protocol\n</code></pre>"},{"location":"guides/cli/#3-monitoring","title":"3. Monitoring","text":"<pre><code># For single task\napflow tasks watch --task-id task-123\n\n# For all tasks\napflow tasks watch --all\n\n# For specific user\napflow tasks list --user-id my-user\n</code></pre>"},{"location":"guides/cli/#4-error-handling","title":"4. Error Handling","text":"<pre><code># Check task status after execution\napflow tasks status &lt;task_id&gt;\n\n# If failed, check error message\n# Error is stored in task.error field\n\n# Cancel stuck tasks\napflow tasks cancel &lt;task_id&gt; --force\n\n# Cancel with custom error message\napflow tasks cancel &lt;task_id&gt; --error-message \"Timed out\"\n</code></pre>"},{"location":"guides/cli/#database-configuration","title":"Database Configuration","text":""},{"location":"guides/cli/#default-duckdb-embedded-zero-config","title":"Default: DuckDB (Embedded, Zero Config)","text":"<p>CLI uses DuckDB by default - no configuration needed:</p> <pre><code># Just use CLI - database is created automatically\napflow run flow --tasks '[{\"id\": \"task1\", \"name\": \"Task 1\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}}]'\n</code></pre> <p>Database file location: <code>~/.aipartnerup/data/apflow.duckdb</code> (or configured path)</p>"},{"location":"guides/cli/#optional-postgresql","title":"Optional: PostgreSQL","text":"<p>If you want to use PostgreSQL (for production or shared access):</p> <pre><code># Set environment variable\nexport DATABASE_URL=\"postgresql+asyncpg://user:password@localhost/apflow\"\n\n# Use CLI as normal - it will connect to PostgreSQL\napflow run flow --tasks '[{\"id\": \"task1\", \"name\": \"Task 1\", \"schemas\": {\"method\": \"system_info_executor\"}, \"inputs\": {\"resource\": \"cpu\"}}]'\n</code></pre> <p>Note: Both CLI and API will use the same database connection string, so they share data automatically.</p>"},{"location":"guides/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/cli/#problem-task-not-found","title":"Problem: \"Task not found\"","text":"<p>Solution: Check if task ID is correct: <pre><code>apflow tasks list  # See all running tasks\napflow tasks status &lt;task_id&gt;  # Check specific task\n</code></pre></p>"},{"location":"guides/cli/#problem-database-connection-error","title":"Problem: \"Database connection error\"","text":"<p>Solution: Check database configuration: <pre><code># For DuckDB (default), no config needed\n# For PostgreSQL, check DATABASE_URL environment variable\necho $DATABASE_URL\n</code></pre></p>"},{"location":"guides/cli/#problem-task-is-stuck","title":"Problem: \"Task is stuck\"","text":"<p>Solution: Cancel and restart: <pre><code>apflow tasks cancel &lt;task_id&gt; --force\napflow run flow &lt;batch_id&gt; --inputs '...'\n</code></pre></p>"},{"location":"guides/cli/#problem-cannot-query-task-status","title":"Problem: \"Cannot query task status\"","text":"<p>Solution: Ensure database is accessible: <pre><code># Check if database file exists (DuckDB)\nls ~/.aipartnerup/data/apflow.duckdb\n\n# Or check PostgreSQL connection\npsql $DATABASE_URL -c \"SELECT COUNT(*) FROM apflow_tasks;\"\n</code></pre></p>"},{"location":"guides/cli/#summary","title":"Summary","text":"<ul> <li>\u2705 CLI is independent - No API server required</li> <li>\u2705 Direct database access - Fast and efficient</li> <li>\u2705 Shared database - CLI and API can work together</li> <li>\u2705 Full functionality - Execute, query, monitor, cancel tasks</li> <li>\u2705 Production ready - Can be used in scripts and automation</li> </ul> <p>Use CLI for local development and automation, use API server for production deployment and remote access.</p>"},{"location":"guides/cli/#cli-extension-dynamic-plugins","title":"CLI Extension (Dynamic Plugins)","text":"<p>The <code>apflow</code> CLI supports dynamic discovery of subcommands through Python's <code>entry_points</code> mechanism. This allows external projects to add their own command groups directly to the main CLI without modifying the core library.</p>"},{"location":"guides/cli/#unified-entry-point","title":"Unified Entry Point","text":"<p>Users always use the same consistent entry point: - <code>apflow &lt;your-plugin-name&gt; &lt;command&gt;</code></p>"},{"location":"guides/cli/#example-users-extension","title":"Example: Users Extension","text":"<p>If a plugin registers a command group called <code>users</code>, a user can simply run: <pre><code>apflow users stat\n</code></pre></p> <p>For more details on how to develop these extensions, see the Custom Tasks Guide.</p>"},{"location":"guides/cli/#extending-and-overriding-cli-commands","title":"Extending and Overriding CLI Commands","text":"<p>apflow allows you to register new CLI command groups or single commands, extend existing groups, or override built-in commands and groups. This is done using the <code>@cli_register</code> decorator, with the <code>group</code> and <code>override=True</code> parameters.</p>"},{"location":"guides/cli/#register-a-new-command-or-group","title":"Register a New Command or Group","text":"<pre><code>from apflow.cli.decorators import cli_register\n\n@cli_register(name=\"my-group\", help=\"My command group\")\nclass MyGroup:\n    def foo(self):\n        print(\"foo\")\n    def bar(self):\n        print(\"bar\")\n</code></pre>"},{"location":"guides/cli/#add-a-subcommand-to-an-existing-group","title":"Add a Subcommand to an Existing Group","text":"<pre><code>@cli_register(group=\"my-group\", name=\"baz\", help=\"Baz command\")\ndef baz():\n    print(\"baz\")\n</code></pre>"},{"location":"guides/cli/#override-an-existing-command-or-group","title":"Override an Existing Command or Group","text":"<pre><code>@cli_register(name=\"my-group\", override=True)\nclass NewMyGroup:\n    ...\n\n@cli_register(group=\"my-group\", name=\"foo\", override=True)\ndef new_foo():\n    print(\"new foo\")\n</code></pre> <p>Override a built-in command (e.g., 'run'): <pre><code>from apflow.cli.decorators import cli_register\n\n@cli_register(name=\"run\", override=True, help=\"Override built-in run command\")\ndef my_run():\n    print(\"This is my custom run command!\")\n</code></pre> Now, running <code>apflow run</code> will execute your custom logic instead of the built-in command.</p>"},{"location":"guides/cli/#core-extension-override","title":"Core Extension Override","text":"<p>apflow also supports custom extensions for executors, hooks, storage backends, and more. You can register your own or override built-in extensions by passing <code>override=True</code> when registering.</p> <p>Best Practices: - Use <code>override=True</code> only when you want to replace an existing command or extension. - Keep extension logic simple and well-documented. - Test your extensions thoroughly.</p>"},{"location":"guides/custom-tasks/","title":"Web Content Extraction: scrape_executor","text":"<p>For any requirement involving extracting the main text or metadata from a website (such as analyzing, summarizing, or evaluating website content), you MUST use the <code>scrape_executor</code>.</p> <p>Do NOT use <code>rest_executor</code> or <code>command_executor</code> for web content extraction.</p> <p>scrape_executor is designed to fetch and extract the main content and metadata from a given URL, making it suitable for analytics, machine learning, or information retrieval tasks.</p>"},{"location":"guides/custom-tasks/#example-scrape-and-analyze-a-website","title":"Example: Scrape and Analyze a Website","text":"<pre><code>[\n    {\n        \"id\": \"task_1\",\n        \"name\": \"Scrape Website Content\",\n        \"schemas\": {\"method\": \"scrape_executor\"},\n        \"inputs\": {\n            \"url\": \"https://example.com\",\n            \"max_chars\": 5000,\n            \"extract_metadata\": true\n        }\n    },\n    {\n        \"id\": \"task_2\",\n        \"name\": \"Analyze Scraped Content\",\n        \"schemas\": {\"method\": \"llm_executor\"},\n        \"parent_id\": \"task_1\",\n        \"dependencies\": [{\"id\": \"task_1\", \"required\": true}],\n        \"inputs\": {\n            \"model\": \"gpt-4\",\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"Analyze the content and provide an evaluation.\"}\n            ]\n        }\n    }\n]\n</code></pre> <p>Tip: - Use <code>scrape_executor</code> for any workflow that needs to extract readable content or metadata from a website for downstream analysis. - Only use <code>rest_executor</code> for raw HTTP APIs, and <code>command_executor</code> for unrelated shell commands.</p>"},{"location":"guides/custom-tasks/#custom-tasks-guide","title":"Custom Tasks Guide","text":"<p>For contributors: See the Extension Registry Design for advanced extension patterns and framework internals. This guide is focused on user-level custom executor development.</p> <p>Learn how to create your own custom executors (tasks) in apflow. This guide will walk you through everything from simple tasks to advanced patterns.</p>"},{"location":"guides/custom-tasks/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>\u2705 How to create custom executors</li> <li>\u2705 How to register and use them</li> <li>\u2705 Input/output validation with Pydantic schemas</li> <li>\u2705 Error handling best practices</li> <li>\u2705 Common patterns and examples</li> <li>\u2705 Testing your custom tasks</li> </ul>"},{"location":"guides/custom-tasks/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Quick Start</li> <li>Understanding Executors</li> <li>Creating Your First Executor</li> <li>Required Components</li> <li>Input Schema</li> <li>Error Handling</li> <li>Common Patterns</li> <li>Advanced Features</li> <li>Best Practices</li> <li>Testing</li> </ol>"},{"location":"guides/custom-tasks/#quick-start","title":"Quick Start","text":"<p>The fastest way to create a custom executor:</p> <pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any\nfrom pydantic import BaseModel, Field\n\n# Define input schema as a Pydantic model\nclass MyFirstInputSchema(BaseModel):\n    \"\"\"Input schema for my first executor\"\"\"\n    data: str = Field(description=\"Input data\")\n\n@executor_register()\nclass MyFirstExecutor(BaseTask):\n    \"\"\"A simple custom executor\"\"\"\n\n    id = \"my_first_executor\"\n    name = \"My First Executor\"\n    description = \"Does something useful\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = MyFirstInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute the task\"\"\"\n        result = f\"Processed: {inputs.get('data', 'no data')}\"\n        return {\"status\": \"completed\", \"result\": result}\n</code></pre> <p>That's it! Just import it and use it:</p> <pre><code># Import to register\nfrom my_module import MyFirstExecutor\n\n# Use it\ntask = await task_manager.task_repository.create_task(\n    name=\"my_first_executor\",  # Must match id\n    user_id=\"user123\",\n    inputs={\"data\": \"Hello!\"}\n)\n</code></pre>"},{"location":"guides/custom-tasks/#understanding-executors","title":"Understanding Executors","text":""},{"location":"guides/custom-tasks/#what-is-an-executor","title":"What is an Executor?","text":"<p>An executor is a piece of code that performs a specific task. Think of it as a function that: - Takes inputs (parameters) - Does some work - Returns a result</p> <p>Example: - An executor that fetches data from an API - An executor that processes files - An executor that sends emails - An executor that runs AI models</p>"},{"location":"guides/custom-tasks/#executor-vs-task","title":"Executor vs Task","text":"<p>Executor: The code that does the work (reusable) Task: An instance of work to be done (specific execution)</p> <p>Analogy: - Executor = A recipe (reusable template) - Task = A specific meal made from the recipe (one-time execution)</p>"},{"location":"guides/custom-tasks/#basetask-vs-executabletask","title":"BaseTask vs ExecutableTask","text":"<p>BaseTask: Recommended base class (simpler, includes registration) <pre><code>from apflow import BaseTask, executor_register\n\n@executor_register()\nclass MyTask(BaseTask):\n    id = \"my_task\"\n    # ...\n</code></pre></p> <p>ExecutableTask: Lower-level interface (more control) <pre><code>from apflow import ExecutableTask\n\nclass MyTask(ExecutableTask):\n    @property\n    def id(self) -&gt; str:\n        return \"my_task\"\n    # ...\n</code></pre></p> <p>Recommendation: Use <code>BaseTask</code> with <code>@executor_register()</code> - it's simpler!</p>"},{"location":"guides/custom-tasks/#creating-your-first-executor","title":"Creating Your First Executor","text":"<p>Let's create a complete, working example step by step.</p>"},{"location":"guides/custom-tasks/#step-1-create-the-executor-class","title":"Step 1: Create the Executor Class","text":"<p>Create a file <code>greeting_executor.py</code>:</p> <pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any, Literal\nfrom pydantic import BaseModel, Field\n\n# Define input schema\nclass GreetingInputSchema(BaseModel):\n    \"\"\"Input schema for greeting executor\"\"\"\n    name: str = Field(description=\"Name of the person to greet\")\n    language: Literal[\"en\", \"es\", \"fr\", \"zh\"] = Field(\n        default=\"en\", description=\"Language for the greeting\"\n    )\n\n@executor_register()\nclass GreetingExecutor(BaseTask):\n    \"\"\"Creates personalized greetings\"\"\"\n\n    id = \"greeting_executor\"\n    name = \"Greeting Executor\"\n    description = \"Creates personalized greeting messages\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = GreetingInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute greeting creation\"\"\"\n        name = inputs.get(\"name\", \"Guest\")\n        language = inputs.get(\"language\", \"en\")\n\n        greetings = {\n            \"en\": f\"Hello, {name}!\",\n            \"es\": f\"\u00a1Hola, {name}!\",\n            \"fr\": f\"Bonjour, {name}!\"\n        }\n\n        return {\n            \"greeting\": greetings.get(language, greetings[\"en\"]),\n            \"name\": name,\n            \"language\": language\n        }\n</code></pre>"},{"location":"guides/custom-tasks/#step-2-use-your-executor","title":"Step 2: Use Your Executor","text":"<p>Create a file <code>use_greeting.py</code>:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n# Import to register the executor\nfrom greeting_executor import GreetingExecutor\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create task using your executor\n    task = await task_manager.task_repository.create_task(\n        name=\"greeting_executor\",  # Must match executor id\n        user_id=\"user123\",\n        inputs={\n            \"name\": \"Alice\",\n            \"language\": \"en\"\n        }\n    )\n\n    # Execute\n    task_tree = TaskTreeNode(task)\n    await task_manager.distribute_task_tree(task_tree)\n\n    # Get result\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Greeting: {result.result['greeting']}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/custom-tasks/#step-3-run-it","title":"Step 3: Run It","text":"<pre><code>python use_greeting.py\n</code></pre> <p>Expected Output: <pre><code>Greeting: Hello, Alice!\n</code></pre></p> <p>Congratulations! You just created and used your first custom executor! \ud83c\udf89</p>"},{"location":"guides/custom-tasks/#required-components","title":"Required Components","text":"<p>Every executor must have these components:</p>"},{"location":"guides/custom-tasks/#1-unique-id","title":"1. Unique ID","text":"<p>Purpose: Identifies the executor (used when creating tasks)</p> <pre><code>id = \"my_executor_id\"  # Must be unique across all executors\n</code></pre> <p>Best Practices: - Use lowercase with underscores - Be descriptive: <code>fetch_user_data</code> not <code>task1</code> - Keep it consistent: don't change after deployment</p>"},{"location":"guides/custom-tasks/#2-display-name","title":"2. Display Name","text":"<p>Purpose: Human-readable name</p> <pre><code>name = \"My Executor\"  # What users see\n</code></pre>"},{"location":"guides/custom-tasks/#3-description","title":"3. Description","text":"<p>Purpose: Explains what the executor does</p> <pre><code>description = \"Fetches user data from the API\"\n</code></pre>"},{"location":"guides/custom-tasks/#4-execute-method","title":"4. Execute Method","text":"<p>Purpose: The actual work happens here</p> <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Execute the task\n\n    Args:\n        inputs: Input parameters (from task.inputs)\n\n    Returns:\n        Execution result dictionary\n    \"\"\"\n    # Your logic here\n    return {\"status\": \"completed\", \"result\": \"...\"}\n</code></pre> <p>Key Points: - Must be <code>async</code> - Receives <code>inputs</code> dictionary - Returns a dictionary - Can raise exceptions (will be caught by TaskManager)</p>"},{"location":"guides/custom-tasks/#5-input-schema","title":"5. Input Schema","text":"<p>Purpose: Defines what inputs are expected (for validation and documentation)</p> <p>Define input schemas as Pydantic BaseModel classes and assign them as <code>ClassVar</code> on the executor. <code>BaseTask</code> automatically implements <code>get_input_schema()</code> by converting the model to JSON Schema:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import ClassVar\n\nclass MyInputSchema(BaseModel):\n    \"\"\"Input schema for my executor\"\"\"\n    param1: str = Field(description=\"Parameter description\")\n\nclass MyExecutor(BaseTask):\n    inputs_schema: ClassVar[type[BaseModel]] = MyInputSchema\n    # get_input_schema() is automatically provided by BaseTask\n</code></pre>"},{"location":"guides/custom-tasks/#input-schema","title":"Input Schema","text":"<p>Input schemas are defined using Pydantic BaseModel classes. <code>BaseTask</code> converts them to JSON Schema automatically via <code>get_input_schema()</code>.</p>"},{"location":"guides/custom-tasks/#basic-schema","title":"Basic Schema","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass MyInputSchema(BaseModel):\n    \"\"\"Input schema for my executor\"\"\"\n    name: str = Field(description=\"Person's name\")\n    age: int = Field(default=0, description=\"Person's age\")\n</code></pre>"},{"location":"guides/custom-tasks/#common-field-types","title":"Common Field Types","text":""},{"location":"guides/custom-tasks/#string","title":"String","text":"<pre><code>name: str = Field(description=\"Person's name\", min_length=1, max_length=100)\n</code></pre>"},{"location":"guides/custom-tasks/#integer","title":"Integer","text":"<pre><code>age: int = Field(description=\"Person's age\", ge=0, le=150)\n</code></pre>"},{"location":"guides/custom-tasks/#boolean","title":"Boolean","text":"<pre><code>enabled: bool = Field(default=False, description=\"Whether feature is enabled\")\n</code></pre>"},{"location":"guides/custom-tasks/#array","title":"Array","text":"<pre><code>items: list[str] = Field(description=\"List of items\", min_length=1)\n</code></pre>"},{"location":"guides/custom-tasks/#object","title":"Object","text":"<pre><code>config: Dict[str, str] = Field(description=\"Configuration object\")\n</code></pre>"},{"location":"guides/custom-tasks/#enum-limited-choices","title":"Enum (Limited Choices)","text":"<pre><code>from typing import Literal\n\nstatus: Literal[\"pending\", \"active\", \"completed\"] = Field(\n    default=\"pending\", description=\"Task status\"\n)\n</code></pre>"},{"location":"guides/custom-tasks/#default-values","title":"Default Values","text":"<p>Provide defaults for optional parameters:</p> <pre><code>timeout: int = Field(default=30, description=\"Timeout in seconds\")\n</code></pre>"},{"location":"guides/custom-tasks/#required-fields","title":"Required Fields","text":"<p>Fields without a default value are required:</p> <pre><code>class MyInputSchema(BaseModel):\n    \"\"\"Input schema\"\"\"\n    name: str = Field(description=\"Name\")          # Required (no default)\n    email: str = Field(description=\"Email\")         # Required (no default)\n    age: int = Field(default=0, description=\"Age\")  # Optional (has default)\n</code></pre>"},{"location":"guides/custom-tasks/#complete-schema-example","title":"Complete Schema Example","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import ClassVar, Dict, Any, Literal, Optional\n\nclass APICallInputSchema(BaseModel):\n    \"\"\"Input schema for API call executor\"\"\"\n    url: str = Field(description=\"API endpoint URL\")\n    method: Literal[\"GET\", \"POST\", \"PUT\", \"DELETE\"] = Field(\n        default=\"GET\", description=\"HTTP method\"\n    )\n    headers: Optional[Dict[str, str]] = Field(\n        default=None, description=\"HTTP headers\"\n    )\n    timeout: int = Field(\n        default=30, description=\"Timeout in seconds\", ge=1, le=300\n    )\n    retry: bool = Field(default=False, description=\"Whether to retry on failure\")\n\n@executor_register()\nclass APICallExecutor(BaseTask):\n    id = \"api_call_executor\"\n    name = \"API Call Executor\"\n    description = \"Calls an external HTTP API\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = APICallInputSchema\n</code></pre>"},{"location":"guides/custom-tasks/#error-handling","title":"Error Handling","text":""},{"location":"guides/custom-tasks/#returning-errors-recommended","title":"Returning Errors (Recommended)","text":"<p>Return error information in the result:</p> <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    try:\n        result = perform_operation(inputs)\n        return {\"status\": \"completed\", \"result\": result}\n    except ValueError as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": \"validation_error\"\n        }\n    except Exception as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": \"execution_error\"\n        }\n</code></pre> <p>Benefits: - More control over error format - Can include additional context - Task status will be \"failed\"</p>"},{"location":"guides/custom-tasks/#raising-exceptions","title":"Raising Exceptions","text":"<p>You can also raise exceptions (TaskManager will catch them):</p> <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    if not inputs.get(\"required_param\"):\n        raise ValueError(\"required_param is required\")\n\n    # Continue with execution\n    return {\"status\": \"completed\", \"result\": \"...\"}\n</code></pre> <p>Note: TaskManager will catch exceptions and mark the task as \"failed\".</p>"},{"location":"guides/custom-tasks/#best-practices","title":"Best Practices","text":"<ol> <li>Validate early: Check inputs at the start</li> <li>Return meaningful errors: Include error type and message</li> <li>Handle specific exceptions: Catch specific errors, not just <code>Exception</code></li> <li>Include context: Add relevant information to error messages</li> </ol> <p>Example: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    url = inputs.get(\"url\")\n    if not url:\n        return {\n            \"status\": \"failed\",\n            \"error\": \"URL is required\",\n            \"error_type\": \"validation_error\",\n            \"field\": \"url\"\n        }\n\n    if not isinstance(url, str):\n        return {\n            \"status\": \"failed\",\n            \"error\": \"URL must be a string\",\n            \"error_type\": \"type_error\",\n            \"field\": \"url\",\n            \"received_type\": type(url).__name__\n        }\n\n    # Continue with execution\n    try:\n        result = await fetch_url(url)\n        return {\"status\": \"completed\", \"result\": result}\n    except TimeoutError:\n        return {\n            \"status\": \"failed\",\n            \"error\": f\"Request to {url} timed out\",\n            \"error_type\": \"timeout_error\"\n        }\n</code></pre></p>"},{"location":"guides/custom-tasks/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/custom-tasks/#pattern-1-http-api-call","title":"Pattern 1: HTTP API Call","text":"<pre><code>import aiohttp\nfrom apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any, Literal, Optional\nfrom pydantic import BaseModel, Field\n\nclass APICallInputSchema(BaseModel):\n    \"\"\"Input schema for API call executor\"\"\"\n    url: str = Field(description=\"API URL\")\n    method: Literal[\"GET\", \"POST\"] = Field(default=\"GET\", description=\"HTTP method\")\n    headers: Optional[Dict[str, str]] = Field(default=None, description=\"HTTP headers\")\n    timeout: int = Field(default=30, description=\"Timeout in seconds\")\n\n@executor_register()\nclass APICallExecutor(BaseTask):\n    \"\"\"Calls an external HTTP API\"\"\"\n\n    id = \"api_call_executor\"\n    name = \"API Call Executor\"\n    description = \"Calls an external HTTP API\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = APICallInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        url = inputs.get(\"url\")\n        method = inputs.get(\"method\", \"GET\")\n        headers = inputs.get(\"headers\", {})\n        timeout = inputs.get(\"timeout\", 30)\n\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.request(\n                    method,\n                    url,\n                    headers=headers,\n                    timeout=aiohttp.ClientTimeout(total=timeout)\n                ) as response:\n                    data = await response.json() if response.content_type == \"application/json\" else await response.text()\n\n                    return {\n                        \"status\": \"completed\",\n                        \"status_code\": response.status,\n                        \"data\": data\n                    }\n        except Exception as e:\n            return {\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"error_type\": type(e).__name__\n            }\n</code></pre>"},{"location":"guides/custom-tasks/#pattern-2-data-processing","title":"Pattern 2: Data Processing","text":"<pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any, Literal\nfrom pydantic import BaseModel, Field\n\nclass DataProcessorInputSchema(BaseModel):\n    \"\"\"Input schema for data processor\"\"\"\n    data: list[float] = Field(description=\"Array of numbers\")\n    operation: Literal[\"sum\", \"average\", \"max\", \"min\"] = Field(\n        default=\"sum\", description=\"Operation to perform\"\n    )\n\n@executor_register()\nclass DataProcessor(BaseTask):\n    \"\"\"Processes data\"\"\"\n\n    id = \"data_processor\"\n    name = \"Data Processor\"\n    description = \"Processes data with various operations\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = DataProcessorInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        data = inputs.get(\"data\", [])\n        operation = inputs.get(\"operation\", \"sum\")\n\n        if operation == \"sum\":\n            result = sum(data)\n        elif operation == \"average\":\n            result = sum(data) / len(data) if data else 0\n        elif operation == \"max\":\n            result = max(data) if data else None\n        elif operation == \"min\":\n            result = min(data) if data else None\n        else:\n            return {\n                \"status\": \"failed\",\n                \"error\": f\"Unknown operation: {operation}\",\n                \"error_type\": \"validation_error\"\n            }\n\n        return {\n            \"status\": \"completed\",\n            \"operation\": operation,\n            \"result\": result,\n            \"input_count\": len(data)\n        }\n</code></pre>"},{"location":"guides/custom-tasks/#pattern-3-file-operations","title":"Pattern 3: File Operations","text":"<pre><code>import aiofiles\nfrom apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any\nfrom pydantic import BaseModel, Field\n\nclass FileReaderInputSchema(BaseModel):\n    \"\"\"Input schema for file reader\"\"\"\n    file_path: str = Field(description=\"Path to file\")\n\n@executor_register()\nclass FileReader(BaseTask):\n    \"\"\"Reads files\"\"\"\n\n    id = \"file_reader\"\n    name = \"File Reader\"\n    description = \"Reads content from files\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = FileReaderInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        file_path = inputs.get(\"file_path\")\n\n        try:\n            self.check_input_schema(inputs)\n        except ValueError as e:\n            return {\"status\": \"failed\", \"error\": str(e), \"error_type\": \"validation_error\"}\n\n        try:\n            async with aiofiles.open(file_path, 'r') as f:\n                content = await f.read()\n\n            return {\n                \"status\": \"completed\",\n                \"file_path\": file_path,\n                \"content\": content,\n                \"size\": len(content)\n            }\n        except FileNotFoundError:\n            return {\n                \"status\": \"failed\",\n                \"error\": f\"File not found: {file_path}\",\n                \"error_type\": \"file_not_found\"\n            }\n        except Exception as e:\n            return {\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"error_type\": type(e).__name__\n            }\n</code></pre>"},{"location":"guides/custom-tasks/#pattern-4-database-query","title":"Pattern 4: Database Query","text":"<pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any, Optional\nfrom pydantic import BaseModel, Field\n\nclass DBQueryInputSchema(BaseModel):\n    \"\"\"Input schema for database query\"\"\"\n    query: str = Field(description=\"SQL query\")\n    params: Optional[Dict[str, Any]] = Field(default=None, description=\"Query parameters\")\n\n@executor_register()\nclass DatabaseQuery(BaseTask):\n    \"\"\"Executes database queries\"\"\"\n\n    id = \"db_query\"\n    name = \"Database Query\"\n    description = \"Executes database queries\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = DBQueryInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        query = inputs.get(\"query\")\n        params = inputs.get(\"params\", {})\n\n        try:\n            self.check_input_schema(inputs)\n        except ValueError as e:\n            return {\"status\": \"failed\", \"error\": str(e), \"error_type\": \"validation_error\"}\n\n        try:\n            # Execute query\n            # result = await self.db.fetch(query, params)\n            result = []  # Placeholder\n\n            return {\n                \"status\": \"completed\",\n                \"rows\": result,\n                \"count\": len(result)\n            }\n        except Exception as e:\n            return {\n                \"status\": \"failed\",\n                \"error\": str(e),\n                \"error_type\": \"database_error\"\n            }\n</code></pre>"},{"location":"guides/custom-tasks/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/custom-tasks/#cancellation-support","title":"Cancellation Support","text":"<p>Implement cancellation for long-running tasks:</p> <pre><code>class CancellableTask(BaseTask):\n    cancelable: bool = True  # Mark as cancellable\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        self._cancelled = False\n\n        for i in range(100):\n            # Check for cancellation\n            if self._cancelled:\n                return {\n                    \"status\": \"cancelled\",\n                    \"message\": \"Task was cancelled\",\n                    \"progress\": i\n                }\n\n            # Do work\n            await asyncio.sleep(0.1)\n\n        return {\"status\": \"completed\", \"result\": \"done\"}\n\n    async def cancel(self) -&gt; Dict[str, Any]:\n        \"\"\"Cancel task execution\"\"\"\n        self._cancelled = True\n        return {\n            \"status\": \"cancelled\",\n            \"message\": \"Cancellation requested\"\n        }\n</code></pre> <p>Note: Not all executors need cancellation. Only implement if your task can be safely cancelled.</p>"},{"location":"guides/custom-tasks/#accessing-task-context","title":"Accessing Task Context","text":"<p>Access task information through the executor:</p> <pre><code>class ContextAwareTask(BaseTask):\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        # Task context is available through TaskManager\n        # You can access it via hooks or by storing task reference\n\n        # Example: Access task ID (if available)\n        # task_id = getattr(self, '_task_id', None)\n\n        return {\"status\": \"completed\"}\n</code></pre>"},{"location":"guides/custom-tasks/#best-practices_1","title":"Best Practices","text":""},{"location":"guides/custom-tasks/#1-keep-tasks-focused","title":"1. Keep Tasks Focused","text":"<p>Good: <pre><code>class FetchUserData(BaseTask):\n    # Only fetches user data\n</code></pre></p> <p>Bad: <pre><code>class DoEverything(BaseTask):\n    # Fetches, processes, saves, sends notifications, etc.\n</code></pre></p>"},{"location":"guides/custom-tasks/#2-validate-inputs-early","title":"2. Validate Inputs Early","text":"<pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    # Validate at the start\n    url = inputs.get(\"url\")\n    if not url:\n        return {\"status\": \"failed\", \"error\": \"URL is required\"}\n\n    if not isinstance(url, str):\n        return {\"status\": \"failed\", \"error\": \"URL must be a string\"}\n\n    # Continue with execution\n</code></pre>"},{"location":"guides/custom-tasks/#3-use-async-properly","title":"3. Use Async Properly","text":"<p>Good: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            data = await response.json()\n    return {\"status\": \"completed\", \"data\": data}\n</code></pre></p> <p>Bad: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    import requests\n    response = requests.get(url)  # Blocking!\n    return {\"status\": \"completed\", \"data\": response.json()}\n</code></pre></p>"},{"location":"guides/custom-tasks/#4-document-your-tasks","title":"4. Document Your Tasks","text":"<pre><code>class MyCustomTask(BaseTask):\n    \"\"\"\n    Custom task that performs specific operations.\n\n    This task processes input data and returns processed results.\n    It supports various processing modes and configurations.\n\n    Example:\n        task = create_task(\n            name=\"my_custom_task\",\n            inputs={\"data\": [1, 2, 3], \"mode\": \"sum\"}\n        )\n    \"\"\"\n</code></pre>"},{"location":"guides/custom-tasks/#5-return-consistent-results","title":"5. Return Consistent Results","text":"<pre><code># Good: Consistent format\nreturn {\n    \"status\": \"completed\",\n    \"result\": result,\n    \"metadata\": {...}\n}\n\n# Bad: Inconsistent format\nreturn result  # Sometimes just the result\nreturn {\"data\": result}  # Sometimes wrapped\n</code></pre>"},{"location":"guides/custom-tasks/#testing","title":"Testing","text":""},{"location":"guides/custom-tasks/#unit-testing","title":"Unit Testing","text":"<p>Test your executor in isolation:</p> <pre><code>import pytest\nfrom my_executors import GreetingExecutor\n\n@pytest.mark.asyncio\nasync def test_greeting_executor():\n    executor = GreetingExecutor()\n\n    # Test with valid inputs\n    result = await executor.execute({\n        \"name\": \"Alice\",\n        \"language\": \"en\"\n    })\n\n    assert result[\"status\"] == \"completed\"\n    assert \"Hello, Alice!\" in result[\"greeting\"]\n\n    # Test with default language\n    result = await executor.execute({\"name\": \"Bob\"})\n    assert result[\"language\"] == \"en\"\n\n    # Test with invalid language\n    result = await executor.execute({\n        \"name\": \"Charlie\",\n        \"language\": \"invalid\"\n    })\n    # Should handle gracefully\n</code></pre>"},{"location":"guides/custom-tasks/#integration-testing","title":"Integration Testing","text":"<p>Test with TaskManager:</p> <pre><code>import pytest\nfrom apflow import TaskManager, TaskTreeNode, create_session\nfrom my_executors import GreetingExecutor\n\n@pytest.mark.asyncio\nasync def test_executor_integration():\n    # Import to register\n    from my_executors import GreetingExecutor\n\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create and execute task\n    task = await task_manager.task_repository.create_task(\n        name=\"greeting_executor\",\n        user_id=\"test_user\",\n        inputs={\"name\": \"Test User\", \"language\": \"en\"}\n    )\n\n    task_tree = TaskTreeNode(task)\n    await task_manager.distribute_task_tree(task_tree)\n\n    # Verify result\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    assert result.status == \"completed\"\n    assert \"Test User\" in result.result[\"greeting\"]\n</code></pre>"},{"location":"guides/custom-tasks/#built-in-executors","title":"Built-in Executors","text":"<p>apflow provides several built-in executors for common use cases. These executors are automatically registered and can be used directly in your tasks.</p>"},{"location":"guides/custom-tasks/#httprest-api-executor","title":"HTTP/REST API Executor","text":"<p>Execute HTTP requests to external APIs, webhooks, and HTTP-based services.</p> <p>Installation: <pre><code># httpx is included in a2a extra\npip install apflow[a2a]\n</code></pre></p> <p>Usage: <pre><code>{\n    \"schemas\": {\n        \"method\": \"rest_executor\"\n    },\n    \"inputs\": {\n        \"url\": \"https://api.example.com/users\",\n        \"method\": \"GET\",\n        \"headers\": {\"Authorization\": \"Bearer token\"},\n        \"timeout\": 30.0\n    }\n}\n</code></pre></p> <p>Features: - Supports GET, POST, PUT, DELETE, PATCH methods - Authentication: Bearer token, Basic auth, API key - Custom headers and query parameters - JSON and form data support - SSL verification control</p>"},{"location":"guides/custom-tasks/#ssh-remote-executor","title":"SSH Remote Executor","text":"<p>Execute commands on remote servers via SSH.</p> <p>Installation: <pre><code>pip install apflow[ssh]\n</code></pre></p> <p>Usage: <pre><code>{\n    \"schemas\": {\n        \"method\": \"ssh_executor\"\n    },\n    \"inputs\": {\n        \"host\": \"example.com\",\n        \"username\": \"user\",\n        \"key_file\": \"/path/to/key\",\n        \"command\": \"ls -la\",\n        \"timeout\": 30\n    }\n}\n</code></pre></p> <p>Features: - Password and key-based authentication - Environment variable support - Automatic key file permission validation - Command timeout handling</p>"},{"location":"guides/custom-tasks/#docker-container-executor","title":"Docker Container Executor","text":"<p>Execute commands in isolated Docker containers.</p> <p>Installation: <pre><code>pip install apflow[docker]\n</code></pre></p> <p>Usage: <pre><code>{\n    \"schemas\": {\n        \"method\": \"docker_executor\"\n    },\n    \"inputs\": {\n        \"image\": \"python:3.11\",\n        \"command\": \"python -c 'print(\\\"Hello\\\")'\",\n        \"env\": {\"KEY\": \"value\"},\n        \"volumes\": {\"/host/path\": \"/container/path\"},\n        \"resources\": {\"cpu\": \"1.0\", \"memory\": \"512m\"}\n    }\n}\n</code></pre></p> <p>Features: - Custom Docker images - Environment variables - Volume mounts - Resource limits (CPU, memory) - Automatic container cleanup</p>"},{"location":"guides/custom-tasks/#grpc-executor","title":"gRPC Executor","text":"<p>Call gRPC services and microservices.</p> <p>Installation: <pre><code>pip install apflow[grpc]\n</code></pre></p> <p>Usage: <pre><code>{\n    \"schemas\": {\n        \"method\": \"grpc_executor\"\n    },\n    \"inputs\": {\n        \"server\": \"localhost:50051\",\n        \"service\": \"Greeter\",\n        \"method\": \"SayHello\",\n        \"request\": {\"name\": \"World\"},\n        \"timeout\": 30.0\n    }\n}\n</code></pre></p> <p>Features: - Dynamic proto loading support - Custom metadata - Timeout handling - Error handling</p>"},{"location":"guides/custom-tasks/#websocket-executor","title":"WebSocket Executor","text":"<p>Bidirectional WebSocket communication.</p> <p>Installation: <pre><code># websockets is included in a2a extra\npip install apflow[a2a]\n</code></pre></p> <p>Usage: <pre><code>{\n    \"schemas\": {\n        \"method\": \"websocket_executor\"\n    },\n    \"inputs\": {\n        \"url\": \"ws://example.com/ws\",\n        \"message\": \"Hello\",\n        \"wait_response\": true,\n        \"timeout\": 30.0\n    }\n}\n</code></pre></p> <p>Features: - Send and receive messages - JSON message support - Configurable response waiting - Connection timeout handling</p>"},{"location":"guides/custom-tasks/#apflow-api-executor","title":"apflow API Executor","text":"<p>Call other apflow API instances for distributed execution.</p> <p>Installation: <pre><code># httpx is included in a2a extra\npip install apflow[a2a]\n</code></pre></p> <p>Usage: <pre><code>{\n    \"schemas\": {\n        \"method\": \"apflow_api_executor\"\n    },\n    \"inputs\": {\n        \"base_url\": \"http://remote-instance:8000\",\n        \"method\": \"tasks.execute\",\n        \"params\": {\"task_id\": \"task-123\"},\n        \"auth_token\": \"eyJ...\",\n        \"wait_for_completion\": true\n    }\n}\n</code></pre></p> <p>Features: - All task management methods (tasks.execute, tasks.create, tasks.get, etc.) - JWT authentication support - Task completion polling - Streaming support - Distributed execution scenarios</p> <p>Use Cases: - Distributed task execution across multiple instances - Service orchestration - Load balancing - Cross-environment task execution</p>"},{"location":"guides/custom-tasks/#mcp-model-context-protocol-executor","title":"MCP (Model Context Protocol) Executor","text":"<p>Interact with MCP servers to access external tools and data sources through the standardized MCP protocol.</p> <p>Installation: <pre><code># MCP executor uses standard library for stdio mode\n# For HTTP mode, httpx is included in a2a extra\npip install apflow[a2a]  # For HTTP transport\n# Or just use stdio mode (no additional dependencies)\n</code></pre></p> <p>Transport Modes:</p> <ol> <li>stdio - Local process communication (no dependencies)</li> <li>http - Remote server communication (requires httpx from [a2a] extra)</li> </ol> <p>Operations: - <code>list_tools</code>: List available MCP tools - <code>call_tool</code>: Call a tool with arguments - <code>list_resources</code>: List available resources - <code>read_resource</code>: Read a resource by URI</p> <p>Usage Examples:</p> <p>stdio Transport - List Tools: <pre><code>{\n    \"schemas\": {\n        \"method\": \"mcp_executor\"\n    },\n    \"inputs\": {\n        \"transport\": \"stdio\",\n        \"command\": [\"python\", \"-m\", \"mcp_server\"],\n        \"operation\": \"list_tools\"\n    }\n}\n</code></pre></p> <p>stdio Transport - Call Tool: <pre><code>{\n    \"schemas\": {\n        \"method\": \"mcp_executor\"\n    },\n    \"inputs\": {\n        \"transport\": \"stdio\",\n        \"command\": [\"python\", \"-m\", \"mcp_server\"],\n        \"operation\": \"call_tool\",\n        \"tool_name\": \"search_web\",\n        \"arguments\": {\n            \"query\": \"Python async programming\"\n        }\n    }\n}\n</code></pre></p> <p>stdio Transport - Read Resource: <pre><code>{\n    \"schemas\": {\n        \"method\": \"mcp_executor\"\n    },\n    \"inputs\": {\n        \"transport\": \"stdio\",\n        \"command\": [\"python\", \"-m\", \"mcp_server\"],\n        \"operation\": \"read_resource\",\n        \"resource_uri\": \"file:///path/to/file.txt\"\n    }\n}\n</code></pre></p> <p>HTTP Transport - List Tools: <pre><code>{\n    \"schemas\": {\n        \"method\": \"mcp_executor\"\n    },\n    \"inputs\": {\n        \"transport\": \"http\",\n        \"url\": \"http://localhost:8000/mcp\",\n        \"operation\": \"list_tools\",\n        \"headers\": {\n            \"Authorization\": \"Bearer token\"\n        }\n    }\n}\n</code></pre></p> <p>HTTP Transport - Call Tool: <pre><code>{\n    \"schemas\": {\n        \"method\": \"mcp_executor\"\n    },\n    \"inputs\": {\n        \"transport\": \"http\",\n        \"url\": \"http://localhost:8000/mcp\",\n        \"operation\": \"call_tool\",\n        \"tool_name\": \"search_web\",\n        \"arguments\": {\n            \"query\": \"Python async\"\n        },\n        \"timeout\": 30.0\n    }\n}\n</code></pre></p> <p>Configuration Options: - <code>transport</code>: \"stdio\" or \"http\" (required) - <code>operation</code>: \"list_tools\", \"call_tool\", \"list_resources\", \"read_resource\" (required) - For stdio:   - <code>command</code>: List of strings for MCP server command (required)   - <code>env</code>: Optional environment variables dict   - <code>cwd</code>: Optional working directory - For http:   - <code>url</code>: MCP server URL (required)   - <code>headers</code>: Optional HTTP headers dict - For call_tool:   - <code>tool_name</code>: Tool name (required)   - <code>arguments</code>: Tool arguments dict (required) - For read_resource:   - <code>resource_uri</code>: Resource URI (required) - <code>timeout</code>: Operation timeout in seconds (default: 30.0)</p> <p>Features: - Support for stdio and HTTP transport modes - JSON-RPC 2.0 protocol compliance - Tool and resource access - Environment variable injection (stdio) - Custom headers (HTTP) - Timeout and cancellation support - Comprehensive error handling</p> <p>Use Cases: - Access external tools via MCP servers - Read data from MCP resources - Integrate with MCP-compatible services - Local and remote MCP server communication</p>"},{"location":"guides/custom-tasks/#email-executor","title":"Email Executor","text":"<p>Send emails via Resend API or SMTP protocol.</p> <p>Installation: <pre><code>pip install apflow[email]\n</code></pre></p> <p>Usage (Resend): <pre><code>{\n    \"schemas\": {\n        \"method\": \"send_email_executor\"\n    },\n    \"inputs\": {\n        \"provider\": \"resend\",\n        \"api_key\": \"re_xxx\",\n        \"to\": [\"user@example.com\"],\n        \"from_email\": \"noreply@example.com\",\n        \"subject\": \"Order Confirmation\",\n        \"html\": \"&lt;h1&gt;Thank you for your order!&lt;/h1&gt;\"\n    }\n}\n</code></pre></p> <p>Usage (SMTP): <pre><code>{\n    \"schemas\": {\n        \"method\": \"send_email_executor\"\n    },\n    \"inputs\": {\n        \"provider\": \"smtp\",\n        \"smtp_host\": \"smtp.example.com\",\n        \"smtp_port\": 587,\n        \"smtp_username\": \"user\",\n        \"smtp_password\": \"pass\",\n        \"to\": [\"user@example.com\"],\n        \"from_email\": \"noreply@example.com\",\n        \"subject\": \"Hello\",\n        \"body\": \"Hello World\"\n    }\n}\n</code></pre></p> <p>Features: - Two providers: Resend (HTTP API) and SMTP - Plain text and HTML email support - CC, BCC, and reply-to recipients - Configurable timeout - SMTP STARTTLS support (enabled by default)</p>"},{"location":"guides/custom-tasks/#llm-executor-llm_executor","title":"LLM Executor (<code>llm_executor</code>)","text":"<p>Direct LLM interaction via LiteLLM, supporting over 100+ providers including OpenAI, Anthropic, Google Gemini, and many others.</p> <p>Installation: <pre><code>pip install apflow[llm]\n</code></pre></p> <p>Features: - Unified Model Access: Use a single interface to interact with any LLM provider. - Streaming Support: Built-in support for real-time streaming results (SSE). - Auto-Config: Automatically handles API keys from environment variables or project-level configuration. - LiteLLM Power: Leverages LiteLLM for robust, production-ready LLM interactions.</p> <p>Usage: <pre><code># Create task using LLM executor\ntask = await task_manager.task_repository.create_task(\n    name=\"llm_executor\",\n    user_id=\"user123\",\n    inputs={\n        \"model\": \"gpt-4o\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Explain quantum entanglement in one sentence.\"}\n        ]\n    }\n)\n</code></pre></p> <p>Input Schema: - <code>model</code>: (required) Model name (provider-prefixed if needed, e.g., <code>gpt-4o</code>, <code>claude-3-5-sonnet-20240620</code>) - <code>messages</code>: (required) Array of message objects (role and content) - <code>stream</code>: (optional) Enable streaming (default: false) - <code>temperature</code>: (optional) Controls randomness (default: 1.0) - <code>max_tokens</code>: (optional) Maximum generation length - <code>api_base</code>: (optional) Custom API base URL - <code>api_key</code>: (optional) Override API key (can be passed via <code>X-LLM-API-KEY</code> header in API)</p>"},{"location":"guides/custom-tasks/#task-tree-generator-executor","title":"Task Tree Generator Executor","text":"<p>Generate valid task tree JSON arrays from natural language requirements using LLM.</p> <p>Installation: <pre><code># Install LLM provider package (choose one)\npip install openai\n# or\npip install anthropic\n</code></pre></p> <p>Usage: <pre><code>{\n    \"schemas\": {\n        \"method\": \"generate_executor\"\n    },\n    \"inputs\": {\n        \"requirement\": \"Fetch data from API, process it, and save to database\",\n        \"user_id\": \"user123\",\n        \"llm_provider\": \"openai\",  # Optional: \"openai\" or \"anthropic\"\n        \"model\": \"gpt-4\",  # Optional: model name\n        \"temperature\": 0.7,  # Optional: LLM temperature\n        \"max_tokens\": 4000  # Optional: maximum tokens\n    }\n}\n</code></pre></p> <p>Features: - Automatically collects available executors and their schemas - Loads framework documentation as LLM context - Generates valid task trees compatible with <code>TaskCreator.create_task_tree_from_array()</code> - Comprehensive validation ensures generated tasks meet all requirements - Supports OpenAI and Anthropic LLM providers - Configurable via environment variables or input parameters</p> <p>Configuration: - <code>OPENAI_API_KEY</code> or <code>ANTHROPIC_API_KEY</code>: LLM API key (environment variable) - <code>OPENAI_MODEL</code> (for OpenAI): Model name (default: \"gpt-4o\") - <code>ANTHROPIC_MODEL</code> (for Anthropic): Model name (default: \"claude-3-5-sonnet-20241022\") - <code>APFLOW_LLM_PROVIDER</code>: Provider selection (default: \"openai\")</p> <p>Output: Returns a JSON array of task objects that can be used with <code>TaskCreator.create_task_tree_from_array()</code>: <pre><code>{\n    \"status\": \"completed\",\n    \"tasks\": [\n        {\n            \"name\": \"rest_executor\",\n            \"inputs\": {\"url\": \"https://api.example.com/data\", \"method\": \"GET\"},\n            \"priority\": 1\n        },\n        {\n            \"name\": \"command_executor\",\n            \"parent_id\": \"task_1\",\n            \"dependencies\": [{\"id\": \"task_1\", \"required\": True}],\n            \"inputs\": {\"command\": \"python process_data.py\"},\n            \"priority\": 2\n        }\n    ],\n    \"count\": 2\n}\n</code></pre></p> <p>Use Cases: - Automatically generate task trees from natural language requirements - Rapid prototyping of workflows - Converting business requirements into executable task structures - Learning tool for understanding task tree patterns</p> <p>CLI Usage Examples:</p> <p>The following examples demonstrate the intelligent task tree generation capabilities:</p> <pre><code># 1. Parallel Workflow - Fetch from multiple APIs in parallel\napflow generate task-tree \"Fetch data from two different APIs in parallel, then merge the results and save to database\"\n\n# 2. ETL Pipeline - Extract, Transform, Load workflow\napflow generate task-tree \"Extract data from REST API, transform it by filtering and aggregating, then load it into database\"\n\n# 3. Multi-Source Data Collection - Parallel system monitoring\napflow generate task-tree \"Collect system information about CPU and memory in parallel, analyze the data, and aggregate results\"\n\n# 4. Complex Processing Flow - Multi-step data processing\napflow generate task-tree \"Call REST API to get user data, process response with Python script, validate processed data, and save to file\"\n\n# 5. Fan-Out Fan-In Pattern - Parallel processing with convergence\napflow generate task-tree \"Fetch data from API, process it in two different ways in parallel (filter and aggregate), merge both results, and save to database\"\n\n# 6. Complete Business Scenario - Real-world monitoring workflow\napflow generate task-tree \"Monitor system resources (CPU, memory, disk) in parallel, analyze metrics, generate report, and send notification if threshold exceeded\"\n\n# 7. Data Pipeline - Multi-source processing\napflow generate task-tree \"Download data from multiple sources simultaneously, transform each dataset independently, then combine all results into single output file\"\n\n# 8. Hierarchical Processing - Category-based organization\napflow generate task-tree \"Fetch data from API, organize into categories, process each category in parallel, then aggregate all category results\"\n\n# 9. Complex Workflow - Complete business process\napflow generate task-tree \"Fetch customer data from API, validate information, process orders in parallel for each customer, aggregate results, calculate totals, and generate final report\"\n\n# 10. With custom LLM parameters\napflow generate task-tree \"Create a workflow\" --temperature 0.9 --max-tokens 6000 --provider openai --model gpt-4o\n\n# 11. Save to database\napflow generate task-tree \"My requirement\" --save --user-id user123\n</code></pre> <p>Tips for Better Results: - Be specific: More detailed requirements lead to better task trees - Mention patterns: Use words like \"parallel\", \"sequential\", \"merge\", \"aggregate\" to guide generation - Specify executors: Mention specific operations (API, database, file, command) for better executor selection - Describe flow: Explain the data flow and execution order in your requirement - Save to file: Use <code>--output tasks.json</code> to save generated tasks for later use</p>"},{"location":"guides/custom-tasks/#summary","title":"Summary","text":"<p>All built-in executors follow the same pattern: 1. Inherit from <code>BaseTask</code> 2. Registered with <code>@executor_register()</code> 3. Support cancellation (where applicable) 4. Provide input schema validation 5. Return consistent result format</p> <p>You can use these executors directly in your task schemas or extend them for custom behavior.</p>"},{"location":"guides/custom-tasks/#next-steps","title":"Next Steps","text":"<ul> <li>Task Orchestration Guide - Learn how to orchestrate multiple tasks</li> <li>Basic Examples - More practical examples</li> <li>Best Practices Guide - Advanced techniques</li> <li>API Reference - Complete API documentation</li> </ul> <p>Need help? Check the FAQ or Quick Start Guide</p>"},{"location":"guides/distributed-cluster/","title":"Distributed Cluster Guide","text":"<p>This guide explains how to run apflow as a distributed cluster for high availability and horizontal scaling of task execution.</p>"},{"location":"guides/distributed-cluster/#overview","title":"Overview","text":"<p>By default, apflow runs as a single process using DuckDB for storage. The distributed cluster mode enables multi-node task execution with centralized coordination:</p> <ul> <li>Lease-based task assignment with automatic expiry and reassignment</li> <li>Automatic leader election via PostgreSQL (no external coordination service)</li> <li>Horizontal scaling by adding worker nodes without code changes</li> <li>Failure recovery through lease expiry and automatic task reassignment</li> </ul> <p>Use distributed mode when you need: - High availability for task execution - Parallel execution across multiple machines - Automatic failover when nodes go down - Scaling beyond a single process</p>"},{"location":"guides/distributed-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>PostgreSQL is required for distributed mode (DuckDB does not support multi-node coordination)</li> <li>All nodes must connect to the same PostgreSQL database</li> <li>Network connectivity between nodes is not required (coordination is database-driven)</li> </ul> <pre><code># Install with PostgreSQL support\npip install apflow[postgres]\n</code></pre> <p>Set the database URL for all nodes:</p> <pre><code>export APFLOW_DATABASE_URL=postgresql+asyncpg://user:pass@db-host/apflow\n</code></pre>"},{"location":"guides/distributed-cluster/#quick-start","title":"Quick Start","text":""},{"location":"guides/distributed-cluster/#1-start-a-leader-node","title":"1. Start a leader node","text":"<pre><code>export APFLOW_CLUSTER_ENABLED=true\nexport APFLOW_DATABASE_URL=postgresql+asyncpg://user:pass@db-host/apflow\nexport APFLOW_NODE_ROLE=auto\nexport APFLOW_NODE_ID=node-leader-1\n\napflow serve --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"guides/distributed-cluster/#2-start-a-worker-node","title":"2. Start a worker node","text":"<p>On a second machine (or process):</p> <pre><code>export APFLOW_CLUSTER_ENABLED=true\nexport APFLOW_DATABASE_URL=postgresql+asyncpg://user:pass@db-host/apflow\nexport APFLOW_NODE_ROLE=worker\nexport APFLOW_NODE_ID=node-worker-1\n\napflow serve --host 0.0.0.0 --port 8001\n</code></pre> <p>The worker will automatically register, start heartbeating, and begin polling for tasks.</p>"},{"location":"guides/distributed-cluster/#3-submit-tasks","title":"3. Submit tasks","text":"<p>Submit tasks to the leader node. The cluster distributes execution across available workers:</p> <pre><code>curl -X POST http://leader-host:8000/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.execute\",\n    \"params\": {\n      \"tasks\": [\n        {\"id\": \"task-1\", \"name\": \"my_task\", \"inputs\": {\"key\": \"value\"}}\n      ]\n    },\n    \"id\": \"1\"\n  }'\n</code></pre>"},{"location":"guides/distributed-cluster/#configuration-reference","title":"Configuration Reference","text":"<p>All distributed configuration is loaded from environment variables. Set <code>APFLOW_CLUSTER_ENABLED=true</code> to activate distributed mode.</p>"},{"location":"guides/distributed-cluster/#cluster-identity","title":"Cluster Identity","text":"Variable Type Default Description <code>APFLOW_CLUSTER_ENABLED</code> boolean <code>false</code> Enable distributed cluster mode <code>APFLOW_NODE_ID</code> string auto-generated Unique identifier for this node. Auto-generated as <code>node-&lt;random&gt;</code> if not set <code>APFLOW_NODE_ROLE</code> string <code>auto</code> Node role: <code>auto</code>, <code>leader</code>, <code>worker</code>, or <code>observer</code>"},{"location":"guides/distributed-cluster/#leader-election","title":"Leader Election","text":"Variable Type Default Description <code>APFLOW_LEADER_LEASE</code> integer <code>30</code> Leader lease duration in seconds. If the leader fails to renew within this window, another node can claim leadership <code>APFLOW_LEADER_RENEW</code> integer <code>10</code> How often the leader renews its lease (seconds). Must be less than <code>APFLOW_LEADER_LEASE</code>"},{"location":"guides/distributed-cluster/#task-lease-management","title":"Task Lease Management","text":"Variable Type Default Description <code>APFLOW_LEASE_DURATION</code> integer <code>30</code> Task lease duration in seconds. Workers must complete or renew within this window <code>APFLOW_LEASE_CLEANUP_INTERVAL</code> integer <code>10</code> How often the leader checks for expired task leases (seconds)"},{"location":"guides/distributed-cluster/#worker-polling","title":"Worker Polling","text":"Variable Type Default Description <code>APFLOW_POLL_INTERVAL</code> integer <code>5</code> How often workers poll for new tasks (seconds) <code>APFLOW_MAX_PARALLEL_TASKS</code> integer <code>4</code> Maximum concurrent tasks per worker node"},{"location":"guides/distributed-cluster/#node-health-monitoring","title":"Node Health Monitoring","text":"Variable Type Default Description <code>APFLOW_HEARTBEAT_INTERVAL</code> integer <code>10</code> How often nodes send heartbeat signals (seconds) <code>APFLOW_NODE_STALE_THRESHOLD</code> integer <code>30</code> Seconds without heartbeat before a node is marked <code>stale</code> <code>APFLOW_NODE_DEAD_THRESHOLD</code> integer <code>120</code> Seconds without heartbeat before a node is marked <code>dead</code> and its tasks are reassigned"},{"location":"guides/distributed-cluster/#node-roles","title":"Node Roles","text":"<p>All nodes run the same codebase. The role determines behavior at runtime.</p>"},{"location":"guides/distributed-cluster/#auto-default","title":"<code>auto</code> (default)","text":"<p>The node attempts leader election on startup. If another leader already holds the lease, it falls back to worker mode. This is the recommended setting for most deployments.</p>"},{"location":"guides/distributed-cluster/#leader","title":"<code>leader</code>","text":"<p>Forces the node to act as the cluster leader. Fails on startup if leadership cannot be acquired.</p> <p>Responsibilities: - Owns task state writes in PostgreSQL - Handles lease acquisition, renewal, and reassignment - Runs cleanup for expired leases - Serves read/write API endpoints</p>"},{"location":"guides/distributed-cluster/#worker","title":"<code>worker</code>","text":"<p>The node never attempts to become leader. It only executes tasks.</p> <p>Responsibilities: - Polls for executable tasks - Acquires a task lease, executes the task, and reports the result - Renews lease during long-running tasks - Sends periodic heartbeats</p>"},{"location":"guides/distributed-cluster/#observer","title":"<code>observer</code>","text":"<p>Read-only mode. Useful for dashboards, CLI access, or monitoring endpoints.</p> <p>Responsibilities: - Serves read-only API endpoints - Does not execute tasks or participate in leader election</p>"},{"location":"guides/distributed-cluster/#architecture","title":"Architecture","text":""},{"location":"guides/distributed-cluster/#component-overview","title":"Component Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  PostgreSQL                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Tasks   \u2502 \u2502  Nodes   \u2502 \u2502 Leader Lease     \u2502  \u2502\n\u2502  \u2502  Table   \u2502 \u2502 Registry \u2502 \u2502 Table            \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502            \u2502            \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Leader  \u2502  \u2502 Worker  \u2502  \u2502 Worker  \u2502\n    \u2502 Node    \u2502  \u2502 Node 1  \u2502  \u2502 Node 2  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/distributed-cluster/#execution-flow","title":"Execution Flow","text":"<ol> <li>Task submitted to the leader via API</li> <li>Leader writes the task to PostgreSQL with status <code>pending</code></li> <li>Workers poll for pending tasks matching their capabilities</li> <li>Worker acquires lease on a task (atomic database operation)</li> <li>Worker executes the task, renewing the lease periodically</li> <li>Worker reports result back to PostgreSQL</li> <li>Leader cleans up expired leases and reassigns failed tasks</li> </ol>"},{"location":"guides/distributed-cluster/#leader-election_1","title":"Leader Election","text":"<p>Leader election uses a SQL-based lease mechanism (no external coordination needed):</p> <ol> <li>On startup, nodes with role <code>auto</code> or <code>leader</code> attempt to insert a row into <code>cluster_leader</code></li> <li>The first successful insert wins leadership</li> <li>The leader renews its lease every <code>APFLOW_LEADER_RENEW</code> seconds</li> <li>If the lease expires (leader crash), any node can claim leadership</li> <li>On graceful shutdown, the leader releases its lease</li> </ol>"},{"location":"guides/distributed-cluster/#deployment-patterns","title":"Deployment Patterns","text":""},{"location":"guides/distributed-cluster/#simple-high-availability-2-nodes","title":"Simple High Availability (2 nodes)","text":"<p>Both nodes set <code>APFLOW_NODE_ROLE=auto</code>. One becomes leader, the other becomes worker. If the leader fails, the worker promotes itself.</p> <pre><code># Node A\nAPFLOW_CLUSTER_ENABLED=true\nAPFLOW_NODE_ROLE=auto\nAPFLOW_NODE_ID=node-a\n\n# Node B\nAPFLOW_CLUSTER_ENABLED=true\nAPFLOW_NODE_ROLE=auto\nAPFLOW_NODE_ID=node-b\n</code></pre>"},{"location":"guides/distributed-cluster/#auto-scaling-workers","title":"Auto-Scaling Workers","text":"<p>One dedicated leader with auto-scaling workers. Add workers dynamically without configuration changes.</p> <pre><code># Leader (fixed)\nAPFLOW_CLUSTER_ENABLED=true\nAPFLOW_NODE_ROLE=leader\nAPFLOW_NODE_ID=leader-1\n\n# Workers (auto-scaled, e.g., in Kubernetes)\nAPFLOW_CLUSTER_ENABLED=true\nAPFLOW_NODE_ROLE=worker\n# APFLOW_NODE_ID auto-generated per instance\nAPFLOW_MAX_PARALLEL_TASKS=8\n</code></pre>"},{"location":"guides/distributed-cluster/#leader-workers-observers","title":"Leader + Workers + Observers","text":"<p>Full deployment with dedicated roles for separation of concerns.</p> <pre><code># Leader: handles coordination\nAPFLOW_NODE_ROLE=leader\n\n# Workers: execute tasks\nAPFLOW_NODE_ROLE=worker\nAPFLOW_MAX_PARALLEL_TASKS=4\n\n# Observers: serve dashboard/CLI\nAPFLOW_NODE_ROLE=observer\n</code></pre>"},{"location":"guides/distributed-cluster/#failure-handling","title":"Failure Handling","text":""},{"location":"guides/distributed-cluster/#worker-crash","title":"Worker Crash","text":"<ol> <li>The worker stops sending heartbeats</li> <li>After <code>APFLOW_NODE_STALE_THRESHOLD</code> seconds (default: 30), the node is marked <code>stale</code></li> <li>After <code>APFLOW_NODE_DEAD_THRESHOLD</code> seconds (default: 120), the node is marked <code>dead</code></li> <li>Task leases held by the dead node expire automatically</li> <li>The leader's cleanup loop detects expired leases and marks those tasks as <code>pending</code></li> <li>Another worker picks up the task on its next poll</li> </ol>"},{"location":"guides/distributed-cluster/#leader-failover","title":"Leader Failover","text":"<ol> <li>The leader stops renewing its lease (crash or network partition)</li> <li>After <code>APFLOW_LEADER_LEASE</code> seconds (default: 30), the lease expires</li> <li>A node with role <code>auto</code> detects the expired lease and claims leadership</li> <li>The new leader resumes lease cleanup and task coordination</li> </ol>"},{"location":"guides/distributed-cluster/#task-lease-expiry","title":"Task Lease Expiry","text":"<p>If a worker takes longer than <code>APFLOW_LEASE_DURATION</code> to complete a task without renewing:</p> <ol> <li>The lease expires</li> <li>The leader's cleanup loop marks the task for reassignment</li> <li>Another worker can acquire the task</li> </ol> <p>Workers automatically renew leases for long-running tasks. Increase <code>APFLOW_LEASE_DURATION</code> if tasks routinely take longer than 30 seconds.</p>"},{"location":"guides/distributed-cluster/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/distributed-cluster/#no-leader-elected","title":"No leader elected","text":"<p>Symptoms: All nodes are workers, no task coordination happening.</p> <p>Possible causes: - All nodes have <code>APFLOW_NODE_ROLE=worker</code> (no node is attempting leader election) - Database connectivity issues preventing lease table writes</p> <p>Fix: Set at least one node to <code>APFLOW_NODE_ROLE=auto</code> or <code>APFLOW_NODE_ROLE=leader</code>.</p>"},{"location":"guides/distributed-cluster/#tasks-stuck-in-pending","title":"Tasks stuck in pending","text":"<p>Symptoms: Tasks remain in <code>pending</code> status indefinitely.</p> <p>Possible causes: - No worker nodes are running - Workers don't have the required executor installed - <code>APFLOW_MAX_PARALLEL_TASKS</code> is reached on all workers</p> <p>Fix: Check worker logs, verify executors are registered, or add more workers.</p>"},{"location":"guides/distributed-cluster/#frequent-task-reassignment","title":"Frequent task reassignment","text":"<p>Symptoms: Tasks are being reassigned repeatedly.</p> <p>Possible causes: - <code>APFLOW_LEASE_DURATION</code> is too short for the task execution time - Worker nodes are overloaded and failing to renew leases</p> <p>Fix: Increase <code>APFLOW_LEASE_DURATION</code> or reduce <code>APFLOW_MAX_PARALLEL_TASKS</code>.</p>"},{"location":"guides/distributed-cluster/#node-marked-as-dead-but-still-running","title":"Node marked as dead but still running","text":"<p>Symptoms: A node is functional but marked <code>dead</code> in the node registry.</p> <p>Possible causes: - Network partition between the node and PostgreSQL - Database connection pool exhausted</p> <p>Fix: Check database connectivity. The node will re-register on its next successful heartbeat.</p>"},{"location":"guides/distributed-cluster/#see-also","title":"See Also","text":"<ul> <li>Environment Variables Reference - All configuration variables</li> <li>API Server Guide - Running the API server</li> <li>Installation - Installing PostgreSQL support</li> </ul>"},{"location":"guides/environment-variables/","title":"Environment Variables Reference","text":"<p>This document describes all environment variables used by apflow.</p>"},{"location":"guides/environment-variables/#naming-convention","title":"Naming Convention","text":"<p>apflow follows a consistent naming pattern for environment variables:</p> <ul> <li>Preferred: Variables with <code>APFLOW_</code> prefix (e.g., <code>APFLOW_LOG_LEVEL</code>)</li> <li>Fallback: Generic names without prefix (e.g., <code>LOG_LEVEL</code>)</li> </ul> <p>When both are set, <code>APFLOW_*</code> variables take precedence. This design allows apflow to: - Run in multi-service environments without conflicts - Integrate with existing systems that use generic variable names - Maintain clear ownership of configuration</p>"},{"location":"guides/environment-variables/#core-configuration","title":"Core Configuration","text":""},{"location":"guides/environment-variables/#database","title":"Database","text":"Variable Type Default Description <code>APFLOW_DATABASE_URL</code> string auto-detected Database connection string. Takes precedence over <code>DATABASE_URL</code> <code>DATABASE_URL</code> string auto-detected Fallback database connection string <p>Priority for DuckDB file location: 1. <code>APFLOW_DATABASE_URL</code> or <code>DATABASE_URL</code> (if set) 2. <code>.data/apflow.duckdb</code> (if exists in project) 3. <code>~/.aipartnerup/data/apflow.duckdb</code> (if exists, legacy) 4. <code>.data/apflow.duckdb</code> (default for new projects) 5. <code>~/.aipartnerup/data/apflow.duckdb</code> (default outside projects)</p> <p>Examples: <pre><code># PostgreSQL\nAPFLOW_DATABASE_URL=postgresql+asyncpg://user:pass@localhost/apflow\n\n# Custom DuckDB path\nAPFLOW_DATABASE_URL=duckdb:///path/to/custom.duckdb\n\n# Using generic fallback\nDATABASE_URL=postgresql+asyncpg://user:pass@localhost/apflow\n</code></pre></p>"},{"location":"guides/environment-variables/#logging","title":"Logging","text":"Variable Type Default Description <code>APFLOW_LOG_LEVEL</code> string INFO Log level for apflow. Takes precedence over <code>LOG_LEVEL</code> <code>LOG_LEVEL</code> string INFO Fallback log level <p>Valid values: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code>, <code>ERROR</code>, <code>CRITICAL</code></p> <p>Examples: <pre><code># Use apflow-specific log level\nAPFLOW_LOG_LEVEL=DEBUG\n\n# Or generic fallback\nLOG_LEVEL=INFO\n</code></pre></p>"},{"location":"guides/environment-variables/#api-server","title":"API Server","text":"Variable Type Default Description <code>APFLOW_API_HOST</code> string 0.0.0.0 API server host address. Takes precedence over <code>API_HOST</code> <code>API_HOST</code> string 0.0.0.0 Fallback API host <code>APFLOW_API_PORT</code> integer 8000 API server port. Takes precedence over <code>API_PORT</code> <code>API_PORT</code> integer 8000 Fallback API port <code>APFLOW_BASE_URL</code> string auto Base URL for API service <code>APFLOW_API_PROTOCOL</code> string a2a API protocol type: <code>a2a</code>, <code>mcp</code>, or <code>graphql</code> <p>Examples: <pre><code># Use apflow-specific configuration\nAPFLOW_API_HOST=127.0.0.1\nAPFLOW_API_PORT=9000\nAPFLOW_API_PROTOCOL=mcp\n# Or GraphQL\nAPFLOW_API_PROTOCOL=graphql\n\n# Or generic fallback\nAPI_HOST=0.0.0.0\nAPI_PORT=8000\n</code></pre></p>"},{"location":"guides/environment-variables/#security","title":"Security","text":"Variable Type Default Description <code>APFLOW_JWT_SECRET</code> string - Secret key for JWT token signing <code>APFLOW_JWT_ALGORITHM</code> string HS256 JWT signing algorithm <p>Example: <pre><code>APFLOW_JWT_SECRET=your-secret-key-here\nAPFLOW_JWT_ALGORITHM=HS256\n</code></pre></p>"},{"location":"guides/environment-variables/#cors","title":"CORS","text":"Variable Type Default Description <code>APFLOW_CORS_ORIGINS</code> string * Comma-separated allowed CORS origins <code>APFLOW_CORS_ALLOW_ALL</code> boolean false Allow all CORS origins <p>Examples: <pre><code># Specific origins\nAPFLOW_CORS_ORIGINS=http://localhost:3000,https://app.example.com\n\n# Allow all (development only)\nAPFLOW_CORS_ALLOW_ALL=true\n</code></pre></p>"},{"location":"guides/environment-variables/#api-features","title":"API Features","text":"Variable Type Default Description <code>APFLOW_ENABLE_SYSTEM_ROUTES</code> boolean true Enable system information routes <code>APFLOW_ENABLE_DOCS</code> boolean true Enable API documentation routes"},{"location":"guides/environment-variables/#cli-configuration","title":"CLI Configuration","text":"Variable Type Default Description <code>APFLOW_CONFIG_DIR</code> string auto-detected Override CLI config directory location <p>Default locations (priority order): 1. <code>APFLOW_CONFIG_DIR</code> (if set) 2. <code>.data/</code> (if in project) 3. <code>~/.aipartnerup/apflow/</code></p>"},{"location":"guides/environment-variables/#storage-configuration","title":"Storage Configuration","text":"Variable Type Default Description <code>APFLOW_MAX_SESSIONS</code> integer 10 Maximum concurrent storage sessions <code>APFLOW_SESSION_TIMEOUT</code> integer 300 Session timeout in seconds <code>APFLOW_TASK_TABLE_NAME</code> string tasks Custom task table name <code>APFLOW_TASK_MODEL_CLASS</code> string - Custom task model class path"},{"location":"guides/environment-variables/#extensions","title":"Extensions","text":"Variable Type Default Description <code>APFLOW_EXTENSIONS</code> string - Comma-separated extensions by directory name to load (also restricts available executors for security) <code>APFLOW_EXTENSIONS_IDS</code> string - Comma-separated extension IDs to load (also restricts available executors for security) <code>APFLOW_LLM_PROVIDER</code> string - LLM provider for AI extensions <p>Example: <pre><code># Load only stdio and http extensions (security: only these executors are accessible)\nAPFLOW_EXTENSIONS=stdio,http\nAPFLOW_EXTENSIONS_IDS=system_info_executor,rest_executor\nAPFLOW_LLM_PROVIDER=openai\n</code></pre></p> <p>Security Note: When <code>APFLOW_EXTENSIONS</code> is set, only executors from the specified extensions can be accessed via API endpoints (<code>tasks.execute</code>, <code>tasks.generate</code>). This provides access control to restrict which executors users can invoke. If not set, all installed executors are available.</p>"},{"location":"guides/environment-variables/#extension-specific-variables","title":"Extension-Specific Variables","text":""},{"location":"guides/environment-variables/#stdio-extension","title":"STDIO Extension","text":"Variable Type Default Description <code>APFLOW_STDIO_ALLOW_COMMAND</code> boolean false Allow arbitrary command execution <code>APFLOW_STDIO_COMMAND_WHITELIST</code> string - Comma-separated allowed commands <p>Example: <pre><code>APFLOW_STDIO_ALLOW_COMMAND=true\nAPFLOW_STDIO_COMMAND_WHITELIST=echo,ls,cat\n</code></pre></p>"},{"location":"guides/environment-variables/#email-extension","title":"Email Extension","text":"<p>The email extension supports sending emails via Resend API or SMTP. Environment variables provide default configuration that can be overridden by task inputs.</p> Variable Type Default Description <code>RESEND_API_KEY</code> string - Resend API key for cloud email sending <code>SMTP_HOST</code> string - SMTP server hostname <code>SMTP_PORT</code> integer 587 SMTP server port <code>SMTP_USERNAME</code> string - SMTP authentication username <code>SMTP_PASSWORD</code> string - SMTP authentication password <code>SMTP_USE_TLS</code> string true Whether to use STARTTLS (\"true\"/\"false\") <code>FROM_EMAIL</code> string - Default sender email address (shared by both providers) <p>Provider Auto-Detection: - If <code>RESEND_API_KEY</code> is set, provider defaults to <code>resend</code> - If <code>SMTP_HOST</code> is set (and no Resend key), provider defaults to <code>smtp</code></p> <p>Example (Resend): <pre><code>RESEND_API_KEY=re_xxxxxxxxxxxxx\nFROM_EMAIL=noreply@example.com\n</code></pre></p> <p>Example (SMTP with Gmail): <pre><code>SMTP_HOST=smtp.gmail.com\nSMTP_PORT=587\nSMTP_USERNAME=your-email@gmail.com\nSMTP_PASSWORD=your-app-password\nSMTP_USE_TLS=true\nFROM_EMAIL=your-email@gmail.com\n</code></pre></p> <p>See also: Email Executor Guide for detailed usage examples.</p>"},{"location":"guides/environment-variables/#scheduler-webhooks","title":"Scheduler &amp; Webhooks","text":"Variable Type Default Description <code>APFLOW_WEBHOOK_SECRET</code> string - HMAC secret key for webhook signature validation <code>APFLOW_WEBHOOK_ALLOWED_IPS</code> string - Comma-separated IP addresses allowed to trigger webhooks <code>APFLOW_WEBHOOK_RATE_LIMIT</code> integer 0 Max webhook requests per minute (0=unlimited) <p>Example: <pre><code>APFLOW_WEBHOOK_SECRET=your-webhook-secret\nAPFLOW_WEBHOOK_ALLOWED_IPS=10.0.0.1,10.0.0.2\nAPFLOW_WEBHOOK_RATE_LIMIT=60\n</code></pre></p>"},{"location":"guides/environment-variables/#daemon","title":"Daemon","text":"Variable Type Default Description <code>APFLOW_DAEMON_PID_FILE</code> string auto Custom daemon PID file location <code>APFLOW_DAEMON_LOG_FILE</code> string auto Custom daemon log file location"},{"location":"guides/environment-variables/#distributed-cluster","title":"Distributed Cluster","text":"<p>These variables configure distributed cluster mode. Set <code>APFLOW_CLUSTER_ENABLED=true</code> to activate. Requires PostgreSQL. See the Distributed Cluster Guide for details.</p>"},{"location":"guides/environment-variables/#cluster-identity","title":"Cluster Identity","text":"Variable Type Default Description <code>APFLOW_CLUSTER_ENABLED</code> boolean false Enable distributed cluster mode <code>APFLOW_NODE_ID</code> string auto-generated Unique identifier for this node <code>APFLOW_NODE_ROLE</code> string auto Node role: <code>auto</code>, <code>leader</code>, <code>worker</code>, or <code>observer</code>"},{"location":"guides/environment-variables/#leader-election","title":"Leader Election","text":"Variable Type Default Description <code>APFLOW_LEADER_LEASE</code> integer 30 Leader lease duration in seconds <code>APFLOW_LEADER_RENEW</code> integer 10 Leader lease renewal interval in seconds"},{"location":"guides/environment-variables/#task-lease-management","title":"Task Lease Management","text":"Variable Type Default Description <code>APFLOW_LEASE_DURATION</code> integer 30 Task lease duration in seconds <code>APFLOW_LEASE_CLEANUP_INTERVAL</code> integer 10 Expired lease cleanup interval in seconds"},{"location":"guides/environment-variables/#worker-polling","title":"Worker Polling","text":"Variable Type Default Description <code>APFLOW_POLL_INTERVAL</code> integer 5 Worker task poll interval in seconds <code>APFLOW_MAX_PARALLEL_TASKS</code> integer 4 Maximum concurrent tasks per worker node"},{"location":"guides/environment-variables/#node-health-monitoring","title":"Node Health Monitoring","text":"Variable Type Default Description <code>APFLOW_HEARTBEAT_INTERVAL</code> integer 10 Heartbeat signal interval in seconds <code>APFLOW_NODE_STALE_THRESHOLD</code> integer 30 Seconds without heartbeat before node is <code>stale</code> <code>APFLOW_NODE_DEAD_THRESHOLD</code> integer 120 Seconds without heartbeat before node is <code>dead</code> <p>Example: <pre><code>APFLOW_CLUSTER_ENABLED=true\nAPFLOW_NODE_ID=node-1\nAPFLOW_NODE_ROLE=auto\nAPFLOW_LEADER_LEASE=30\nAPFLOW_MAX_PARALLEL_TASKS=8\n</code></pre></p>"},{"location":"guides/environment-variables/#development-testing","title":"Development &amp; Testing","text":"Variable Type Default Description <code>APFLOW_DEMO_SLEEP_SCALE</code> float 1.0 Scale factor for demo sleep times <p>Example: <pre><code># Speed up demos by 10x\nAPFLOW_DEMO_SLEEP_SCALE=0.1\n</code></pre></p>"},{"location":"guides/environment-variables/#third-party-service-keys","title":"Third-Party Service Keys","text":"<p>These variables follow the standard naming conventions of third-party services and should not have <code>APFLOW_</code> prefix:</p> Variable Service Description <code>OPENAI_API_KEY</code> OpenAI API key for OpenAI services <code>OPENAI_MODEL</code> OpenAI Default OpenAI model name <code>ANTHROPIC_API_KEY</code> Anthropic API key for Anthropic services <code>ANTHROPIC_MODEL</code> Anthropic Default Anthropic model name <p>Example: <pre><code>OPENAI_API_KEY=sk-...\nOPENAI_MODEL=gpt-4\nANTHROPIC_API_KEY=sk-ant-...\nANTHROPIC_MODEL=claude-3-opus-20240229\n</code></pre></p>"},{"location":"guides/environment-variables/#complete-example","title":"Complete Example","text":"<p>Here's a complete <code>.env</code> file example:</p> <pre><code># Database (choose one)\nAPFLOW_DATABASE_URL=duckdb:///.data/apflow.duckdb\n# APFLOW_DATABASE_URL=postgresql+asyncpg://user:pass@localhost/apflow\n\n# Logging\nAPFLOW_LOG_LEVEL=INFO\n\n# API Server\nAPFLOW_API_HOST=0.0.0.0\nAPFLOW_API_PORT=8000\nAPFLOW_API_PROTOCOL=a2a\n\n# Security\nAPFLOW_JWT_SECRET=your-secret-key-change-in-production\nAPFLOW_JWT_ALGORITHM=HS256\n\n# CORS (adjust for production)\nAPFLOW_CORS_ORIGINS=http://localhost:3000,https://app.example.com\n\n# Features\nAPFLOW_ENABLE_SYSTEM_ROUTES=true\nAPFLOW_ENABLE_DOCS=true\n\n# Distributed Cluster (optional)\n# APFLOW_CLUSTER_ENABLED=true\n# APFLOW_NODE_ROLE=auto\n# APFLOW_MAX_PARALLEL_TASKS=4\n\n# LLM Services (if using AI extensions)\nOPENAI_API_KEY=sk-...\nOPENAI_MODEL=gpt-4\n</code></pre>"},{"location":"guides/environment-variables/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>.env</code> file: Store environment variables in a <code>.env</code> file in your project root</li> <li>Never commit secrets: Add <code>.env</code> to <code>.gitignore</code></li> <li>Use APFLOW_ prefix: Prefer <code>APFLOW_*</code> variables for better isolation</li> <li>Document overrides: When using generic fallbacks, document why</li> <li>Validate in production: Always validate required variables are set in production</li> </ol>"},{"location":"guides/environment-variables/#priority-summary","title":"Priority Summary","text":"<p>When multiple configuration sources exist, apflow follows this priority:</p> <ol> <li>Environment variables with APFLOW_ prefix (highest)</li> <li>Generic environment variables (fallback)</li> <li>CLI config files (<code>.data/</code> or <code>~/.aipartnerup/apflow/</code>)</li> <li>Default values (lowest)</li> </ol> <p>This allows maximum flexibility while maintaining sensible defaults.</p>"},{"location":"guides/executor-selection/","title":"Executor Selection Guide","text":"<p>Choose the right executor for your task. This guide helps you decide which built-in executor to use or when to create a custom one.</p>"},{"location":"guides/executor-selection/#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>What does your task need to do?\n\u2502\n\u251c\u2500 Call an HTTP/REST API?\n\u2502  \u2514\u2500 Use: rest_executor\n\u2502     Example: Fetch data from third-party API, trigger webhooks\n\u2502\n\u251c\u2500 Execute commands on a remote server?\n\u2502  \u251c\u2500 Via SSH \u2192 Use: ssh_executor\n\u2502  \u2502  Example: Deploy code, run maintenance scripts\n\u2502  \u2514\u2500 In a container \u2192 Use: docker_executor\n\u2502     Example: Run isolated workloads, reproducible environments\n\u2502\n\u251c\u2500 Call a gRPC service?\n\u2502  \u2514\u2500 Use: grpc_executor\n\u2502     Example: Microservice communication, high-performance RPC\n\u2502\n\u251c\u2500 Use AI agents?\n\u2502  \u251c\u2500 Multi-agent collaboration \u2192 Use: crewai_executor\n\u2502  \u2502  Example: Research team, content generation workflow\n\u2502  \u251c\u2500 Single LLM call \u2192 Use: litellm_executor\n\u2502  \u2502  Example: Text generation, completion, chat\n\u2502  \u2514\u2500 Use MCP tools \u2192 Use: mcp_executor\n\u2502     Example: Call Model Context Protocol services\n\u2502\n\u251c\u2500 Real-time bidirectional communication?\n\u2502  \u2514\u2500 Use: websocket_executor\n\u2502     Example: Live updates, streaming data\n\u2502\n\u2514\u2500 Custom business logic?\n   \u2514\u2500 Create: Custom Executor\n      Example: Domain-specific operations, internal services\n</code></pre>"},{"location":"guides/executor-selection/#executor-comparison","title":"Executor Comparison","text":"Executor Use Case Pros Cons Installation REST HTTP API calls Simple, universal, no dependencies No persistent connections Built-in SSH Remote command execution Full server access, scriptable Requires SSH credentials <code>pip install apflow[ssh]</code> Docker Containerized execution Isolated, reproducible Requires Docker daemon <code>pip install apflow[docker]</code> gRPC High-performance RPC Fast, type-safe, efficient Requires proto definitions <code>pip install apflow[grpc]</code> WebSocket Bidirectional streaming Real-time, low latency Connection management Built-in CrewAI Multi-agent AI workflows AI-native, collaborative agents Requires API keys (OpenAI) <code>pip install apflow[crewai]</code> LiteLLM LLM API calls Supports 100+ models API costs <code>pip install apflow[llm]</code> MCP MCP tool calls Standard protocol, composable Requires MCP servers <code>pip install apflow[mcp]</code>"},{"location":"guides/executor-selection/#detailed-scenarios","title":"Detailed Scenarios","text":""},{"location":"guides/executor-selection/#scenario-1-call-third-party-api","title":"Scenario 1: Call Third-Party API","text":"<p>Executor: <code>rest_executor</code></p> <p>Example: Fetch weather data</p> <pre><code>from apflow import TaskBuilder, execute_tasks\n\ntask = TaskBuilder(\"fetch_weather\", \"rest_executor\")\\\n    .with_inputs({\n        \"url\": \"https://api.weather.com/v1/forecast\",\n        \"method\": \"GET\",\n        \"params\": {\"city\": \"San Francisco\"},\n        \"headers\": {\"Authorization\": \"Bearer YOUR_TOKEN\"}\n    })\\\n    .build()\n\nresult = execute_tasks([task])\nprint(result[\"json\"])\n</code></pre> <p>When to use: - Calling REST APIs (GET, POST, PUT, DELETE) - Webhook notifications - Third-party service integration</p> <p>Alternatives: - gRPC executor (if service supports gRPC) - Custom executor (if complex authentication or business logic needed)</p>"},{"location":"guides/executor-selection/#scenario-2-execute-commands-on-remote-server","title":"Scenario 2: Execute Commands on Remote Server","text":"<p>Executor: <code>ssh_executor</code></p> <p>Example: Deploy application</p> <pre><code>task = TaskBuilder(\"deploy_app\", \"ssh_executor\")\\\n    .with_inputs({\n        \"host\": \"prod-server.example.com\",\n        \"port\": 22,\n        \"username\": \"deploy\",\n        \"key_file\": \"~/.ssh/deploy_key\",\n        \"command\": \"bash /opt/scripts/deploy.sh\",\n        \"timeout\": 300  # 5 minutes\n    })\\\n    .build()\n\nresult = execute_tasks([task])\nprint(f\"Exit code: {result['return_code']}\")\nprint(f\"Output: {result['stdout']}\")\n</code></pre> <p>When to use: - Remote deployments - Server maintenance - Running scripts on remote machines - System administration tasks</p> <p>Alternatives: - Docker executor (if containerized deployment) - REST executor (if server has API for deployments)</p>"},{"location":"guides/executor-selection/#scenario-3-run-isolated-workload","title":"Scenario 3: Run Isolated Workload","text":"<p>Executor: <code>docker_executor</code></p> <p>Example: Process data in Python container</p> <pre><code>task = TaskBuilder(\"process_data\", \"docker_executor\")\\\n    .with_inputs({\n        \"image\": \"python:3.11\",\n        \"command\": \"python /app/process.py\",\n        \"volumes\": {\n            \"/host/data\": \"/app/data\",\n            \"/host/results\": \"/app/results\"\n        },\n        \"env\": {\n            \"INPUT_FILE\": \"/app/data/input.csv\",\n            \"OUTPUT_FILE\": \"/app/results/output.csv\"\n        },\n        \"timeout\": 600\n    })\\\n    .build()\n\nresult = execute_tasks([task])\nprint(f\"Container logs: {result['logs']}\")\n</code></pre> <p>When to use: - Isolated execution environments - Reproducible workflows - Running code with specific dependencies - Batch processing</p> <p>Alternatives: - SSH executor (if remote execution acceptable) - Custom executor (if can run locally without isolation)</p>"},{"location":"guides/executor-selection/#scenario-4-high-performance-microservice-communication","title":"Scenario 4: High-Performance Microservice Communication","text":"<p>Executor: <code>grpc_executor</code></p> <p>Example: Call internal microservice</p> <pre><code>task = TaskBuilder(\"call_service\", \"grpc_executor\")\\\n    .with_inputs({\n        \"host\": \"service.internal\",\n        \"port\": 50051,\n        \"method\": \"GetUser\",\n        \"data\": {\"user_id\": \"12345\"}\n    })\\\n    .build()\n\nresult = execute_tasks([task])\nprint(result[\"response\"])\n</code></pre> <p>When to use: - Microservice architectures - Low-latency requirements - Type-safe APIs - Internal service communication</p> <p>Alternatives: - REST executor (if HTTP is acceptable) - Custom executor (if using different protocol)</p>"},{"location":"guides/executor-selection/#scenario-5-multi-agent-ai-workflow","title":"Scenario 5: Multi-Agent AI Workflow","text":"<p>Executor: <code>crewai_executor</code></p> <p>Example: Research and write article</p> <pre><code>task = TaskBuilder(\"write_article\", \"crewai_executor\")\\\n    .with_inputs({\n        \"crew_config\": {\n            \"agents\": [\n                {\n                    \"role\": \"Researcher\",\n                    \"goal\": \"Research topic thoroughly\",\n                    \"backstory\": \"Expert researcher\"\n                },\n                {\n                    \"role\": \"Writer\",\n                    \"goal\": \"Write engaging article\",\n                    \"backstory\": \"Professional writer\"\n                }\n            ],\n            \"tasks\": [\n                {\n                    \"description\": \"Research topic: {topic}\",\n                    \"agent\": \"Researcher\"\n                },\n                {\n                    \"description\": \"Write article based on research\",\n                    \"agent\": \"Writer\"\n                }\n            ]\n        },\n        \"inputs\": {\"topic\": \"Quantum Computing\"}\n    })\\\n    .build()\n\nresult = execute_tasks([task])\nprint(result[\"output\"])\n</code></pre> <p>When to use: - Multi-agent collaboration - Complex AI workflows - Research and content generation - Agent-based problem solving</p> <p>Alternatives: - LiteLLM executor (for single LLM calls) - Custom executor (for custom AI workflows)</p>"},{"location":"guides/executor-selection/#scenario-6-single-llm-api-call","title":"Scenario 6: Single LLM API Call","text":"<p>Executor: <code>litellm_executor</code></p> <p>Example: Generate text completion</p> <pre><code>task = TaskBuilder(\"generate_text\", \"litellm_executor\")\\\n    .with_inputs({\n        \"model\": \"gpt-4\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n            {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms\"}\n        ],\n        \"max_tokens\": 500\n    })\\\n    .build()\n\nresult = execute_tasks([task])\nprint(result[\"completion\"])\n</code></pre> <p>When to use: - Single LLM calls - Text generation - Chat completions - Model comparisons (supports 100+ models)</p> <p>Alternatives: - CrewAI executor (for multi-agent workflows) - Custom executor (for custom LLM integration)</p>"},{"location":"guides/executor-selection/#custom-executors","title":"Custom Executors","text":"<p>When built-in executors don't fit your needs:</p>"},{"location":"guides/executor-selection/#create-a-custom-executor","title":"Create a Custom Executor","text":"<pre><code>from apflow import executor_register\nfrom apflow.extensions import BaseTask\nfrom typing import Dict, Any\n\n@executor_register()\nclass MyCustomExecutor(BaseTask):\n    \"\"\"\n    Custom executor for domain-specific operations\n    \"\"\"\n    id = \"my_custom_executor\"\n    name = \"My Custom Executor\"\n    description = \"Handles custom business logic\"\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        # Your custom logic here\n        result = self.process_data(inputs[\"data\"])\n\n        return {\n            \"success\": True,\n            \"result\": result\n        }\n\n    def process_data(self, data):\n        # Custom processing\n        return f\"Processed: {data}\"\n</code></pre> <p>When to create custom executor: - Domain-specific business logic - Integration with internal systems - Custom authentication/authorization - Complex data transformations - When no built-in executor fits</p>"},{"location":"guides/executor-selection/#executor-capabilities-matrix","title":"Executor Capabilities Matrix","text":"Feature REST SSH Docker gRPC WebSocket CrewAI LiteLLM MCP Cancellation \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c Streaming \u274c \u274c \u274c \u26a0\ufe0f \u2705 \u26a0\ufe0f \u26a0\ufe0f \u274c Timeout \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Retries Manual Manual Manual Manual Manual Manual Manual Manual Authentication \u2705 \u2705 \u274c \u26a0\ufe0f \u26a0\ufe0f \u2705 \u2705 \u26a0\ufe0f <p>Legend: - \u2705 Fully supported - \u26a0\ufe0f Partially supported or requires configuration - \u274c Not supported</p>"},{"location":"guides/executor-selection/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/executor-selection/#latency","title":"Latency","text":"<p>Low latency (&lt; 100ms typical): - gRPC executor (binary protocol) - Custom executor (local)</p> <p>Medium latency (100ms - 1s): - REST executor (HTTP overhead) - WebSocket executor (after connection)</p> <p>High latency (&gt; 1s): - SSH executor (network + auth) - Docker executor (container startup) - AI executors (LLM API latency)</p>"},{"location":"guides/executor-selection/#throughput","title":"Throughput","text":"<p>High throughput: - gRPC executor (efficient serialization) - Custom executor (no network overhead)</p> <p>Medium throughput: - REST executor - WebSocket executor</p> <p>Lower throughput: - SSH executor (connection overhead) - Docker executor (container lifecycle) - AI executors (rate limits)</p>"},{"location":"guides/executor-selection/#security-considerations","title":"Security Considerations","text":"Executor Security Notes REST \u2705 Blocks private IPs by default (SSRF protection)\u26a0\ufe0f Verify SSL certificates\u26a0\ufe0f Handle secrets securely SSH \u2705 Validates key permissions\u26a0\ufe0f Use key-based auth (not passwords)\u26a0\ufe0f Restrict SSH access Docker \u26a0\ufe0f Runs with host Docker privileges\u26a0\ufe0f Validate image sources\u26a0\ufe0f Limit container resources gRPC \u26a0\ufe0f Implement authentication\u26a0\ufe0f Use TLS in production AI \u26a0\ufe0f Protect API keys\u26a0\ufe0f Monitor costs\u26a0\ufe0f Validate AI outputs"},{"location":"guides/executor-selection/#next-steps","title":"Next Steps","text":"<ul> <li>REST Executor Documentation</li> <li>SSH Executor Documentation</li> <li>Creating Custom Executors</li> <li>Task Dependencies</li> </ul>"},{"location":"guides/executor-selection/#quick-reference","title":"Quick Reference","text":"<pre><code># Install executor extras\npip install apflow[ssh]        # SSH executor\npip install apflow[docker]     # Docker executor\npip install apflow[grpc]       # gRPC executor\npip install apflow[crewai]     # CrewAI executor\npip install apflow[llm]        # LiteLLM executor\npip install apflow[mcp]        # MCP executor\npip install apflow[all]        # All executors\n\n# List available executors\napflow executors list\n\n# Get executor details\napflow executors get &lt;executor-id&gt;\n</code></pre>"},{"location":"guides/extensions/","title":"Extensions in apflow","text":"<p>apflow supports a powerful extension system for both CLI commands and core functionality. You can add your own extensions or override existing ones, making it easy to customize and adapt apflow to your needs.</p>"},{"location":"guides/extensions/#cli-extensions-custom-commands-and-overriding","title":"CLI Extensions: Custom Commands and Overriding","text":"<p>You can register new CLI command groups or single commands using the <code>@cli_register</code> decorator. You can also extend or override existing commands and groups by specifying the <code>group</code> and <code>override=True</code> parameters.</p> <p>For detailed usage and examples, see the CLI Guide and Library Usage Guide.</p>"},{"location":"guides/extensions/#core-extensions-custom-and-override","title":"Core Extensions: Custom and Override","text":"<p>apflow also supports custom extensions for executors, hooks, storage backends, and more. You can register your own or override built-in extensions by passing <code>override=True</code> when registering.</p> <p>For how to override executors and other extensions, see the API Quick Reference and Python API Reference.</p> <p>For more details, see the Custom Tasks Guide.</p>"},{"location":"guides/extensions/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>override=True</code> only when you want to replace an existing command or extension.</li> <li>Keep extension logic simple and well-documented.</li> <li>Test your extensions thoroughly.</li> </ul>"},{"location":"guides/faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Common questions and answers about apflow.</p>"},{"location":"guides/faq/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Getting Started</li> <li>Tasks and Executors</li> <li>Task Orchestration</li> <li>Troubleshooting</li> <li>Advanced Topics</li> </ol>"},{"location":"guides/faq/#getting-started","title":"Getting Started","text":""},{"location":"guides/faq/#q-how-do-i-install-apflow","title":"Q: How do I install apflow?","text":"<p>A: Use pip:</p> <pre><code># Minimal installation (core only)\npip install apflow\n\n# Full installation (all features)\npip install apflow[all]\n\n# Specific features\npip install apflow[crewai]  # LLM support\npip install apflow[cli]      # CLI tools\npip install apflow[a2a]      # A2A Protocol server\n</code></pre>"},{"location":"guides/faq/#q-do-i-need-to-set-up-a-database","title":"Q: Do I need to set up a database?","text":"<p>A: No! DuckDB is used by default and requires no setup. It just works out of the box.</p> <p>If you want to use PostgreSQL:</p> <pre><code>export DATABASE_URL=\"postgresql+asyncpg://user:password@localhost/dbname\"\n</code></pre>"},{"location":"guides/faq/#q-what-python-version-do-i-need","title":"Q: What Python version do I need?","text":"<p>A: Python 3.10 or higher (3.12+ recommended).</p>"},{"location":"guides/faq/#q-where-should-i-start","title":"Q: Where should I start?","text":"<p>A:  1. Quick Start Guide - Get running in 10 minutes 2. First Steps Tutorial - Complete beginner tutorial 3. Core Concepts - Understand the fundamentals</p>"},{"location":"guides/faq/#tasks-and-executors","title":"Tasks and Executors","text":""},{"location":"guides/faq/#q-whats-the-difference-between-a-task-and-an-executor","title":"Q: What's the difference between a Task and an Executor?","text":"<p>A: - Executor: The code that does the work (reusable template) - Task: An instance of work to be done (specific execution)</p> <p>Analogy: - Executor = A recipe (reusable) - Task = A specific meal made from the recipe (one-time)</p>"},{"location":"guides/faq/#q-how-do-i-create-a-custom-executor","title":"Q: How do I create a custom executor?","text":"<p>A: Use <code>BaseTask</code> with <code>@executor_register()</code>:</p> <pre><code>from apflow import BaseTask, executor_register\nfrom typing import ClassVar, Dict, Any\nfrom pydantic import BaseModel, Field\n\nclass MyInputSchema(BaseModel):\n    \"\"\"Input schema for my executor\"\"\"\n    param: str = Field(default=\"\", description=\"Parameter\")\n\n@executor_register()\nclass MyExecutor(BaseTask):\n    id = \"my_executor\"\n    name = \"My Executor\"\n    description = \"Does something\"\n\n    inputs_schema: ClassVar[type[BaseModel]] = MyInputSchema\n\n    async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        return {\"status\": \"completed\", \"result\": \"...\"}\n</code></pre> <p>See the Custom Tasks Guide for details.</p>"},{"location":"guides/faq/#q-how-do-i-use-my-custom-executor","title":"Q: How do I use my custom executor?","text":"<p>A:  1. Import it (this registers it automatically): <pre><code>from my_module import MyExecutor\n</code></pre></p> <ol> <li>Use it when creating tasks: <pre><code>task = await task_manager.task_repository.create_task(\n    name=\"my_executor\",  # Must match executor id\n    user_id=\"user123\",\n    inputs={...}\n)\n</code></pre></li> </ol>"},{"location":"guides/faq/#q-my-executor-isnt-being-found-whats-wrong","title":"Q: My executor isn't being found. What's wrong?","text":"<p>A: Common issues:</p> <ol> <li> <p>Not imported: Make sure you import the executor class: <pre><code>from my_module import MyExecutor  # This registers it\n</code></pre></p> </li> <li> <p>Wrong name: The <code>name</code> field must match the executor <code>id</code>: <pre><code># Executor\nid = \"my_executor\"\n\n# Task\nname=\"my_executor\"  # Must match!\n</code></pre></p> </li> <li> <p>Not registered: Make sure you use <code>@executor_register()</code>: <pre><code>@executor_register()  # Don't forget this!\nclass MyExecutor(BaseTask):\n    ...\n</code></pre></p> </li> </ol>"},{"location":"guides/faq/#q-can-i-use-built-in-executors","title":"Q: Can I use built-in executors?","text":"<p>A: Yes! Built-in executors are automatically available:</p> <p>Core Executors (always available): - <code>system_info_executor</code> - Get system information (CPU, memory, disk) - <code>command_executor</code> - Execute shell commands (requires security config) - <code>aggregate_results_executor</code> - Aggregate results from multiple tasks</p> <p>Remote Execution Executors: - <code>rest_executor</code> - HTTP/REST API calls (requires <code>pip install apflow[http]</code>) - <code>ssh_executor</code> - Remote command execution via SSH (requires <code>pip install apflow[ssh]</code>) - <code>grpc_executor</code> - gRPC service calls (requires <code>pip install apflow[grpc]</code>) - <code>websocket_executor</code> - Bidirectional WebSocket communication - <code>apflow_api_executor</code> - Call other apflow API instances - <code>mcp_executor</code> - Model Context Protocol executor (stdio: no dependencies, HTTP: requires <code>[a2a]</code>)</p> <p>MCP Server: - MCP Server exposes apflow task orchestration as MCP tools and resources - Start with: <code>APFLOW_API_PROTOCOL=mcp python -m apflow.api.main</code> - Provides 8 MCP tools: execute_task, create_task, get_task, update_task, delete_task, list_tasks, get_task_status, cancel_task - Provides 2 MCP resources: task://{task_id}, tasks:// - Supports both HTTP and stdio transport modes</p> <p>Container Executors: - <code>docker_executor</code> - Containerized command execution (requires <code>pip install apflow[docker]</code>)</p> <p>AI Executors (optional): - <code>crewai_executor</code> - LLM-based agents (requires <code>pip install apflow[crewai]</code>) - <code>batch_crewai_executor</code> - Batch execution of multiple crews (requires <code>pip install apflow[crewai]</code>)</p> <p>Generation Executors: - <code>generate_executor</code> - Generate task tree JSON arrays from natural language requirements using LLM (requires <code>pip install openai</code> or <code>pip install anthropic</code>)</p> <p>Just use them by name: <pre><code>task = await task_manager.task_repository.create_task(\n    name=\"system_info_executor\",\n    user_id=\"user123\",\n    inputs={\"resource\": \"cpu\"}\n)\n</code></pre></p> <p>For more details on all executors, see the Custom Tasks Guide.</p>"},{"location":"guides/faq/#task-orchestration","title":"Task Orchestration","text":""},{"location":"guides/faq/#q-whats-the-difference-between-parent_id-and-dependencies","title":"Q: What's the difference between <code>parent_id</code> and <code>dependencies</code>?","text":"<p>A: This is a critical distinction!</p> <ul> <li><code>parent_id</code>: Organizational only (like folders) - does NOT affect execution order</li> <li><code>dependencies</code>: Controls execution order - determines when tasks run</li> </ul> <p>Example: <pre><code># Task B is a child of Task A (organizational)\n# But Task B depends on Task C (execution order)\ntask_a = create_task(name=\"task_a\")\ntask_c = create_task(name=\"task_c\")\ntask_b = create_task(\n    name=\"task_b\",\n    parent_id=task_a.id,  # Organizational\n    dependencies=[{\"id\": task_c.id, \"required\": True}]  # Execution order\n)\n# Execution: C runs first, then B (regardless of parent-child)\n</code></pre></p> <p>See Task Orchestration Guide for details.</p>"},{"location":"guides/faq/#q-how-do-i-make-tasks-run-in-parallel","title":"Q: How do I make tasks run in parallel?","text":"<p>A: Don't add dependencies between them:</p> <pre><code># Task 1 (no dependencies)\ntask1 = create_task(name=\"task1\", ...)\n\n# Task 2 (no dependencies on task1)\ntask2 = create_task(name=\"task2\", ...)\n\n# Both run in parallel!\n</code></pre>"},{"location":"guides/faq/#q-how-do-i-make-tasks-run-sequentially","title":"Q: How do I make tasks run sequentially?","text":"<p>A: Add dependencies:</p> <pre><code># Task 1\ntask1 = create_task(name=\"task1\", ...)\n\n# Task 2 depends on Task 1\ntask2 = create_task(\n    name=\"task2\",\n    dependencies=[{\"id\": task1.id, \"required\": True}],\n    ...\n)\n\n# Execution order: Task 1 \u2192 Task 2\n</code></pre>"},{"location":"guides/faq/#q-how-do-priorities-work","title":"Q: How do priorities work?","text":"<p>A: Lower numbers = higher priority = execute first:</p> <pre><code>0 = Urgent (highest)\n1 = High\n2 = Normal (default)\n3 = Low (lowest)\n</code></pre> <p>Important: Dependencies take precedence over priorities!</p>"},{"location":"guides/faq/#q-my-task-is-stuck-in-pending-status-why","title":"Q: My task is stuck in \"pending\" status. Why?","text":"<p>A: Common causes:</p> <ol> <li>Dependencies not satisfied: Check if dependency tasks are completed</li> <li>Executor not found: Verify task name matches executor ID</li> <li>Task not executed: Make sure you called <code>distribute_task_tree()</code></li> <li>Parent task failed: Check parent task status</li> </ol> <p>Debug: <pre><code>task = await task_manager.task_repository.get_task_by_id(task_id)\nprint(f\"Status: {task.status}\")\nprint(f\"Error: {task.error}\")\nprint(f\"Dependencies: {task.dependencies}\")\n</code></pre></p>"},{"location":"guides/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/faq/#q-task-executor-not-found-error","title":"Q: Task executor not found error","text":"<p>Error: <code>Task executor not found: executor_id</code></p> <p>Solutions: 1. For built-in executors: Make sure you've imported the extension: <pre><code>import apflow.extensions.stdio  # Registers built-in executors\n</code></pre></p> <ol> <li>For custom executors: </li> <li>Make sure you used <code>@executor_register()</code></li> <li>Import the executor class in your main script</li> <li>Verify the <code>name</code> field matches the executor <code>id</code></li> </ol>"},{"location":"guides/faq/#q-database-connection-error","title":"Q: Database connection error","text":"<p>Error: Database connection issues</p> <p>Solutions: - DuckDB (default): No setup needed! It just works. - PostgreSQL: Set environment variable: <pre><code>export DATABASE_URL=\"postgresql+asyncpg://user:password@localhost/dbname\"\n</code></pre></p>"},{"location":"guides/faq/#q-import-error","title":"Q: Import error","text":"<p>Error: <code>ModuleNotFoundError: No module named 'apflow'</code></p> <p>Solution: <pre><code>pip install apflow\n</code></pre></p>"},{"location":"guides/faq/#q-task-stays-in-in_progress-forever","title":"Q: Task stays in \"in_progress\" forever","text":"<p>Problem: Task never completes</p> <p>Solutions: 1. Check if executor is hanging (infinite loop, waiting for resource) 2. Verify executor supports cancellation 3. Check for deadlocks in dependencies 4. Review executor implementation</p> <p>Debug: <pre><code># Check task status\ntask = await task_manager.task_repository.get_task_by_id(task_id)\nprint(f\"Status: {task.status}\")\nprint(f\"Error: {task.error}\")\n\n# Try cancelling\nif task.status == \"in_progress\":\n    await task_manager.cancel_task(task_id, \"Manual cancellation\")\n</code></pre></p>"},{"location":"guides/faq/#q-dependency-not-satisfied","title":"Q: Dependency not satisfied","text":"<p>Problem: Task waiting for dependency that never completes</p> <p>Solutions: 1. Verify dependency task ID is correct 2. Check if dependency task failed 3. Ensure dependency task is in the same tree 4. Check dependency task status</p> <p>Debug: <pre><code># Check dependency status\ndependency = await task_manager.task_repository.get_task_by_id(dependency_id)\nprint(f\"Dependency status: {dependency.status}\")\nprint(f\"Dependency error: {dependency.error}\")\n</code></pre></p>"},{"location":"guides/faq/#q-priority-not-working","title":"Q: Priority not working","text":"<p>Problem: Tasks not executing in expected order</p> <p>Solutions: 1. Verify priority values (lower = higher priority) 2. Check if dependencies override priority (they do!) 3. Ensure tasks are at the same level in the tree 4. Remember: dependencies take precedence!</p>"},{"location":"guides/faq/#advanced-topics","title":"Advanced Topics","text":""},{"location":"guides/faq/#q-how-do-i-use-crewai-llm-tasks","title":"Q: How do I use CrewAI (LLM tasks)?","text":"<p>A: </p> <ol> <li> <p>Install: <pre><code>pip install apflow[crewai]\n</code></pre></p> </li> <li> <p>Create a crew: <pre><code>from apflow.extensions.crewai import CrewaiExecutor\nfrom apflow.core.extensions import get_registry\n\ncrew = CrewaiExecutor(\n    id=\"my_crew\",\n    name=\"My Crew\",\n    agents=[{\"role\": \"Analyst\", \"goal\": \"Analyze data\"}],\n    tasks=[{\"description\": \"Analyze: {text}\", \"agent\": \"Analyst\"}]\n)\n\nget_registry().register(crew)\n</code></pre></p> </li> <li> <p>Use it: <pre><code>task = await task_manager.task_repository.create_task(\n    name=\"my_crew\",\n    user_id=\"user123\",\n    inputs={\"text\": \"Analyze this data\"}\n)\n</code></pre></p> </li> </ol> <p>Note: Requires LLM API key (OpenAI, Anthropic, etc.)</p>"},{"location":"guides/faq/#q-how-do-i-set-llm-api-keys","title":"Q: How do I set LLM API keys?","text":"<p>A: </p> <p>Option 1: Environment variable <pre><code>export OPENAI_API_KEY=\"sk-your-key\"\nexport ANTHROPIC_API_KEY=\"sk-ant-your-key\"\n</code></pre></p> <p>Option 2: Request header (for API server) <pre><code>X-LLM-API-KEY: openai:sk-your-key\n</code></pre></p> <p>Option 3: Configuration <pre><code># In your code\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-your-key\"\n</code></pre></p>"},{"location":"guides/faq/#q-how-do-i-handle-task-failures","title":"Q: How do I handle task failures?","text":"<p>A: </p> <ol> <li> <p>Check task status: <pre><code>task = await task_manager.task_repository.get_task_by_id(task_id)\nif task.status == \"failed\":\n    print(f\"Error: {task.error}\")\n</code></pre></p> </li> <li> <p>Handle in executor: <pre><code>async def execute(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    try:\n        result = perform_operation(inputs)\n        return {\"status\": \"completed\", \"result\": result}\n    except Exception as e:\n        return {\n            \"status\": \"failed\",\n            \"error\": str(e),\n            \"error_type\": type(e).__name__\n        }\n</code></pre></p> </li> <li> <p>Use optional dependencies for fallback: <pre><code># Primary task\nprimary = create_task(name=\"primary_task\", ...)\n\n# Fallback task (runs even if primary fails)\nfallback = create_task(\n    name=\"fallback_task\",\n    dependencies=[{\"id\": primary.id, \"required\": False}],  # Optional\n    ...\n)\n</code></pre></p> </li> </ol>"},{"location":"guides/faq/#q-can-i-cancel-a-running-task","title":"Q: Can I cancel a running task?","text":"<p>A: Yes, if the executor supports cancellation:</p> <pre><code>result = await task_manager.cancel_task(\n    task_id=\"task_123\",\n    error_message=\"User requested cancellation\"\n)\n</code></pre> <p>Note: Not all executors support cancellation. Check <code>executor.cancelable</code> property.</p>"},{"location":"guides/faq/#q-how-do-i-get-real-time-updates","title":"Q: How do I get real-time updates?","text":"<p>A: Use streaming execution:</p> <pre><code># Execute with streaming\nawait task_manager.distribute_task_tree_with_streaming(\n    task_tree,\n    use_callback=True\n)\n</code></pre> <p>Or use the API server with SSE (Server-Sent Events).</p>"},{"location":"guides/faq/#q-how-do-i-use-hooks","title":"Q: How do I use hooks?","text":"<p>A: </p> <p>Pre-execution hook: <pre><code>from apflow import register_pre_hook\n\n@register_pre_hook\nasync def validate_inputs(task):\n    \"\"\"Validate inputs before execution\"\"\"\n    if task.inputs and \"url\" in task.inputs:\n        url = task.inputs[\"url\"]\n        if not url.startswith((\"http://\", \"https://\")):\n            task.inputs[\"url\"] = f\"https://{url}\"\n</code></pre></p> <p>Post-execution hook: <pre><code>from apflow import register_post_hook\n\n@register_post_hook\nasync def log_results(task, inputs, result):\n    \"\"\"Log results after execution\"\"\"\n    print(f\"Task {task.id} completed:\")\n    print(f\"  Result: {result}\")\n</code></pre></p>"},{"location":"guides/faq/#q-how-do-i-extend-taskmodel","title":"Q: How do I extend TaskModel?","text":"<p>A: </p> <pre><code>from apflow.core.storage.sqlalchemy.models import TaskModel\nfrom apflow import set_task_model_class\nfrom sqlalchemy import Column, String\n\nclass CustomTaskModel(TaskModel):\n    \"\"\"Custom TaskModel with additional fields\"\"\"\n    __tablename__ = \"apflow_tasks\"\n\n    project_id = Column(String(255), nullable=True, index=True)\n\n# Set before creating tasks\nset_task_model_class(CustomTaskModel)\n\n# Now tasks can have project_id\ntask = await task_manager.task_repository.create_task(\n    name=\"my_task\",\n    user_id=\"user123\",\n    project_id=\"proj-123\",  # Custom field\n    inputs={...}\n)\n</code></pre>"},{"location":"guides/faq/#still-have-questions","title":"Still Have Questions?","text":"<ul> <li>Documentation Index - Browse all documentation</li> <li>Quick Start Guide - Get started quickly</li> <li>Examples - See practical examples</li> <li>GitHub Issues - Report bugs</li> <li>GitHub Discussions - Ask questions</li> </ul> <p>Found an issue? Report it on GitHub</p>"},{"location":"guides/library-usage/","title":"Using apflow as a Library","text":"<p>This guide shows how to use <code>apflow</code> as a library in your own project (e.g., <code>apflow-demo</code>) and customize it with your own routes, middleware, and configurations.</p>"},{"location":"guides/library-usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding main.py</li> <li>Basic Setup</li> <li>Database Configuration</li> <li>Custom Routes</li> <li>Custom Middleware</li> <li>Custom TaskRoutes</li> <li>Complete Example</li> </ul>"},{"location":"guides/library-usage/#understanding-mainpy","title":"Understanding main.py","text":"<p>When using <code>apflow</code> as a library, you need to understand what <code>main.py</code> does and why:</p>"},{"location":"guides/library-usage/#what-mainpy-does","title":"What main.py Does","text":"<p>The <code>main.py</code> file in apflow provides two main functions for library usage:</p> <ol> <li><code>create_runnable_app()</code> - Creates a fully initialized application instance</li> <li>Loads <code>.env</code> file from the calling project's directory (not library's directory)</li> <li>Initializes extensions (executors, hooks, storage backends)</li> <li>Loads custom TaskModel if specified</li> <li>Auto-initializes examples if database is empty</li> <li>Creates the API application with proper configuration</li> <li> <p>Returns the app instance (you run the server yourself)</p> </li> <li> <p><code>main()</code> - Complete entry point that handles everything and runs the server</p> </li> <li>Does everything <code>create_runnable_app()</code> does</li> <li>Additionally runs the uvicorn server with configurable parameters</li> </ol>"},{"location":"guides/library-usage/#why-you-need-these-steps","title":"Why You Need These Steps","text":"<ul> <li>Extensions initialization: Without this, executors (like <code>crewai_executor</code>, <code>command_executor</code>) won't be available</li> <li>Custom TaskModel: Allows you to extend the TaskModel with custom fields</li> <li>Database configuration: Ensures database connection is set up before the app starts</li> <li>Smart .env loading: Automatically loads <code>.env</code> from your project directory when used as a library</li> </ul>"},{"location":"guides/library-usage/#solution-use-main-or-create_runnable_app","title":"Solution: Use <code>main()</code> or <code>create_runnable_app()</code>","text":"<p>Instead of manually implementing all these steps, use the provided functions:</p> <pre><code># Option 1: Complete solution (recommended for most cases)\nfrom apflow.api.main import main\n\n# This handles everything and runs the server\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code># Option 2: Get app instance and run server yourself\nfrom apflow.api.main import create_runnable_app\nimport uvicorn\n\n# This handles initialization and returns the app\napp = create_runnable_app()\n\n# Run with custom uvicorn configuration\nuvicorn.run(app, host=\"0.0.0.0\", port=8080)\n</code></pre> <p>If you need more control, you can manually call each step (see Option B in Basic Setup).</p>"},{"location":"guides/library-usage/#basic-setup","title":"Basic Setup","text":""},{"location":"guides/library-usage/#1-install-as-dependency","title":"1. Install as Dependency","text":"<p>Add <code>apflow</code> to your project's dependencies:</p> <pre><code># In your project (e.g., apflow-demo)\npip install apflow[a2a]\n# Or with all features\npip install apflow[all]\n</code></pre>"},{"location":"guides/library-usage/#2-create-your-application","title":"2. Create Your Application","text":"<p>Option A: Using <code>main()</code> (Recommended - Simplest)</p> <p>The <code>main()</code> function handles all initialization steps and runs the server automatically:</p> <pre><code>from apflow.api.main import main\nfrom apflow.core.storage.factory import configure_database\n\n# Configure database (optional, can use DATABASE_URL env var instead)\nconfigure_database(\n    connection_string=\"postgresql+asyncpg://user:password@localhost/dbname\"\n)\n\n# Run server with all initialization handled automatically\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Option A-2: Using <code>create_runnable_app()</code> (If you need the app object without running server)</p> <p>If you need the app object but want to run the server yourself:</p> <pre><code>from apflow.api.main import create_runnable_app\nfrom apflow.core.storage.factory import configure_database\nimport uvicorn\n\n# Configure database\nconfigure_database(\n    connection_string=\"postgresql+asyncpg://user:password@localhost/dbname\"\n)\n\n# Create app with all initialization handled automatically\napp = create_runnable_app(protocol=\"a2a\")\n\n# Run with uvicorn\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre> <p>Note: <code>create_runnable_app()</code> is the recommended function for getting the app instance. For complete server startup, use <code>main()</code> instead.</p> <p>Option B: Manual Setup (More Control)</p> <p>If you need more control, you can manually handle initialization:</p> <pre><code>import os\nimport warnings\nfrom pathlib import Path\nfrom apflow.api.app import create_app_by_protocol\nfrom apflow.api.extensions import initialize_extensions, _load_custom_task_model\nfrom apflow.core.storage.factory import configure_database\nimport uvicorn\n\n# 1. Load .env file (optional)\ntry:\n    from dotenv import load_dotenv\n    env_path = Path(__file__).parent / \".env\"\n    if env_path.exists():\n        load_dotenv(env_path)\nexcept ImportError:\n    pass\n\n# 2. Suppress warnings (optional)\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning, module=\"pysbd\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"websockets\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"uvicorn\")\n\n# 3. Configure database\nconfigure_database(\n    connection_string=os.getenv(\"DATABASE_URL\")\n)\n\n# 4. Initialize extensions (registers executors, hooks, etc.)\ninitialize_extensions(\n    load_custom_task_model=True,\n    auto_init_examples=False,  # Examples are deprecated\n)\n\n# 5. Load custom TaskModel if specified in env var\n_load_custom_task_model()\n\n# 6. Create app\napp = create_app_by_protocol(\n    protocol=\"a2a\",\n    auto_initialize_extensions=False,  # Already initialized above\n)\n\n# 7. Run server\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</code></pre>"},{"location":"guides/library-usage/#database-configuration","title":"Database Configuration","text":""},{"location":"guides/library-usage/#using-environment-variable","title":"Using Environment Variable","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># .env (in your project root, e.g., apflow-demo/.env)\nDATABASE_URL=postgresql+asyncpg://user:password@localhost/dbname?sslmode=require\n</code></pre> <p>Important: When using apflow as a library, the <code>.env</code> file should be in your project's root directory, not in the library's installation directory. The library will automatically look for <code>.env</code> in:</p> <ol> <li>Current working directory (where you run the script)</li> <li>Directory of the main script (where your <code>main.py</code> or entry script is located)</li> <li>Library's own directory (only when developing the library itself, not when installed as a package)</li> </ol> <p>This ensures that your project's <code>.env</code> file is loaded, not the library's <code>.env</code> file.</p>"},{"location":"guides/library-usage/#using-code","title":"Using Code","text":"<pre><code>from apflow.core.storage.factory import configure_database\n\n# PostgreSQL with SSL\nconfigure_database(\n    connection_string=\"postgresql+asyncpg://user:password@host:port/dbname?sslmode=require&amp;sslrootcert=/path/to/ca.crt\"\n)\n\n# DuckDB\nconfigure_database(path=\"./data/app.duckdb\")\n</code></pre>"},{"location":"guides/library-usage/#custom-routes","title":"Custom Routes","text":"<p>Add your own API routes to extend the application:</p> <p>Using <code>main()</code> (Recommended):</p> <pre><code>from starlette.routing import Route\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\nfrom apflow.api.main import main\n\n# Define custom route handlers\nasync def health_check(request: Request) -&gt; JSONResponse:\n    \"\"\"Custom health check endpoint\"\"\"\n    return JSONResponse({\n        \"status\": \"healthy\",\n        \"service\": \"my-custom-service\"\n    })\n\nasync def custom_api_handler(request: Request) -&gt; JSONResponse:\n    \"\"\"Custom API endpoint\"\"\"\n    data = await request.json()\n    return JSONResponse({\n        \"message\": \"Custom endpoint\",\n        \"received\": data\n    })\n\n# Create custom routes\ncustom_routes = [\n    Route(\"/health\", health_check, methods=[\"GET\"]),\n    Route(\"/api/custom\", custom_api_handler, methods=[\"POST\"]),\n]\n\n# Run server with custom routes\nif __name__ == \"__main__\":\n    main(custom_routes=custom_routes)\n</code></pre> <p>Using <code>create_runnable_app()</code>:</p> <pre><code>from apflow.api.main import create_runnable_app\n\n# Create app with custom routes\napp = create_runnable_app(\n    protocol=\"a2a\",\n    custom_routes=custom_routes\n)\n</code></pre>"},{"location":"guides/library-usage/#custom-middleware","title":"Custom Middleware","text":"<p>Add custom middleware for request processing, logging, authentication, etc.:</p> <p>Using <code>main()</code> (Recommended):</p> <pre><code>from starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\nimport time\nfrom apflow.api.main import main\n\nclass RequestLoggingMiddleware(BaseHTTPMiddleware):\n    \"\"\"Custom middleware to log all requests\"\"\"\n\n    async def dispatch(self, request: Request, call_next):\n        start_time = time.time()\n\n        # Log request\n        print(f\"Request: {request.method} {request.url.path}\")\n\n        # Process request\n        response = await call_next(request)\n\n        # Log response\n        process_time = time.time() - start_time\n        print(f\"Response: {response.status_code} ({process_time:.3f}s)\")\n\n        return response\n\nclass CustomAuthMiddleware(BaseHTTPMiddleware):\n    \"\"\"Custom authentication middleware\"\"\"\n\n    async def dispatch(self, request: Request, call_next):\n        # Skip auth for certain paths\n        if request.url.path in [\"/health\", \"/docs\", \"/openapi.json\"]:\n            return await call_next(request)\n\n        # Check custom header\n        api_key = request.headers.get(\"X-API-KEY\")\n        if not api_key or api_key != \"your-secret-key\":\n            return JSONResponse(\n                {\"error\": \"Unauthorized\"},\n                status_code=401\n            )\n\n        return await call_next(request)\n\n# Run server with custom middleware\nif __name__ == \"__main__\":\n    main(\n        custom_middleware=[\n            RequestLoggingMiddleware,\n            CustomAuthMiddleware,\n        ]\n    )\n</code></pre> <p>Using <code>create_runnable_app()</code>:</p> <pre><code>from apflow.api.main import create_runnable_app\n\napp = create_runnable_app(\n    protocol=\"a2a\",\n    custom_middleware=[\n        RequestLoggingMiddleware,\n        CustomAuthMiddleware,\n    ]\n)\n</code></pre> <p>Note: Custom middleware is added after default middleware (CORS, LLM API key, JWT), so it runs in the order you provide.</p>"},{"location":"guides/library-usage/#custom-taskroutes","title":"Custom TaskRoutes","text":"<p>Extend TaskRoutes to customize task management behavior:</p> <pre><code>from apflow.api.routes.tasks import TaskRoutes\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\nfrom apflow.api.app import create_app_by_protocol\n\nclass MyCustomTaskRoutes(TaskRoutes):\n    \"\"\"Custom TaskRoutes with additional functionality\"\"\"\n\n    async def handle_task_requests(self, request: Request) -&gt; JSONResponse:\n        # Add custom logic before handling request\n        print(f\"Custom task request: {request.method} {request.url.path}\")\n\n        # Call parent implementation\n        response = await super().handle_task_requests(request)\n\n        # Add custom logic after handling request\n        # (e.g., custom logging, metrics, etc.)\n\n        return response\n\n# Create app with custom TaskRoutes\napp = create_app_by_protocol(\n    protocol=\"a2a\",\n    task_routes_class=MyCustomTaskRoutes\n)\n</code></pre>"},{"location":"guides/library-usage/#running-your-application","title":"Running Your Application","text":"<pre><code># With environment variable\nexport DATABASE_URL=\"postgresql+asyncpg://user:password@localhost/dbname\"\npython app.py\n\n# Or with .env file\n# .env\nDATABASE_URL=postgresql+asyncpg://user:password@localhost/dbname\n\npython app.py\n</code></pre>"},{"location":"guides/library-usage/#middleware-order","title":"Middleware Order","text":"<p>Middleware is added in the following order:</p> <ol> <li>Default middleware (added by apflow):</li> <li>CORS middleware</li> <li>LLM API key middleware</li> <li> <p>JWT authentication middleware (if enabled)</p> </li> <li> <p>Custom middleware (added by you):</p> </li> <li>Added in the order you provide in <code>custom_middleware</code> list</li> </ol> <p>This means your custom middleware runs after the default middleware, so it can: - Access JWT-authenticated user information - Modify responses from default routes - Add additional logging/metrics</p>"},{"location":"guides/library-usage/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for configuration (database URL, secrets, etc.)</li> <li>Load .env files early in your application startup</li> <li>Configure database before creating the app</li> <li>Keep custom routes focused and well-documented</li> <li>Test middleware thoroughly as it affects all requests</li> <li>Use type hints for better code clarity</li> </ol>"},{"location":"guides/library-usage/#extending-and-overriding-cli-commands-and-extensions","title":"Extending and Overriding CLI Commands and Extensions","text":"<p>apflow supports advanced extensibility for both CLI commands and core extensions. You can add your own commands, extend existing groups, or override built-in commands and extensions using simple decorators and parameters.</p>"},{"location":"guides/library-usage/#cli-command-extension-and-override","title":"CLI Command Extension and Override","text":"<p>You can register new CLI command groups or single commands using the <code>@cli_register</code> decorator. To extend an existing group, use the <code>group</code> parameter. To override an existing command or group, set <code>override=True</code>.</p> <p>Register a new command group: <pre><code>from apflow.cli.decorators import cli_register\n\n@cli_register(name=\"my-group\", help=\"My command group\")\nclass MyGroup:\n    def foo(self):\n        print(\"foo\")\n    def bar(self):\n        print(\"bar\")\n</code></pre></p> <p>Add a subcommand to an existing group: <pre><code>@cli_register(group=\"my-group\", name=\"baz\", help=\"Baz command\")\ndef baz():\n    print(\"baz\")\n</code></pre></p> <p>Override an existing command or group: <pre><code>@cli_register(name=\"my-group\", override=True)\nclass NewMyGroup:\n    ...\n\n@cli_register(group=\"my-group\", name=\"foo\", override=True)\ndef new_foo():\n    print(\"new foo\")\n</code></pre></p> <p>Override a built-in command (e.g., 'run'): <pre><code>from apflow.cli.decorators import cli_register\n\n@cli_register(name=\"run\", override=True, help=\"Override built-in run command\")\ndef my_run():\n    print(\"This is my custom run command!\")\n</code></pre> Now, running <code>apflow run</code> will execute your custom logic instead of the built-in command.</p>"},{"location":"guides/library-usage/#core-extension-override","title":"Core Extension Override","text":"<p>apflow also supports custom extensions for executors, hooks, storage backends, and more. You can register your own or override built-in extensions by passing <code>override=True</code> when registering.</p> <p>Best Practices: - Use <code>override=True</code> only when you want to replace an existing command or extension. - Keep extension logic simple and well-documented. - Test your extensions thoroughly.</p>"},{"location":"guides/library-usage/#quick-reference-what-mainpy-does","title":"Quick Reference: What main.py Does","text":"<p>For reference, here's what <code>apflow.api.main.main()</code> and <code>create_runnable_app()</code> do:</p> <ol> <li>\u2705 Loads <code>.env</code> file (from calling project's directory when used as library)</li> <li>\u2705 Sets up development environment (only when running library's own main.py directly)</li> <li>\u2705 Initializes extensions (<code>initialize_extensions()</code>)</li> <li>\u2705 Loads custom TaskModel (<code>_load_custom_task_model()</code>)</li> <li>\u2705 Auto-initializes examples if database is empty</li> <li>\u2705 Creates app (<code>create_app_by_protocol()</code>)</li> <li>\u2705 Runs uvicorn server (only in <code>main()</code>, not in <code>create_runnable_app()</code>)</li> </ol> <p>Using <code>main()</code> or <code>create_runnable_app()</code>: All steps are handled automatically.</p> <p>Manual setup: You need to call steps 3-5 yourself before creating the app (see Option B in Basic Setup).</p>"},{"location":"guides/library-usage/#complete-example","title":"Complete Example","text":"<p>Here's a complete example combining all features:</p> <pre><code>\"\"\"\nComplete example: Using apflow as a library in your own project\n\nThis example shows how to use apflow as a library with custom routes,\nmiddleware, and configurations. All initialization steps are handled automatically\nby create_runnable_app() or main().\n\"\"\"\n\nimport os\nfrom starlette.routing import Route\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\n\n# ============================================================================\n# Step 1: Configure database (REQUIRED)\n# ============================================================================\nfrom apflow.core.storage.factory import configure_database\n\n# Option A: Use environment variable (recommended)\n# Set DATABASE_URL in .env file or environment variable\n# DATABASE_URL=postgresql+asyncpg://user:password@localhost/dbname\n# DATABASE_URL=duckdb:///./data/app.duckdb\n\n# Option B: Configure programmatically\n# configure_database(connection_string=\"postgresql+asyncpg://user:password@localhost/dbname\")\n# Or for DuckDB:\n# configure_database(path=\"./data/app.duckdb\")\n\n# Note: .env file is automatically loaded by create_runnable_app() or main()\n# from your project's directory (not from library's directory)\n\n# ============================================================================\n# Step 2: Define custom routes\n# ============================================================================\nasync def health_check(request: Request) -&gt; JSONResponse:\n    \"\"\"Health check endpoint\"\"\"\n    return JSONResponse({\n        \"status\": \"healthy\",\n        \"service\": \"my-custom-service\"\n    })\n\nasync def custom_api(request: Request) -&gt; JSONResponse:\n    \"\"\"Custom API endpoint\"\"\"\n    data = await request.json()\n    return JSONResponse({\n        \"message\": \"Custom API endpoint\",\n        \"received\": data\n    })\n\ncustom_routes = [\n    Route(\"/health\", health_check, methods=[\"GET\"]),\n    Route(\"/api/custom\", custom_api, methods=[\"POST\"]),\n]\n\n# ============================================================================\n# Step 3: Define custom middleware\n# ============================================================================\nclass LoggingMiddleware(BaseHTTPMiddleware):\n    \"\"\"Log all requests\"\"\"\n    async def dispatch(self, request: Request, call_next):\n        print(f\"Request: {request.method} {request.url.path}\")\n        response = await call_next(request)\n        print(f\"Response: {response.status_code}\")\n        return response\n\nclass RateLimitMiddleware(BaseHTTPMiddleware):\n    \"\"\"Simple rate limiting middleware\"\"\"\n    def __init__(self, app, requests_per_minute: int = 60):\n        super().__init__(app)\n        self.requests_per_minute = requests_per_minute\n\n    async def dispatch(self, request: Request, call_next):\n        # Simple rate limiting logic here\n        # (In production, use Redis or similar)\n        return await call_next(request)\n\ncustom_middleware = [\n    LoggingMiddleware,\n    RateLimitMiddleware,\n]\n\n# ============================================================================\n# Step 4: Create application (Option A - Recommended: Using main())\n# ============================================================================\n# Option A: Using main() - Simplest approach, handles everything automatically\nfrom apflow.api.main import main\n\nif __name__ == \"__main__\":\n    # main() handles all initialization and runs the server\n    main(\n        protocol=\"a2a\",\n        custom_routes=custom_routes,\n        custom_middleware=custom_middleware,\n        host=\"0.0.0.0\",\n        port=8000,\n        workers=1,\n    )\n\n# ============================================================================\n# Step 4: Create application (Option B - Using create_runnable_app())\n# ============================================================================\n# Option B: Using create_runnable_app() - Get app instance and run server yourself\n# from apflow.api.main import create_runnable_app\n# import uvicorn\n#\n# if __name__ == \"__main__\":\n#     # create_runnable_app() handles all initialization and returns the app\n#     app = create_runnable_app(\n#         protocol=\"a2a\",\n#         custom_routes=custom_routes,\n#         custom_middleware=custom_middleware,\n#     )\n#     \n#     # Run with custom uvicorn configuration\n#     uvicorn.run(\n#         app,\n#         host=\"0.0.0.0\",\n#         port=8000,\n#         workers=1,\n#         loop=\"asyncio\",\n#         limit_concurrency=100,\n#         limit_max_requests=1000,\n#         access_log=True,\n#     )\n\n# ============================================================================\n# What's automatically handled by create_runnable_app() or main():\n# ============================================================================\n# 1. \u2705 Loads .env file from your project's directory\n# 2. \u2705 Initializes extensions (executors, hooks, storage backends)\n# 3. \u2705 Loads custom TaskModel if specified in APFLOW_TASK_MODEL_CLASS\n# 4. \u2705 Auto-initializes examples if database is empty\n# 5. \u2705 Creates the API application with proper configuration\n# 6. \u2705 (main() only) Runs the uvicorn server\n#\n# You don't need to manually call these steps anymore!\n</code></pre>"},{"location":"guides/library-usage/#next-steps","title":"Next Steps","text":"<ul> <li>See API Documentation for available APIs</li> <li>See Task Management Guide for task operations</li> <li>See Extension Development for creating custom executors</li> </ul>"},{"location":"guides/scheduler/","title":"Task Scheduling","text":"<p>APFlow provides built-in task scheduling capabilities with support for both internal scheduling and external scheduler integration.</p>"},{"location":"guides/scheduler/#overview","title":"Overview","text":"<p>The scheduler module offers:</p> <ul> <li>Internal Scheduler: Built-in scheduler for standalone deployment</li> <li>External Gateway: Integration with external schedulers (cron, Kubernetes, etc.)</li> <li>Calendar Integration: iCal export for calendar applications</li> </ul>"},{"location":"guides/scheduler/#schedule-types","title":"Schedule Types","text":"<p>APFlow supports six schedule types:</p> Type Expression Example Description <code>once</code> <code>2024-01-15T09:00:00Z</code> Single execution at a specific datetime <code>interval</code> <code>3600</code> Recurring at fixed intervals (in seconds) <code>cron</code> <code>0 9 * * 1-5</code> Standard cron expression <code>daily</code> <code>09:00</code> Daily at specific time (HH:MM) <code>weekly</code> <code>1,3,5 09:00</code> Weekly on specific days (1=Mon, 7=Sun) <code>monthly</code> <code>1,15 09:00</code> Monthly on specific dates"},{"location":"guides/scheduler/#installation","title":"Installation","text":"<p>Install with scheduling support:</p> <pre><code>pip install apflow[scheduling]\n</code></pre> <p>Or with all features:</p> <pre><code>pip install apflow[standard]\n</code></pre>"},{"location":"guides/scheduler/#quick-start","title":"Quick Start","text":""},{"location":"guides/scheduler/#1-create-a-scheduled-task","title":"1. Create a Scheduled Task","text":"<p>Use the CLI to create and configure a scheduled task:</p> <pre><code># Create a task\napflow tasks create --name \"Daily Report\" --inputs '{\"report_type\": \"summary\"}'\n\n# Configure scheduling\napflow tasks update &lt;task_id&gt; \\\n    --schedule-type daily \\\n    --schedule-expression \"09:00\" \\\n    --schedule-enabled\n\n# Initialize the schedule (calculates next_run_at)\napflow tasks scheduled init &lt;task_id&gt;\n</code></pre>"},{"location":"guides/scheduler/#2-start-the-internal-scheduler","title":"2. Start the Internal Scheduler","text":"<pre><code># Run in foreground\napflow scheduler start\n\n# Run with verbose logging (DEBUG level)\napflow scheduler start --verbose\n\n# Run in background\napflow scheduler start --background\n\n# With custom options\napflow scheduler start \\\n    --poll-interval 30 \\\n    --max-concurrent 5 \\\n    --user-id user123\n</code></pre>"},{"location":"guides/scheduler/#3-monitor-scheduled-tasks","title":"3. Monitor Scheduled Tasks","text":"<pre><code># List all scheduled tasks\napflow scheduler list\n\n# List with filters\napflow scheduler list --status running\napflow scheduler list --type daily --all\n\n# Check for due tasks\napflow tasks scheduled due\n\n# Check for tasks due before a specific time\napflow tasks scheduled due --before \"2024-12-31T23:59:59Z\"\n\n# Check scheduler status\napflow scheduler status\n</code></pre>"},{"location":"guides/scheduler/#internal-scheduler","title":"Internal Scheduler","text":"<p>The internal scheduler polls the database for due tasks and executes them automatically.</p>"},{"location":"guides/scheduler/#configuration-options","title":"Configuration Options","text":"Option Default Description <code>--poll-interval</code> 60 Seconds between checking for due tasks <code>--max-concurrent</code> 10 Maximum concurrent task executions <code>--timeout</code> 3600 Default task timeout in seconds <code>--user-id</code> None Only process tasks for this user <code>--background</code> False Run as background daemon <code>--verbose</code> / <code>-v</code> False Enable DEBUG-level logging output"},{"location":"guides/scheduler/#python-api","title":"Python API","text":"<pre><code>import asyncio\nfrom apflow.scheduler import InternalScheduler\nfrom apflow.scheduler.base import SchedulerConfig\n\n# Configure scheduler\nconfig = SchedulerConfig(\n    poll_interval=30,           # Check every 30 seconds\n    max_concurrent_tasks=5,     # Max 5 parallel tasks\n    task_timeout=1800,          # 30 minute timeout\n    user_id=\"user123\"           # Optional: filter by user\n)\n\n# Create and start scheduler\nscheduler = InternalScheduler(config)\n\nasync def main():\n    # Register completion callback\n    def on_complete(task_id, success, result):\n        print(f\"Task {task_id}: {'completed' if success else 'failed'}\")\n\n    scheduler.on_task_complete(on_complete)\n\n    # Start scheduler\n    await scheduler.start()\n\n    # Run until interrupted\n    try:\n        while True:\n            await asyncio.sleep(1)\n    except KeyboardInterrupt:\n        await scheduler.stop()\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/scheduler/#scheduler-authentication","title":"Scheduler Authentication","text":"<p>When running in API mode, the scheduler needs an admin token to trigger tasks on behalf of any user. The token is resolved in this order:</p> <ol> <li><code>admin_auth_token</code> from <code>config.cli.yaml</code> (if explicitly configured)</li> <li>Auto-generated admin JWT using the local <code>jwt_secret</code> from <code>config.cli.yaml</code></li> </ol> <p>The auto-generated token is cached for the scheduler session and eliminates the need for manual token configuration when running locally. At startup, the scheduler logs its auth identity (subject and source) for troubleshooting.</p>"},{"location":"guides/scheduler/#listing-scheduled-tasks","title":"Listing Scheduled Tasks","text":"<pre><code># List all enabled scheduled tasks\napflow scheduler list\n\n# Include disabled schedules\napflow scheduler list --all\n\n# Filter by schedule type\napflow scheduler list --type daily\n\n# Filter by task status\napflow scheduler list --status running\n\n# JSON output\napflow scheduler list -f json\n</code></pre>"},{"location":"guides/scheduler/#scheduler-lifecycle","title":"Scheduler Lifecycle","text":"<pre><code>start() \u2192 running \u2192 pause() \u2192 paused \u2192 resume() \u2192 running \u2192 stop() \u2192 stopped\n</code></pre>"},{"location":"guides/scheduler/#external-scheduler-integration","title":"External Scheduler Integration","text":"<p>APFlow provides gateway APIs for integration with external schedulers.</p>"},{"location":"guides/scheduler/#webhook-gateway","title":"Webhook Gateway","text":"<p>External schedulers can trigger task execution via HTTP webhooks.</p>"},{"location":"guides/scheduler/#generate-webhook-url","title":"Generate Webhook URL","text":"<pre><code>apflow scheduler webhook-url &lt;task_id&gt; --base-url https://api.example.com\n</code></pre> <p>Output: <pre><code>URL: https://api.example.com/webhook/trigger/abc123\nMethod: POST\n</code></pre></p>"},{"location":"guides/scheduler/#cron-integration","title":"Cron Integration","text":"<pre><code># Add to crontab\n0 9 * * 1-5 curl -X POST https://api.example.com/webhook/trigger/abc123\n</code></pre>"},{"location":"guides/scheduler/#kubernetes-cronjob","title":"Kubernetes CronJob","text":"<pre><code>from apflow.scheduler.gateway.webhook import generate_kubernetes_cronjob\n\nmanifest = generate_kubernetes_cronjob(\n    task_id=\"abc123\",\n    task_name=\"Daily Report\",\n    schedule_expression=\"0 9 * * *\",\n    webhook_url=\"https://api.example.com/webhook/trigger/abc123\",\n    namespace=\"production\"\n)\n</code></pre>"},{"location":"guides/scheduler/#api-endpoints-for-external-schedulers","title":"API Endpoints for External Schedulers","text":"Endpoint Method Description <code>tasks.scheduled.list</code> JSON-RPC List all scheduled tasks (supports <code>status</code> filter) <code>tasks.scheduled.due</code> JSON-RPC Get tasks due for execution <code>tasks.scheduled.init</code> JSON-RPC Initialize/recalculate next_run_at <code>tasks.scheduled.complete</code> JSON-RPC Mark task completed, calculate next run (supports <code>calculate_next_run</code> flag) <code>tasks.scheduled.export-ical</code> JSON-RPC Export scheduled tasks as iCalendar format <code>tasks.webhook.trigger</code> JSON-RPC Trigger task execution via webhook <code>/webhook/trigger/{task_id}</code> REST POST Simple REST endpoint for external schedulers"},{"location":"guides/scheduler/#webhook-authentication","title":"Webhook Authentication","text":"<p>The webhook trigger endpoint supports a three-layer authentication priority chain:</p> Priority Method Description 1 JWT Standard JWT via <code>Authorization</code> header (handled by middleware) 2 Webhook verify hook Custom verification via <code>@register_webhook_verify_hook</code> decorator 3 APFLOW_WEBHOOK_SECRET Internal HMAC signature validation <p>IP whitelist (<code>APFLOW_WEBHOOK_ALLOWED_IPS</code>) and rate limit (<code>APFLOW_WEBHOOK_RATE_LIMIT</code>) are applied as additional protection after authentication.</p> <p>Custom webhook verification hook:</p> <pre><code>from apflow import register_webhook_verify_hook\nfrom apflow.core.types import WebhookVerifyContext, WebhookVerifyResult\n\n@register_webhook_verify_hook\nasync def verify_tenant_webhook(ctx: WebhookVerifyContext) -&gt; WebhookVerifyResult:\n    # ctx provides: task_id, signature, timestamp, client_ip\n    if is_valid_signature(ctx.signature, ctx.timestamp):\n        return WebhookVerifyResult(valid=True, user_id=\"tenant-user\")\n    return WebhookVerifyResult(valid=False, error=\"Invalid signature\")\n</code></pre>"},{"location":"guides/scheduler/#rest-webhook-endpoint","title":"REST Webhook Endpoint","text":"<p>The simplest way to trigger tasks from external schedulers:</p> <pre><code># Basic trigger (with JWT)\ncurl -X POST https://api.example.com/webhook/trigger/abc123 \\\n  -H \"Authorization: Bearer &lt;jwt-token&gt;\"\n\n# With HMAC signature validation (if APFLOW_WEBHOOK_SECRET is configured)\ncurl -X POST https://api.example.com/webhook/trigger/abc123 \\\n  -H \"X-Webhook-Signature: &lt;hmac-signature&gt;\" \\\n  -H \"X-Webhook-Timestamp: &lt;unix-timestamp&gt;\"\n\n# Synchronous execution (wait for result)\ncurl -X POST \"https://api.example.com/webhook/trigger/abc123?async=false\"\n</code></pre>"},{"location":"guides/scheduler/#json-rpc-webhook-trigger","title":"JSON-RPC Webhook Trigger","text":"<pre><code>curl -X POST https://api.example.com/tasks/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.webhook.trigger\",\n    \"params\": {\n      \"task_id\": \"abc123\",\n      \"async_execution\": true\n    },\n    \"id\": 1\n  }'\n</code></pre>"},{"location":"guides/scheduler/#get-due-tasks","title":"Get Due Tasks","text":"<pre><code>curl -X POST https://api.example.com/tasks/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.scheduled.due\",\n    \"params\": {\n      \"limit\": 10\n    },\n    \"id\": 1\n  }'\n</code></pre>"},{"location":"guides/scheduler/#calendar-integration","title":"Calendar Integration","text":"<p>Export scheduled tasks to iCalendar format for viewing in calendar applications.</p>"},{"location":"guides/scheduler/#cli-export","title":"CLI Export","text":"<pre><code># Export to file\napflow scheduler export-ical -o schedule.ics\n\n# Export for specific user\napflow scheduler export-ical --user-id user123 -o user_schedule.ics\n\n# Custom calendar name\napflow scheduler export-ical --name \"My Task Schedule\" -o schedule.ics\n</code></pre>"},{"location":"guides/scheduler/#api-export","title":"API Export","text":"<pre><code>curl -X POST https://api.example.com/tasks/ \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"jsonrpc\": \"2.0\",\n    \"method\": \"tasks.scheduled.export-ical\",\n    \"params\": {\n      \"calendar_name\": \"APFlow Tasks\",\n      \"enabled_only\": true,\n      \"limit\": 100\n    },\n    \"id\": 1\n  }'\n</code></pre>"},{"location":"guides/scheduler/#python-api_1","title":"Python API","text":"<pre><code>from apflow.scheduler.gateway import ICalExporter\n\nexporter = ICalExporter(\n    calendar_name=\"APFlow Tasks\",\n    base_url=\"https://app.example.com\",\n    default_duration_minutes=30\n)\n\n# Export all scheduled tasks\nical_content = await exporter.export_tasks(\n    user_id=\"user123\",\n    enabled_only=True\n)\n\n# Write to file\nwith open(\"schedule.ics\", \"w\") as f:\n    f.write(ical_content)\n</code></pre>"},{"location":"guides/scheduler/#calendar-subscription-url","title":"Calendar Subscription URL","text":"<p>Generate a URL for live calendar subscription:</p> <pre><code>from apflow.scheduler.gateway.ical import generate_ical_feed_url\n\nurl = generate_ical_feed_url(\n    base_url=\"https://api.example.com\",\n    user_id=\"user123\",\n    api_key=\"your-api-key\"\n)\n# Result: https://api.example.com/scheduler/ical?user_id=user123&amp;api_key=your-api-key\n</code></pre>"},{"location":"guides/scheduler/#execution-mode","title":"Execution Mode","text":"<p>When a scheduled task is triggered, APFlow always loads the task tree from the database and executes it using the unified tree execution model. Dependency cascade is handled by <code>execute_after_task</code> regardless of task structure.</p> <p>For tasks with children, all child tasks are reset to clean <code>pending</code> state before each scheduled run. This ensures every execution cycle starts fresh \u2014 previous results, errors, and progress are cleared automatically.</p> <pre><code># Scheduled execution cycle\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Root Task      \u2502  \u2190 Scheduler triggers\n\u2502  (scheduled)    \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u251c\u2500 Child 1     \u2502  \u2190 All children reset to pending\n\u2502  \u251c\u2500 Child 2     \u2502     and re-executed in dependency order\n\u2502  \u2514\u2500 Child 3     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/scheduler/#multi-user-support","title":"Multi-User Support","text":"<p>APFlow supports multi-user scheduling through the <code>user_id</code> field:</p> <pre><code># Create task with user_id\nawait task_repository.create_task(\n    name=\"User Report\",\n    user_id=\"user123\",\n    schedule_type=\"daily\",\n    schedule_expression=\"09:00\",\n    schedule_enabled=True\n)\n\n# Start scheduler for specific user\nconfig = SchedulerConfig(user_id=\"user123\")\nscheduler = InternalScheduler(config)\n</code></pre>"},{"location":"guides/scheduler/#best-practices","title":"Best Practices","text":""},{"location":"guides/scheduler/#1-use-appropriate-schedule-types","title":"1. Use Appropriate Schedule Types","text":"<ul> <li>once: One-time scheduled events</li> <li>interval: Regular polling or heartbeat tasks</li> <li>cron: Complex schedules with minute-level control</li> <li>daily/weekly/monthly: Simple recurring tasks</li> </ul>"},{"location":"guides/scheduler/#2-set-schedule-boundaries","title":"2. Set Schedule Boundaries","text":"<pre><code>from datetime import datetime, timezone, timedelta\n\nawait task_repository.update_task(\n    task_id=task_id,\n    schedule_start_at=datetime.now(timezone.utc),\n    schedule_end_at=datetime.now(timezone.utc) + timedelta(days=30),\n    max_runs=100  # Stop after 100 executions\n)\n</code></pre>"},{"location":"guides/scheduler/#3-monitor-execution","title":"3. Monitor Execution","text":"<pre><code># Check run count\ntask = await task_repository.get_task_by_id(task_id)\nprint(f\"Executed {task.run_count} times\")\nprint(f\"Last run: {task.last_run_at}\")\nprint(f\"Next run: {task.next_run_at}\")\n</code></pre>"},{"location":"guides/scheduler/#4-handle-failures","title":"4. Handle Failures","text":"<pre><code># The scheduler automatically:\n# - Records errors in task.error\n# - Calculates next_run_at regardless of success/failure\n# - Respects max_runs limit\n# - Disables schedule when schedule_end_at is reached\n</code></pre>"},{"location":"guides/scheduler/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/scheduler/#tasks-not-executing","title":"Tasks Not Executing","text":"<ol> <li> <p>Check schedule is enabled:    <pre><code>apflow tasks get &lt;task_id&gt; | grep schedule_enabled\n</code></pre></p> </li> <li> <p>Verify next_run_at is set:    <pre><code>apflow tasks scheduled init &lt;task_id&gt;\n</code></pre></p> </li> <li> <p>Check scheduler is running:    <pre><code>apflow scheduler status\n</code></pre></p> </li> </ol>"},{"location":"guides/scheduler/#scheduler-wont-start","title":"Scheduler Won't Start","text":"<p>Check for existing process: <pre><code>apflow scheduler status\napflow scheduler stop  # If stale\napflow scheduler start\n</code></pre></p>"},{"location":"guides/scheduler/#ical-not-updating","title":"iCal Not Updating","text":"<p>Calendar applications cache feeds. Try: - Force refresh in calendar app - Wait for cache expiration (varies by app) - Use unique URL with timestamp for testing</p>"},{"location":"guides/scheduler/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      APFlow Scheduler                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502              Scheduler Interface                     \u2502   \u2502\n\u2502  \u2502         start() | stop() | trigger()                \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502         \u2502                \u2502                \u2502                 \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502  Internal \u2502    \u2502  Webhook  \u2502    \u2502   iCal    \u2502         \u2502\n\u2502   \u2502 Scheduler \u2502    \u2502  Gateway  \u2502    \u2502  Export   \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502         \u2502                \u2502                \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n          \u2502                \u2502                \u2502\n          \u25bc                \u25bc                \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   Task    \u2502    \u2502   cron    \u2502    \u2502  Google   \u2502\n    \u2502 Executor  \u2502    \u2502   K8s     \u2502    \u2502 Calendar  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502  Temporal \u2502    \u2502  Outlook  \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/scheduler/#api-reference","title":"API Reference","text":""},{"location":"guides/scheduler/#schedulerconfig","title":"SchedulerConfig","text":"Parameter Type Default Description poll_interval int 60 Seconds between polls max_concurrent_tasks int 10 Max parallel executions task_timeout int 3600 Task timeout in seconds retry_on_failure bool False Retry failed tasks max_retries int 3 Max retry attempts user_id str None User ID filter"},{"location":"guides/scheduler/#schedulerstats","title":"SchedulerStats","text":"Field Type Description state SchedulerState Current state started_at datetime Start time tasks_executed int Total executed tasks_succeeded int Successful count tasks_failed int Failed count active_tasks int Currently running"},{"location":"guides/scheduler/#webhookconfig","title":"WebhookConfig","text":"Parameter Type Default Description secret_key str None HMAC signature key allowed_ips list None Allowed IP addresses rate_limit int 0 Requests/minute (0=unlimited) timeout int 3600 Task timeout async_execution bool True Execute in background"},{"location":"guides/task-orchestration/","title":"Task Orchestration Guide","text":"<p>Master task orchestration in apflow. Learn how to create complex workflows, manage dependencies, and optimize execution.</p>"},{"location":"guides/task-orchestration/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>\u2705 How to build task trees and hierarchies</li> <li>\u2705 How dependencies control execution order</li> <li>\u2705 How priorities affect scheduling</li> <li>\u2705 Common patterns and best practices</li> <li>\u2705 Advanced orchestration techniques</li> </ul>"},{"location":"guides/task-orchestration/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Concepts</li> <li>Creating Task Trees</li> <li>Dependencies</li> <li>Priorities</li> <li>Common Patterns</li> <li>Best Practices</li> <li>Advanced Topics</li> <li>Task Copy and Re-execution</li> <li>Task Re-execution</li> </ol>"},{"location":"guides/task-orchestration/#core-concepts","title":"Core Concepts","text":""},{"location":"guides/task-orchestration/#what-is-task-orchestration","title":"What is Task Orchestration?","text":"<p>Task orchestration is the process of coordinating multiple tasks to work together. Think of it like conducting an orchestra - each musician (task) plays their part, but the conductor (TaskManager) ensures they play in harmony.</p>"},{"location":"guides/task-orchestration/#key-components","title":"Key Components","text":"<p>TaskManager: The orchestrator that coordinates everything - Manages task execution - Resolves dependencies - Handles priorities - Tracks status</p> <p>Task: A unit of work - Has an executor (the code that runs) - Has inputs (parameters) - Has a status (pending \u2192 in_progress \u2192 completed)</p> <p>Task Tree: The structure that organizes tasks - Hierarchical (parent-child relationships) - Can have dependencies (execution order) - Can have priorities (scheduling)</p>"},{"location":"guides/task-orchestration/#task-tree-structure","title":"Task Tree Structure","text":"<pre><code>Root Task\n\u2502\n\u251c\u2500\u2500 Child Task 1 (depends on nothing)\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 Grandchild Task 1.1 (depends on Child Task 1)\n\u2502\n\u251c\u2500\u2500 Child Task 2 (depends on nothing, runs in parallel with Child Task 1)\n\u2502\n\u2514\u2500\u2500 Child Task 3 (depends on Child Task 1 and Child Task 2)\n</code></pre>"},{"location":"guides/task-orchestration/#critical-distinction-parent-child-vs-dependencies","title":"Critical Distinction: Parent-Child vs Dependencies","text":"<p>\u26a0\ufe0f This is the most important concept to understand!</p>"},{"location":"guides/task-orchestration/#parent-child-relationship-parent_id","title":"Parent-Child Relationship (<code>parent_id</code>)","text":"<p>Purpose: Organization only - like folders in a file system</p> <ul> <li>Used to organize the task tree structure</li> <li>Helps visualize the hierarchy</li> <li>Does NOT affect execution order</li> <li>Purely organizational</li> </ul> <p>Example: <pre><code># Task B is a child of Task A (organizational)\ntask_a = create_task(name=\"task_a\")\ntask_b = create_task(name=\"task_b\", parent_id=task_a.id)  # Child of A\n</code></pre></p>"},{"location":"guides/task-orchestration/#dependencies-dependencies","title":"Dependencies (<code>dependencies</code>)","text":"<p>Purpose: Execution control - determines when tasks run</p> <ul> <li>Determines execution order</li> <li>A task waits for its dependencies to complete</li> <li>Controls the actual execution sequence</li> <li>This is what makes tasks wait for each other</li> </ul> <p>Example: <pre><code># Task B depends on Task C (execution order)\ntask_c = create_task(name=\"task_c\")\ntask_b = create_task(\n    name=\"task_b\",\n    dependencies=[{\"id\": task_c.id, \"required\": True}]  # Waits for C\n)\n# Execution order: C runs first, then B\n</code></pre></p>"},{"location":"guides/task-orchestration/#combined-example","title":"Combined Example","text":"<pre><code># Task B is a child of Task A (organizational)\n# But Task B depends on Task C (execution order)\ntask_a = create_task(name=\"task_a\")\ntask_c = create_task(name=\"task_c\")\ntask_b = create_task(\n    name=\"task_b\",\n    parent_id=task_a.id,  # Organizational: B is child of A\n    dependencies=[{\"id\": task_c.id, \"required\": True}]  # Execution: B waits for C\n)\n# Execution order: C executes first, then B (regardless of parent-child)\n</code></pre> <p>Visual Representation: <pre><code>Task A (root)\n\u2502\n\u2514\u2500\u2500 Task B (child of A, but depends on C)\n    \u2502\n    \u2514\u2500\u2500 (depends on) Task C (not a child, but must run first!)\n</code></pre></p> <p>Key Takeaway:  - Use <code>parent_id</code> for organization (like folders) - Use <code>dependencies</code> for execution order (when tasks run)</p>"},{"location":"guides/task-orchestration/#task-lifecycle","title":"Task Lifecycle","text":"<p>Tasks go through these states:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; pending: Task Created\n\n    pending --&gt; in_progress: Execution Started\n    pending --&gt; cancelled: User Cancellation\n\n    in_progress --&gt; completed: Execution Successful\n    in_progress --&gt; failed: Execution Failed\n    in_progress --&gt; cancelled: User Cancellation\n\n    completed --&gt; [*]: Task Finished\n    failed --&gt; [*]: Task Finished\n    cancelled --&gt; [*]: Task Finished\n\n    note right of pending\n        Initial state after task creation.\n        Waiting for dependencies to be satisfied.\n    end note\n\n    note right of in_progress\n        Task is currently executing.\n        Executor is running the task logic.\n    end note\n\n    note right of completed\n        Task finished successfully.\n        Result is available in task.result.\n    end note\n\n    note right of failed\n        Task execution failed.\n        Error details in task.error.\n    end note\n\n    note right of cancelled\n        Task was cancelled before completion.\n        Can be cancelled from any active state.\n    end note</code></pre> <p>Status Meanings: - pending: Created but not executed yet - in_progress: Currently executing - completed: Finished successfully - failed: Execution failed (check <code>task.error</code>) - cancelled: Was cancelled (check <code>task.error</code>)</p>"},{"location":"guides/task-orchestration/#creating-task-trees","title":"Creating Task Trees","text":""},{"location":"guides/task-orchestration/#basic-task-tree","title":"Basic Task Tree","text":"<p>The simplest possible tree - a single task:</p> <pre><code>import asyncio\nfrom apflow import TaskManager, TaskTreeNode, create_session\n\nasync def main():\n    db = create_session()\n    task_manager = TaskManager(db)\n\n    # Create a task\n    task = await task_manager.task_repository.create_task(\n        name=\"system_info_executor\",  # Executor ID\n        user_id=\"user123\",\n        priority=2,\n        inputs={\"resource\": \"cpu\"}\n    )\n\n    # Build task tree (even single tasks need a tree)\n    task_tree = TaskTreeNode(task)\n\n    # Execute\n    await task_manager.distribute_task_tree(task_tree)\n\n    # Get result\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    print(f\"Status: {result.status}\")\n    print(f\"Result: {result.result}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/task-orchestration/#hierarchical-task-tree","title":"Hierarchical Task Tree","text":"<p>Create a tree with parent-child relationships:</p> <pre><code># Create root task\nroot_task = await task_manager.task_repository.create_task(\n    name=\"root_task\",\n    user_id=\"user123\",\n    priority=1\n)\n\n# Create child tasks\nchild1 = await task_manager.task_repository.create_task(\n    name=\"child_task_1\",\n    user_id=\"user123\",\n    parent_id=root_task.id,  # Child of root\n    priority=2,\n    inputs={\"step\": 1}\n)\n\nchild2 = await task_manager.task_repository.create_task(\n    name=\"child_task_2\",\n    user_id=\"user123\",\n    parent_id=root_task.id,  # Also child of root\n    priority=2,\n    inputs={\"step\": 2}\n)\n\n# Build tree\nroot = TaskTreeNode(root_task)\nroot.add_child(TaskTreeNode(child1))\nroot.add_child(TaskTreeNode(child2))\n\n# Execute\nawait task_manager.distribute_task_tree(root)\n</code></pre> <p>Visual Structure: <pre><code>Root Task\n\u2502\n\u251c\u2500\u2500 Child Task 1\n\u2514\u2500\u2500 Child Task 2\n</code></pre></p>"},{"location":"guides/task-orchestration/#dependencies","title":"Dependencies","text":"<p>Dependencies are the mechanism that controls execution order. They ensure tasks run in the correct sequence.</p>"},{"location":"guides/task-orchestration/#dependency-resolution-flow","title":"Dependency Resolution Flow","text":"<p>The following diagram illustrates how the system resolves task dependencies:</p> <pre><code>flowchart TD\n    Start([Task Ready to Execute]) --&gt; GetDeps[Get Task Dependencies]\n    GetDeps --&gt; HasDeps{Has&lt;br/&gt;Dependencies?}\n\n    HasDeps --&gt;|No| ExecuteTask[Execute Task Immediately]\n    HasDeps --&gt;|Yes| CheckDepStatus[Check Each Dependency Status]\n\n    CheckDepStatus --&gt; AllRequiredComplete{All Required&lt;br/&gt;Dependencies&lt;br/&gt;Complete?}\n\n    AllRequiredComplete --&gt;|No| CheckOptional{Has Optional&lt;br/&gt;Dependencies?}\n    AllRequiredComplete --&gt;|Yes| MergeResults[Merge Dependency Results]\n\n    CheckOptional --&gt;|Yes| CheckOptionalStatus{Optional Dependencies&lt;br/&gt;Complete or Failed?}\n    CheckOptional --&gt;|No| WaitForDeps[Wait for Required Dependencies]\n\n    CheckOptionalStatus --&gt;|Complete| MergeResults\n    CheckOptionalStatus --&gt;|Failed| MergeResults\n    CheckOptionalStatus --&gt;|In Progress| WaitForDeps\n\n    WaitForDeps --&gt; CheckDepStatus\n\n    MergeResults --&gt; CollectResults[Collect Results from Dependencies]\n    CollectResults --&gt; MergeWithInputs[Merge with Task Inputs]\n\n    MergeWithInputs --&gt; ExecuteTask\n\n    ExecuteTask --&gt; TaskComplete([Task Execution Complete])\n\n    style Start fill:#e1f5ff\n    style ExecuteTask fill:#c8e6c9\n    style TaskComplete fill:#c8e6c9\n    style WaitForDeps fill:#fff9c4\n    style MergeResults fill:#e1bee7</code></pre>"},{"location":"guides/task-orchestration/#basic-dependency","title":"Basic Dependency","text":"<pre><code># Task 1: Fetch data\nfetch_task = await task_manager.task_repository.create_task(\n    name=\"fetch_data\",\n    user_id=\"user123\",\n    priority=1,\n    inputs={\"url\": \"https://api.example.com/data\"}\n)\n\n# Task 2: Process data (depends on Task 1)\nprocess_task = await task_manager.task_repository.create_task(\n    name=\"process_data\",\n    user_id=\"user123\",\n    parent_id=fetch_task.id,\n    dependencies=[{\"id\": fetch_task.id, \"required\": True}],  # Waits for fetch_task\n    priority=2,\n    inputs={\"operation\": \"analyze\"}\n)\n\n# Build tree\ntask_tree = TaskTreeNode(fetch_task)\ntask_tree.add_child(TaskTreeNode(process_task))\n\n# Execute\n# Execution order: fetch_task \u2192 process_task (automatic!)\nawait task_manager.distribute_task_tree(task_tree)\n</code></pre> <p>Execution Flow: <pre><code>Fetch Task (runs first)\n    \u2193\nProcess Task (waits for Fetch, then runs)\n</code></pre></p>"},{"location":"guides/task-orchestration/#sequential-pipeline","title":"Sequential Pipeline","text":"<p>Create a pipeline where each task depends on the previous:</p> <pre><code># Step 1: Fetch\nfetch = await task_manager.task_repository.create_task(\n    name=\"fetch_data\",\n    user_id=\"user123\",\n    priority=1\n)\n\n# Step 2: Process (depends on fetch)\nprocess = await task_manager.task_repository.create_task(\n    name=\"process_data\",\n    user_id=\"user123\",\n    parent_id=fetch.id,\n    dependencies=[{\"id\": fetch.id, \"required\": True}],\n    priority=2\n)\n\n# Step 3: Save (depends on process)\nsave = await task_manager.task_repository.create_task(\n    name=\"save_results\",\n    user_id=\"user123\",\n    parent_id=fetch.id,\n    dependencies=[{\"id\": process.id, \"required\": True}],\n    priority=3\n)\n\n# Build pipeline\nroot = TaskTreeNode(fetch)\nroot.add_child(TaskTreeNode(process))\nroot.add_child(TaskTreeNode(save))\n\n# Execute\n# Order: Fetch \u2192 Process \u2192 Save (automatic!)\nawait task_manager.distribute_task_tree(root)\n</code></pre> <p>Execution Flow: <pre><code>Fetch \u2192 Process \u2192 Save\n</code></pre></p>"},{"location":"guides/task-orchestration/#multiple-dependencies","title":"Multiple Dependencies","text":"<p>A task can depend on multiple other tasks:</p> <pre><code># Task 1\ntask1 = await task_manager.task_repository.create_task(\n    name=\"task1\",\n    user_id=\"user123\",\n    priority=1\n)\n\n# Task 2\ntask2 = await task_manager.task_repository.create_task(\n    name=\"task2\",\n    user_id=\"user123\",\n    priority=1\n)\n\n# Task 3 depends on BOTH Task 1 and Task 2\ntask3 = await task_manager.task_repository.create_task(\n    name=\"task3\",\n    user_id=\"user123\",\n    parent_id=root_task.id,\n    dependencies=[\n        {\"id\": task1.id, \"required\": True},\n        {\"id\": task2.id, \"required\": True}\n    ],\n    priority=2\n)\n</code></pre> <p>Execution Flow: <pre><code>Task 1 \u2500\u2500\u2510\n         \u251c\u2500\u2500\u2192 Task 3 (waits for both)\nTask 2 \u2500\u2500\u2518\n</code></pre></p> <p>Task 3 will only execute after both Task 1 and Task 2 complete.</p>"},{"location":"guides/task-orchestration/#dependency-types","title":"Dependency Types","text":""},{"location":"guides/task-orchestration/#required-dependencies","title":"Required Dependencies","text":"<p>Required dependencies must complete successfully:</p> <pre><code>dependencies=[\n    {\"id\": task1.id, \"required\": True}  # Must complete successfully\n]\n</code></pre> <p>Behavior: - Task waits for dependency to complete - If dependency fails, dependent task does NOT execute - This is the default behavior</p>"},{"location":"guides/task-orchestration/#optional-dependencies","title":"Optional Dependencies","text":"<p>Optional dependencies allow execution even if dependency fails:</p> <pre><code>dependencies=[\n    {\"id\": task1.id, \"required\": False}  # Can execute even if task1 fails\n]\n</code></pre> <p>Behavior: - Task waits for dependency to complete (or fail) - If dependency fails, dependent task still executes - Useful for fallback scenarios</p> <p>Example: <pre><code># Primary task\nprimary = await task_manager.task_repository.create_task(\n    name=\"primary_task\",\n    user_id=\"user123\",\n    priority=1\n)\n\n# Fallback task (runs even if primary fails)\nfallback = await task_manager.task_repository.create_task(\n    name=\"fallback_task\",\n    user_id=\"user123\",\n    dependencies=[{\"id\": primary.id, \"required\": False}],  # Optional\n    priority=2\n)\n</code></pre></p>"},{"location":"guides/task-orchestration/#priorities","title":"Priorities","text":"<p>Priorities control execution order when multiple tasks are ready to run.</p>"},{"location":"guides/task-orchestration/#priority-levels","title":"Priority Levels","text":"<pre><code>0 = Urgent (highest priority)\n1 = High\n2 = Normal (default)\n3 = Low (lowest priority)\n</code></pre> <p>Rule: Lower numbers = higher priority = execute first</p>"},{"location":"guides/task-orchestration/#basic-priority","title":"Basic Priority","text":"<pre><code># Urgent task (executes first)\nurgent = await task_manager.task_repository.create_task(\n    name=\"urgent_task\",\n    user_id=\"user123\",\n    priority=0  # Highest priority\n)\n\n# Normal task (executes after urgent)\nnormal = await task_manager.task_repository.create_task(\n    name=\"normal_task\",\n    user_id=\"user123\",\n    priority=2  # Normal priority\n)\n</code></pre> <p>Execution Order: 1. Urgent task (priority 0) 2. Normal task (priority 2)</p>"},{"location":"guides/task-orchestration/#priority-with-dependencies","title":"Priority with Dependencies","text":"<p>Important: Dependencies take precedence over priorities!</p> <pre><code># Task 1 (priority 2, no dependencies)\ntask1 = await task_manager.task_repository.create_task(\n    name=\"task1\",\n    user_id=\"user123\",\n    priority=2\n)\n\n# Task 2 (priority 0, depends on Task 1)\ntask2 = await task_manager.task_repository.create_task(\n    name=\"task2\",\n    user_id=\"user123\",\n    dependencies=[{\"id\": task1.id, \"required\": True}],\n    priority=0  # Higher priority, but still waits for Task 1!\n)\n</code></pre> <p>Execution Order: 1. Task 1 (priority 2, but no dependencies - runs first) 2. Task 2 (priority 0, but waits for Task 1 - runs second)</p> <p>Key Point: Even though Task 2 has higher priority, it waits for Task 1 because of the dependency!</p>"},{"location":"guides/task-orchestration/#priority-best-practices","title":"Priority Best Practices","text":"<ol> <li>Use consistently: Establish priority conventions in your project</li> <li>Don't overuse: Most tasks should use priority 2 (normal)</li> <li>Reserve 0 for emergencies: Only use priority 0 for critical tasks</li> <li>Remember dependencies: Priorities only matter when tasks are ready to run</li> </ol>"},{"location":"guides/task-orchestration/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/task-orchestration/#pattern-1-sequential-pipeline","title":"Pattern 1: Sequential Pipeline","text":"<p>Use Case: Steps that must happen in order</p> <pre><code>Task 1 \u2192 Task 2 \u2192 Task 3\n</code></pre> <p>Implementation: <pre><code>task1 = create_task(name=\"step1\")\ntask2 = create_task(\n    name=\"step2\",\n    dependencies=[{\"id\": task1.id, \"required\": True}]\n)\ntask3 = create_task(\n    name=\"step3\",\n    dependencies=[{\"id\": task2.id, \"required\": True}]\n)\n</code></pre></p> <p>Examples: - Data pipeline: Fetch \u2192 Process \u2192 Save - Build process: Compile \u2192 Test \u2192 Deploy - ETL: Extract \u2192 Transform \u2192 Load</p>"},{"location":"guides/task-orchestration/#pattern-2-fan-out-parallel","title":"Pattern 2: Fan-Out (Parallel)","text":"<p>Use Case: One task spawns multiple independent tasks</p> <pre><code>Root Task\n\u2502\n\u251c\u2500\u2500 Task 1 (parallel)\n\u251c\u2500\u2500 Task 2 (parallel)\n\u2514\u2500\u2500 Task 3 (parallel)\n</code></pre> <p>Implementation: <pre><code>root = create_task(name=\"root\")\ntask1 = create_task(name=\"task1\", parent_id=root.id)  # No dependencies\ntask2 = create_task(name=\"task2\", parent_id=root.id)  # No dependencies\ntask3 = create_task(name=\"task3\", parent_id=root.id)  # No dependencies\n</code></pre></p> <p>Examples: - Process multiple files in parallel - Call multiple APIs simultaneously - Run independent analyses</p>"},{"location":"guides/task-orchestration/#pattern-3-fan-in-converge","title":"Pattern 3: Fan-In (Converge)","text":"<p>Use Case: Multiple tasks converge to one final task</p> <pre><code>Task 1 \u2500\u2500\u2510\nTask 2 \u2500\u2500\u251c\u2500\u2500\u2192 Final Task\nTask 3 \u2500\u2500\u2518\n</code></pre> <p>Implementation: <pre><code>task1 = create_task(name=\"task1\")\ntask2 = create_task(name=\"task2\")\ntask3 = create_task(name=\"task3\")\n\nfinal = create_task(\n    name=\"final\",\n    dependencies=[\n        {\"id\": task1.id, \"required\": True},\n        {\"id\": task2.id, \"required\": True},\n        {\"id\": task3.id, \"required\": True}\n    ]\n)\n</code></pre></p> <p>Examples: - Aggregate results from multiple sources - Combine data from parallel processing - Merge multiple analyses</p>"},{"location":"guides/task-orchestration/#pattern-4-complex-workflow","title":"Pattern 4: Complex Workflow","text":"<p>Use Case: Combination of patterns</p> <pre><code>Root\n\u2502\n\u251c\u2500\u2500 Task 1 \u2500\u2500\u2510\n\u2502           \u2502\n\u251c\u2500\u2500 Task 2 \u2500\u253c\u2500\u2500\u2192 Task 4 \u2500\u2500\u2510\n\u2502           \u2502              \u2502\n\u2514\u2500\u2500 Task 3 \u2500\u2518              \u251c\u2500\u2500\u2192 Final\n                            \u2502\n                            \u2514\u2500\u2500\u2192 Task 5\n</code></pre> <p>Implementation: <pre><code>root = create_task(name=\"root\")\ntask1 = create_task(name=\"task1\", parent_id=root.id)\ntask2 = create_task(name=\"task2\", parent_id=root.id)\ntask3 = create_task(name=\"task3\", parent_id=root.id)\n\ntask4 = create_task(\n    name=\"task4\",\n    parent_id=root.id,\n    dependencies=[\n        {\"id\": task1.id, \"required\": True},\n        {\"id\": task2.id, \"required\": True},\n        {\"id\": task3.id, \"required\": True}\n    ]\n)\n\ntask5 = create_task(name=\"task5\", parent_id=root.id)\n\nfinal = create_task(\n    name=\"final\",\n    parent_id=root.id,\n    dependencies=[\n        {\"id\": task4.id, \"required\": True},\n        {\"id\": task5.id, \"required\": True}\n    ]\n)\n</code></pre></p>"},{"location":"guides/task-orchestration/#pattern-5-conditional-execution","title":"Pattern 5: Conditional Execution","text":"<p>Use Case: Fallback or alternative paths</p> <pre><code>Primary Task \u2500\u2500\u2510\n               \u251c\u2500\u2500\u2192 Success Task\n               \u2502\nFallback Task \u2500\u2518\n</code></pre> <p>Implementation: <pre><code>primary = create_task(name=\"primary\")\nfallback = create_task(name=\"fallback\")\n\nsuccess = create_task(\n    name=\"success\",\n    dependencies=[\n        {\"id\": primary.id, \"required\": False},  # Optional\n        {\"id\": fallback.id, \"required\": False}  # Optional\n    ]\n)\n</code></pre></p>"},{"location":"guides/task-orchestration/#best-practices","title":"Best Practices","text":""},{"location":"guides/task-orchestration/#1-use-meaningful-task-names","title":"1. Use Meaningful Task Names","text":"<p>Good: <pre><code>name=\"fetch_user_data\"\nname=\"process_payment\"\nname=\"send_notification_email\"\n</code></pre></p> <p>Bad: <pre><code>name=\"task1\"\nname=\"do_stuff\"\nname=\"x\"\n</code></pre></p>"},{"location":"guides/task-orchestration/#2-set-appropriate-priorities","title":"2. Set Appropriate Priorities","text":"<p>Convention: - <code>0</code>: Critical/emergency tasks only - <code>1</code>: High priority business tasks - <code>2</code>: Normal tasks (default) - <code>3</code>: Low priority/background tasks</p> <p>Example: <pre><code># Critical payment processing\npayment = create_task(name=\"process_payment\", priority=0)\n\n# Normal data processing\ndata = create_task(name=\"process_data\", priority=2)\n\n# Background cleanup\ncleanup = create_task(name=\"cleanup\", priority=3)\n</code></pre></p>"},{"location":"guides/task-orchestration/#3-handle-dependencies-explicitly","title":"3. Handle Dependencies Explicitly","text":"<p>Always specify dependencies explicitly:</p> <p>Good: <pre><code>task2 = create_task(\n    name=\"task2\",\n    dependencies=[{\"id\": task1.id, \"required\": True}]  # Explicit\n)\n</code></pre></p> <p>Bad: <pre><code># Relying on implicit order - don't do this!\ntask1 = create_task(...)\ntask2 = create_task(...)  # No dependency, but hoping task1 runs first\n</code></pre></p>"},{"location":"guides/task-orchestration/#4-use-parent-child-for-organization-dependencies-for-execution","title":"4. Use Parent-Child for Organization, Dependencies for Execution","text":"<p>Remember: - <code>parent_id</code>: Organization (like folders) - <code>dependencies</code>: Execution order (when tasks run)</p> <p>Good: <pre><code>root = create_task(name=\"root\")\nchild = create_task(\n    name=\"child\",\n    parent_id=root.id,  # Organizational\n    dependencies=[{\"id\": \"other_task\", \"required\": True}]  # Execution\n)\n</code></pre></p> <p>Bad: <pre><code># Don't rely on parent-child for execution order!\nchild = create_task(\n    name=\"child\",\n    parent_id=root.id  # This doesn't guarantee execution order!\n)\n</code></pre></p>"},{"location":"guides/task-orchestration/#5-handle-errors-gracefully","title":"5. Handle Errors Gracefully","text":"<p>Always check task status:</p> <pre><code>await task_manager.distribute_task_tree(task_tree)\n\n# Check all tasks\nfor task in tasks:\n    result = await task_manager.task_repository.get_task_by_id(task.id)\n    if result.status == \"failed\":\n        print(f\"Task {task.id} failed: {result.error}\")\n        # Handle failure appropriately\n</code></pre>"},{"location":"guides/task-orchestration/#6-keep-task-trees-manageable","title":"6. Keep Task Trees Manageable","text":"<p>Good: - Break complex workflows into smaller trees - Use clear naming conventions - Document complex dependencies</p> <p>Bad: - Creating massive trees with hundreds of tasks - Unclear dependency chains - No documentation</p>"},{"location":"guides/task-orchestration/#task-cancellation","title":"Task Cancellation","text":"<p>Cancel a running task:</p> <pre><code>result = await task_manager.cancel_task(\n    task_id=\"task_123\",\n    error_message=\"User requested cancellation\"\n)\n</code></pre> <p>Note: Not all executors support cancellation. Check <code>executor.cancelable</code> property.</p>"},{"location":"guides/task-orchestration/#task-re-execution","title":"Task Re-execution","text":"<p>Tasks can be marked for re-execution if dependencies fail:</p> <pre><code># If a dependency fails, dependent tasks are marked for re-execution\n# This allows retrying failed workflows\n</code></pre>"},{"location":"guides/task-orchestration/#streaming-execution","title":"Streaming Execution","text":"<p>Get real-time updates during execution:</p> <pre><code># Execute with streaming\nawait task_manager.distribute_task_tree_with_streaming(\n    task_tree,\n    use_callback=True\n)\n</code></pre>"},{"location":"guides/task-orchestration/#dependency-resolution","title":"Dependency Resolution","text":"<p>Dependency results are automatically merged into task inputs:</p> <p>```python</p>"},{"location":"guides/task-orchestration/#copy-link-and-snapshot-behavior","title":"Copy, Link, and Snapshot Behavior","text":"<p>from_copy: - Deep copy of a task or tree (optionally with children and dependencies) - All copied tasks get new IDs and are independent from the original - Useful for retries, A/B testing, and templating</p> <p>from_link: - Creates a reference to an existing task (no data duplication) - Linked tasks share results with the original - Useful for deduplication and sharing computation Note: If <code>user_id</code> is specified, the linked task's <code>user_id</code> must match. Linking to a task with a different <code>user_id</code> is not allowed for security and data isolation reasons.</p> <p>from_archive: - Creates a frozen, read-only copy of a task or tree - Preserves the state at the time of archive - Useful for audit, compliance, and reproducibility</p> <p>from_mixed: - Advanced: mix links and copies in a new tree - Enables hybrid workflows (e.g., copy some tasks, link others) Note: When using links in a mixed tree, if <code>user_id</code> is specified, all linked tasks must have the same <code>user_id</code>. Linking to tasks with a different <code>user_id</code> is not permitted.</p> <p>What Gets Copied or Linked: - Task definition (name, schemas, inputs, params, etc.) - Task hierarchy (parent-child relationships) - Dependencies (when copying recursively) - Task metadata (user_id, priority, etc.)</p> <p>What Gets Reset (for copies): - <code>status</code>: Set to <code>\"pending\"</code> - <code>result</code>: Set to <code>None</code> - <code>progress</code>: Set to <code>0.0</code> - Execution timestamps</p> <p>What Gets Preserved: - Original task remains unchanged - Execution history and results are preserved in the original</p> <p>Deduplication: - When copying with children, dependencies are only copied once - The copied tree maintains the same structure as the original Problem: Task never completes</p> <p>Solutions: 1. Check if executor is hanging 2. Verify executor supports cancellation 3. Check for deadlocks in dependencies 4. Review executor implementation</p>"},{"location":"guides/task-orchestration/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Tasks Guide - Create your own executors</li> <li>Basic Examples - More practical examples</li> <li>Best Practices Guide - Advanced techniques</li> <li>API Reference - Complete API documentation</li> </ul> <p>Need help? Check the FAQ or Quick Start Guide</p>"},{"location":"guides/task-orchestration/#task-data-fields-inputs-params-and-result","title":"Task Data Fields: inputs, params, and result","text":"<p>To ensure clean, composable, and predictable task orchestration, apflow enforces a strict separation of concerns for task data fields:</p> <ul> <li>inputs: Only business input data for the task. This is the data the executor will process (e.g., text to summarize, file to process, etc.). It should never include configuration, credentials, or executor-specific settings.</li> <li>params: Only executor configuration and setup parameters (e.g., API keys, model names, connection info). These are used to initialize the executor and are not passed as business data.</li> <li>result: Only the pure business output of the task. This is the value that downstream tasks will consume as their <code>inputs</code>. The <code>result</code> should not include logs, token usage, internal metadata, or any executor-specific structure\u2014just the output relevant to the user or next task.</li> </ul> <p>Why this matters: This separation ensures that: - Task data flows are clean and composable in a task tree. - Executors are reusable and predictable. - Downstream tasks can directly use upstream results as their inputs, without worrying about mixed-in configuration or irrelevant metadata.</p> <p>Best Practice: - Executors should only use <code>params</code> for initialization, process <code>inputs</code> as business data, and return a clean <code>result</code>. - Never mix configuration into <code>inputs</code> or <code>result</code>.</p> <p>For detailed field definitions and examples, see the Data Model Protocol documentation.</p>"},{"location":"protocol/","title":"AI Partner Up Flow Protocol","text":"<p>The AI Partner Up Flow Protocol defines the standard for interaction between different components of the system, enabling interoperability across multiple language implementations.</p>"},{"location":"protocol/#what-is-the-protocol","title":"What is the Protocol?","text":"<p>The protocol is a language-agnostic specification that ensures seamless communication between nodes running different implementations. It provides all specifications needed to implement a compatible library without reference to specific implementations.</p>"},{"location":"protocol/#key-features","title":"Key Features","text":"<ul> <li>Language Agnostic: Designed to be implemented in any programming language (Python, Go, Rust, JavaScript, etc.)</li> <li>Interoperability: Ensures seamless communication between nodes running different implementations</li> <li>Extensibility: Allows for future enhancements without breaking existing implementations</li> <li>Completeness: Provides all specifications needed to implement a compatible library</li> </ul>"},{"location":"protocol/#protocol-documentation","title":"Protocol Documentation","text":"<p>The protocol specification is organized into the following documents:</p>"},{"location":"protocol/#getting-started","title":"\ud83d\udccb Getting Started","text":"<ol> <li>Overview - Protocol introduction, versioning, and standards</li> <li>Protocol goals and version information</li> <li>Standards compliance (JSON-RPC 2.0, A2A Protocol, JSON Schema)</li> <li>Implementation guide</li> </ol>"},{"location":"protocol/#core-concepts","title":"\ud83d\udd11 Core Concepts","text":"<ol> <li>Core Concepts - Fundamental concepts and interfaces</li> <li>Flow, Task, Executor, Node, TaskManager</li> <li> <p>Key definitions and relationships</p> </li> <li> <p>Data Model - Complete task schema and data structures</p> </li> <li>Task schema definition</li> <li>Field specifications and types</li> <li>JSON Schema definitions</li> </ol>"},{"location":"protocol/#execution","title":"\u2699\ufe0f Execution","text":"<ol> <li>Execution Lifecycle - State machine and execution rules</li> <li>Task state transitions</li> <li>Execution rules and dependencies</li> <li>Priority scheduling</li> </ol>"},{"location":"protocol/#reference","title":"\ud83d\udcda Reference","text":"<ol> <li>Examples - Comprehensive examples and use cases</li> <li>Basic task examples</li> <li>Complex workflow examples</li> <li> <p>Real-world scenarios</p> </li> <li> <p>Interface Protocol - API specifications</p> </li> <li>JSON-RPC 2.0 API</li> <li>A2A Protocol integration</li> <li>Transport layer specifications</li> </ol>"},{"location":"protocol/#compliance","title":"\u2705 Compliance","text":"<ol> <li>Conformance - Implementation requirements and compliance</li> <li>MUST/SHOULD/MAY requirements</li> <li>Compliance checklist</li> <li> <p>Testing guidelines</p> </li> <li> <p>Error Handling - Error codes and handling procedures</p> </li> <li>Standard error codes</li> <li>Error response format</li> <li> <p>Error handling best practices</p> </li> <li> <p>Validation - Validation rules and algorithms</p> </li> <li>Input validation</li> <li>Schema validation</li> <li>Validation algorithms</li> </ol>"},{"location":"protocol/#quick-navigation","title":"Quick Navigation","text":""},{"location":"protocol/#for-implementers","title":"For Implementers","text":"<p>New to the protocol? Start here:</p> <ol> <li>Overview - Understand the protocol goals and structure</li> <li>Core Concepts - Learn the fundamental concepts</li> <li>Data Model - Understand the task schema</li> <li>Execution Lifecycle - Learn how tasks execute</li> <li>Examples - See practical examples</li> </ol> <p>Implementing a library? Follow this path:</p> <ol> <li>Read Core Concepts and Data Model</li> <li>Study Execution Lifecycle for state transitions</li> <li>Review Examples for usage patterns</li> <li>Implement core components (Task, Executor, TaskManager, Storage, API)</li> <li>Validate using Conformance checklist</li> <li>Test with Examples and Validation rules</li> </ol>"},{"location":"protocol/#for-users","title":"For Users","text":"<p>Using an existing implementation? Check these:</p> <ul> <li>Interface Protocol - API reference for clients</li> <li>Examples - Usage examples</li> <li>Error Handling - Understanding error responses</li> </ul>"},{"location":"protocol/#protocol-version","title":"Protocol Version","text":"<p>Current Version: 1.0</p> <p>Version Format: <code>MAJOR.MINOR.PATCH</code> - MAJOR: Breaking changes to the protocol - MINOR: New features that are backward compatible - PATCH: Bug fixes and clarifications</p> <p>MUST: Implementations MUST specify the protocol version they support.</p> <p>SHOULD: Implementations SHOULD be backward compatible with previous minor versions when possible.</p>"},{"location":"protocol/#protocol-standards","title":"Protocol Standards","text":"<p>The protocol builds on and complies with established standards:</p> <ul> <li>JSON-RPC 2.0: All RPC operations use JSON-RPC 2.0 format</li> <li>A2A Protocol: Enhanced agent-to-agent communication</li> <li>JSON Schema Draft 7: Task schema definitions and validation</li> <li>UUID (RFC 4122): Task identifiers</li> <li>ISO 8601: Timestamp fields</li> </ul> <p>See Overview for detailed compliance requirements.</p>"},{"location":"protocol/#next-steps","title":"Next Steps","text":"<ul> <li>New to the protocol? \u2192 Start with Overview</li> <li>Implementing a library? \u2192 Read Core Concepts</li> <li>Using an API? \u2192 Check Interface Protocol</li> <li>Need examples? \u2192 See Examples</li> </ul> <p>Ready to dive in? \u2192 Start with Overview \u2192</p>"},{"location":"protocol/01-overview/","title":"Protocol Overview","text":"<p>The AI Partner Up Flow Protocol defines the standard for interaction between different components of the system, enabling interoperability across multiple language implementations.</p>"},{"location":"protocol/01-overview/#protocol-goals","title":"Protocol Goals","text":"<ul> <li>Language Agnostic: The protocol is designed to be implemented in any programming language (Python, Go, Rust, JavaScript, etc.).</li> <li>Interoperability: Ensures seamless communication between nodes running different implementations.</li> <li>Extensibility: Allows for future enhancements without breaking existing implementations.</li> <li>Completeness: Provides all specifications needed to implement a compatible library without reference to specific implementations.</li> </ul>"},{"location":"protocol/01-overview/#protocol-version","title":"Protocol Version","text":"<p>Current Version: 1.0</p> <p>Version Format: <code>MAJOR.MINOR.PATCH</code> - MAJOR: Breaking changes to the protocol - MINOR: New features that are backward compatible - PATCH: Bug fixes and clarifications</p> <p>MUST: Implementations MUST specify the protocol version they support.</p> <p>SHOULD: Implementations SHOULD be backward compatible with previous minor versions when possible.</p>"},{"location":"protocol/01-overview/#key-concepts","title":"Key Concepts","text":"<ul> <li>Flow: A complete workflow or process, structured as a hierarchical tree of Tasks.</li> <li>Task: The atomic unit of execution within a Flow.</li> <li>Executor: The component responsible for performing the actual work defined by a Task.</li> <li>Node: A participant in the network that implements the protocol.</li> <li>TaskManager: The orchestrator that coordinates task execution, dependency resolution, and priority scheduling.</li> </ul> <p>See Core Concepts for detailed definitions.</p>"},{"location":"protocol/01-overview/#protocol-standards","title":"Protocol Standards","text":"<p>The protocol builds on and complies with established standards:</p>"},{"location":"protocol/01-overview/#json-rpc-20","title":"JSON-RPC 2.0","text":"<p>Standard: JSON-RPC 2.0 Specification</p> <p>Usage: All RPC operations use JSON-RPC 2.0 format.</p> <p>Compliance:  - MUST: Implementations MUST support JSON-RPC 2.0 over HTTP. - MUST: Implementations MUST use standard JSON-RPC 2.0 error codes for protocol errors.</p> <p>See Interface Protocol for complete API specification.</p>"},{"location":"protocol/01-overview/#a2a-protocol","title":"A2A Protocol","text":"<p>Standard: A2A Protocol</p> <p>Usage: Enhanced agent-to-agent communication with streaming and push notifications.</p> <p>Compliance: - SHOULD: Implementations SHOULD support A2A Protocol for enhanced features. - SHOULD: Implementations SHOULD support agent card discovery.</p> <p>See Interface Protocol for A2A Protocol integration details.</p>"},{"location":"protocol/01-overview/#json-schema","title":"JSON Schema","text":"<p>Standard: JSON Schema Draft 7</p> <p>Usage: Task schema definitions and input validation.</p> <p>Compliance: - MUST: Implementations MUST validate tasks against JSON Schema definitions. - SHOULD: Implementations SHOULD use JSON Schema for input validation.</p> <p>See Data Model for complete schema definitions.</p>"},{"location":"protocol/01-overview/#uuid","title":"UUID","text":"<p>Standard: RFC 4122 - UUID</p> <p>Usage: Task identifiers (<code>id</code>, <code>parent_id</code>, dependency IDs).</p> <p>Compliance: - MUST: Implementations MUST use UUID v4 format for all task identifiers. - MUST: Implementations MUST validate UUID format.</p>"},{"location":"protocol/01-overview/#iso-8601","title":"ISO 8601","text":"<p>Standard: ISO 8601 - Date and Time</p> <p>Usage: Timestamp fields (<code>created_at</code>, <code>started_at</code>, <code>updated_at</code>, <code>completed_at</code>).</p> <p>Compliance: - MUST: Implementations MUST use ISO 8601 format for timestamps. - SHOULD: Implementations SHOULD use UTC timezone.</p>"},{"location":"protocol/01-overview/#protocol-structure","title":"Protocol Structure","text":"<p>The protocol specification is organized into the following documents:</p> <ol> <li>Overview (this document): Protocol introduction, versioning, and standards</li> <li>Core Concepts: Fundamental concepts and interfaces</li> <li>Data Model: Complete task schema and data structures</li> <li>Execution Lifecycle: State machine and execution rules</li> <li>Examples: Comprehensive examples and use cases</li> <li>Interface Protocol: API specifications (JSON-RPC 2.0, A2A Protocol)</li> <li>Conformance: Implementation requirements and compliance</li> <li>Error Handling: Error codes and handling procedures</li> <li>Validation: Validation rules and algorithms</li> </ol>"},{"location":"protocol/01-overview/#implementation-guide","title":"Implementation Guide","text":""},{"location":"protocol/01-overview/#getting-started-for-implementers","title":"Getting Started for Implementers","text":"<p>To implement a compatible library:</p> <ol> <li>Read the Specification: Start with Core Concepts and Data Model.</li> <li>Understand the State Machine: Study Execution Lifecycle for state transitions and execution rules.</li> <li>Review Examples: See Examples for practical usage patterns.</li> <li>Implement Core Components:</li> <li>Task data structures</li> <li>Executor interface</li> <li>TaskManager (orchestration)</li> <li>Storage interface</li> <li>API interface (JSON-RPC 2.0)</li> <li>Validate Implementation: Use Conformance checklist to verify compliance.</li> <li>Test: Create test cases based on Examples and Validation rules.</li> </ol>"},{"location":"protocol/01-overview/#core-implementation-requirements","title":"Core Implementation Requirements","text":"<p>MUST implement: - Complete Task schema - State machine (all states and valid transitions) - Dependency resolution - Priority scheduling - Executor interface - Storage interface - JSON-RPC 2.0 API</p> <p>SHOULD implement: - A2A Protocol support - Streaming (SSE, WebSocket, or push notifications) - Input validation - Error handling</p> <p>MAY implement: - Authentication - Retry mechanisms - Advanced features</p> <p>See Conformance for complete requirements.</p>"},{"location":"protocol/01-overview/#testing-your-implementation","title":"Testing Your Implementation","text":"<ol> <li>Unit Tests: Test individual components (task validation, state transitions, dependency resolution).</li> <li>Integration Tests: Test complete workflows (task execution, dependency chains, error handling).</li> <li>Compliance Tests: Verify all MUST requirements are met.</li> <li>Interoperability Tests: Test with other implementations (if available).</li> </ol>"},{"location":"protocol/01-overview/#contributing-implementations","title":"Contributing Implementations","text":"<p>SHOULD: Implementations SHOULD be open source and publicly available.</p> <p>SHOULD: Implementations SHOULD include: - Complete documentation - Test suite - Example code - Protocol version compatibility information</p> <p>MAY: Implementations MAY seek protocol compliance certification.</p>"},{"location":"protocol/01-overview/#version-history","title":"Version History","text":""},{"location":"protocol/01-overview/#version-10-current","title":"Version 1.0 (Current)","text":"<p>Initial Release: Complete protocol specification including: - Task data model - State machine - Dependency resolution - Priority scheduling - Executor interface - Storage interface - JSON-RPC 2.0 API - A2A Protocol integration - Validation rules - Error handling</p> <p>Breaking Changes: None (initial version).</p>"},{"location":"protocol/01-overview/#compatibility","title":"Compatibility","text":""},{"location":"protocol/01-overview/#backward-compatibility","title":"Backward Compatibility","text":"<p>SHOULD: New minor versions SHOULD be backward compatible with previous minor versions.</p> <p>MUST: Breaking changes MUST result in a new major version.</p>"},{"location":"protocol/01-overview/#forward-compatibility","title":"Forward Compatibility","text":"<p>MAY: Implementations MAY support features from future protocol versions if they are optional.</p> <p>SHOULD: Implementations SHOULD gracefully handle unknown fields (ignore or preserve).</p>"},{"location":"protocol/01-overview/#protocol-independence","title":"Protocol Independence","text":"<p>This protocol specification is independent of any specific implementation:</p> <ul> <li>Language Agnostic: No language-specific code or examples (except for reference).</li> <li>Implementation Agnostic: No assumptions about internal implementation details.</li> <li>Complete: All information needed for implementation is in the protocol documents.</li> </ul> <p>MUST: Implementations MUST not require knowledge of specific implementations (e.g., Python reference implementation) to be compliant.</p> <p>SHOULD: Implementations SHOULD reference this protocol specification as the source of truth.</p>"},{"location":"protocol/01-overview/#reference-implementation","title":"Reference Implementation","text":"<p>The Python implementation (<code>apflow</code>) serves as a reference implementation:</p> <ul> <li>Demonstrates protocol compliance</li> <li>Provides examples and patterns</li> <li>Validates protocol completeness</li> </ul> <p>Note: The reference implementation is for reference only. Implementations in other languages MUST follow the protocol specification, not the reference implementation's internal details.</p>"},{"location":"protocol/01-overview/#protocol-extensions","title":"Protocol Extensions","text":"<p>The protocol supports extensions for custom functionality:</p> <ol> <li>Custom Executors: Implement custom executors for specific use cases</li> <li>System executors (system information, commands)</li> <li>Data processing executors (aggregation, transformation)</li> <li>External service executors (HTTP, APIs, databases)</li> <li>AI/LLM executors (LLM agents, AI models)</li> <li> <p>Domain-specific executors (business logic, integrations)</p> </li> <li> <p>Storage Backends: Implement custom storage backends</p> </li> <li>Database backends (SQL, NoSQL)</li> <li>File-based storage</li> <li> <p>In-memory storage</p> </li> <li> <p>Transport Layers: Implement custom transport layers</p> </li> <li>WebSocket</li> <li>Message queues</li> <li>Custom protocols</li> </ol> <p>MUST: Extensions MUST not break protocol compliance.</p> <p>SHOULD: Extensions SHOULD be documented and made available to the community.</p> <p>Note: While implementations may provide various executor types (e.g., <code>system_info_executor</code>, <code>command_executor</code>, <code>crew_manager</code>), the protocol only specifies the interface and registration mechanism. Specific executor identifiers are implementation-specific.</p>"},{"location":"protocol/01-overview/#getting-help","title":"Getting Help","text":"<ul> <li>Protocol Questions: Review the protocol documentation</li> <li>Implementation Questions: See Examples and Core Concepts</li> <li>Compliance Questions: See Conformance</li> <li>Error Handling: See Error Handling</li> </ul>"},{"location":"protocol/01-overview/#see-also","title":"See Also","text":"<ul> <li>Core Concepts - Fundamental protocol concepts</li> <li>Data Model - Complete task schema</li> <li>Execution Lifecycle - State machine specification</li> <li>Interface Protocol - API specifications</li> <li>Conformance - Implementation requirements</li> </ul>"},{"location":"protocol/02-core-concepts/","title":"Core Concepts","text":"<p>This section defines the fundamental concepts of the AI Partner Up Flow Protocol. Understanding these concepts is essential for implementing the protocol in any language.</p>"},{"location":"protocol/02-core-concepts/#protocol-independence","title":"Protocol Independence","text":"<p>MUST: This specification is language-agnostic. Implementations in any programming language MUST adhere to the data structures and behaviors defined here.</p> <p>SHOULD: Implementations SHOULD use standard formats (JSON, JSON Schema) for interoperability.</p>"},{"location":"protocol/02-core-concepts/#flow","title":"Flow","text":"<p>A Flow represents a complete workflow or process. It is structured as a hierarchical tree of Tasks.</p>"},{"location":"protocol/02-core-concepts/#flow-characteristics","title":"Flow Characteristics","text":"<ul> <li>Structure: A Flow is a Directed Acyclic Graph (DAG) where tasks are nodes and dependencies define the edges.</li> <li>Root Task: Every flow has a single root task. Complex flows are built by adding children to this root task.</li> <li>Execution: A flow is executed by executing its root task, which triggers execution of dependent tasks according to dependencies and priorities.</li> </ul>"},{"location":"protocol/02-core-concepts/#flow-lifecycle","title":"Flow Lifecycle","text":"<ol> <li>Creation: Flow is created with a root task</li> <li>Definition: Tasks are added to the flow (as children or dependencies)</li> <li>Validation: Flow structure is validated (no circular dependencies, valid references)</li> <li>Execution: Flow is executed (root task starts, dependencies are resolved)</li> <li>Completion: Flow completes when all tasks reach terminal states</li> </ol> <p>MUST: Implementations MUST support flow creation, validation, and execution.</p>"},{"location":"protocol/02-core-concepts/#task","title":"Task","text":"<p>A Task is the atomic unit of execution within a Flow.</p>"},{"location":"protocol/02-core-concepts/#task-definition","title":"Task Definition","text":"<ul> <li>Identity: Each task has a unique <code>id</code> (UUID) that persists across the network.</li> <li>Definition: A task is defined by its <code>name</code> (what to do) and <code>inputs</code> (data to work on).</li> <li>State: A task has a well-defined state (e.g., <code>pending</code>, <code>in_progress</code>, <code>completed</code>).</li> <li>Execution: A task is executed by an Executor, which processes the task's <code>inputs</code> and produces a <code>result</code>.</li> </ul>"},{"location":"protocol/02-core-concepts/#task-components","title":"Task Components","text":"<ol> <li>Metadata: <code>id</code>, <code>name</code>, <code>status</code>, <code>priority</code>, <code>user_id</code></li> <li>Structure: <code>parent_id</code>, <code>dependencies</code></li> <li>Configuration: <code>schemas</code>, <code>params</code></li> <li>Data: <code>inputs</code>, <code>result</code>, <code>error</code></li> <li>Tracking: <code>progress</code>, timestamps (<code>created_at</code>, <code>started_at</code>, <code>completed_at</code>)</li> </ol>"},{"location":"protocol/02-core-concepts/#task-provenance-and-references","title":"Task Provenance and References","text":"<p>To support provenance and advanced referencing, the following fields are included in the Task model:</p> <ul> <li><code>origin_type</code>: Indicates how the task was created. Possible values:<ul> <li><code>create</code>: Task created freshly</li> <li><code>link</code>: Task linked from another. The source task MUST be in <code>completed</code> status (in principle).</li> <li><code>copy</code>: Task copied from another (can be modified)</li> <li><code>archive</code>: Task archive from another (cannot be modified). The source task MUST be in <code>completed</code> status (in principle).</li> </ul> </li> <li><code>original_task_id</code>: The ID of the source task if this task was copied, linked, or snapshotted.</li> <li><code>has_references</code>: Boolean indicating whether this task is referenced/copied by others.</li> </ul> <p>These fields enable tracking task lineage, enforcing immutability for snapshots, and ensuring that links/snapshots only reference completed tasks.</p> <p>See Data Model for complete task schema.</p>"},{"location":"protocol/02-core-concepts/#executor","title":"Executor","text":"<p>An Executor is the component responsible for performing the actual work defined by a Task.</p>"},{"location":"protocol/02-core-concepts/#executor-role","title":"Executor Role","text":"<ul> <li>Input: Takes a Task's <code>inputs</code> and <code>schemas</code></li> <li>Processing: Performs the operation (executor-specific)</li> <li>Output: Produces a <code>result</code> (or raises an error)</li> </ul>"},{"location":"protocol/02-core-concepts/#executor-types","title":"Executor Types","text":"<p>Executors can be categorized by their functionality and implementation:</p>"},{"location":"protocol/02-core-concepts/#by-functionality","title":"By Functionality","text":"<ol> <li>System Executors: Interact with the local system</li> <li>System information queries (CPU, memory, disk)</li> <li>Command execution (shell commands)</li> <li> <p>File operations</p> </li> <li> <p>Data Processing Executors: Process and transform data</p> </li> <li>Data aggregation (combining results from multiple tasks)</li> <li>Data transformation</li> <li> <p>Data validation</p> </li> <li> <p>External Service Executors: Interact with external services</p> </li> <li>HTTP/HTTPS requests (REST APIs, webhooks)</li> <li>Database operations</li> <li> <p>Third-party API integrations</p> </li> <li> <p>AI/LLM Executors: Execute AI-powered tasks</p> </li> <li>LLM-based agents (e.g., CrewAI)</li> <li>AI model inference</li> <li> <p>Natural language processing</p> </li> <li> <p>Custom Executors: User-defined executors for specific use cases</p> </li> <li>Business logic executors</li> <li>Domain-specific operations</li> <li>Integration with proprietary systems</li> </ol>"},{"location":"protocol/02-core-concepts/#by-implementation-category","title":"By Implementation Category","text":"<ol> <li>Built-in Executors: Provided by the framework implementation</li> <li>System information executors</li> <li>Command executors</li> <li>Result aggregation executors</li> <li> <p>Tool executors (GitHub, web scraping, etc.)</p> </li> <li> <p>Optional Executors: Provided by optional extensions</p> </li> <li>LLM executors (requires AI/LLM dependencies)</li> <li>HTTP executors (may require additional dependencies)</li> <li> <p>Specialized executors</p> </li> <li> <p>Custom Executors: Created by users</p> </li> <li>User-defined executors implementing the Executor interface</li> <li>Registered with ExecutorRegistry</li> </ol> <p>MUST: The protocol defines how to invoke an executor, but not what the executor does internally.</p> <p>Note: Specific executor implementations (e.g., <code>system_info_executor</code>, <code>command_executor</code>, <code>crew_manager</code>) are implementation-specific. The protocol only specifies the interface and registration mechanism, not the specific executor types that must be provided.</p>"},{"location":"protocol/02-core-concepts/#common-executor-patterns","title":"Common Executor Patterns","text":"<p>While the protocol does not mandate specific executor types, common patterns include:</p> <ol> <li>System Information Executors: Query system resources (CPU, memory, disk)</li> <li>Command Executors: Execute shell commands or system operations</li> <li>Data Aggregation Executors: Combine results from multiple dependent tasks</li> <li>HTTP/API Executors: Make HTTP requests to external services</li> <li>LLM Executors: Execute AI/LLM-powered tasks (requires AI dependencies)</li> <li>Tool Executors: Integrate with external tools (GitHub, web scraping, etc.)</li> </ol> <p>MUST: Implementations MUST support custom executors for user-defined functionality.</p> <p>SHOULD: Implementations SHOULD provide common built-in executors for standard operations.</p> <p>MAY: Implementations MAY provide specialized executors for specific domains.</p>"},{"location":"protocol/02-core-concepts/#executor-interface-specification","title":"Executor Interface Specification","text":"<p>All executors MUST implement the following interface (language-agnostic specification):</p>"},{"location":"protocol/02-core-concepts/#required-methods","title":"Required Methods","text":"<p><code>execute(inputs: Object) -&gt; Object</code></p> <p>Executes the task with the given inputs.</p> <p>Input: - <code>inputs</code> (object): Task inputs (from <code>task.inputs</code>)</p> <p>Output: - Returns: Execution result (any JSON-serializable object) - Throws: Error if execution fails</p> <p>MUST: Executors MUST return a result object (not a primitive value). MUST: Executors MUST raise/throw errors for execution failures. SHOULD: Executors SHOULD validate inputs before execution.</p> <p>Example (conceptual): <pre><code># Python example (for reference only)\nasync def execute(self, inputs):\n    # Validate inputs\n    if not inputs.get(\"url\"):\n        raise ValueError(\"url is required\")\n\n    # Perform work\n    result = await fetch_url(inputs[\"url\"])\n\n    # Return result\n    return {\"content\": result, \"status\": \"success\"}\n</code></pre></p>"},{"location":"protocol/02-core-concepts/#optional-methods","title":"Optional Methods","text":"<p><code>cancel() -&gt; void</code></p> <p>Cancels the execution if supported.</p> <p>MUST: If executor does not support cancellation, this method MAY not exist or MAY be a no-op. SHOULD: If executor supports cancellation, it SHOULD stop execution gracefully.</p> <p><code>get_input_schema() -&gt; Object</code></p> <p>Returns JSON Schema defining valid inputs.</p> <p>SHOULD: Executors SHOULD provide input schema for validation. MAY: If not provided, inputs are not validated against a schema.</p> <p>Example (conceptual): <pre><code># Python example (for reference only)\ndef get_input_schema(self):\n    return {\n        \"type\": \"object\",\n        \"required\": [\"url\"],\n        \"properties\": {\n            \"url\": {\n                \"type\": \"string\",\n                \"format\": \"uri\"\n            }\n        }\n    }\n</code></pre></p>"},{"location":"protocol/02-core-concepts/#executor-registration","title":"Executor Registration","text":"<p>Executors MUST be registered with an <code>ExecutorRegistry</code> before they can be used.</p> <p>Registration Requirements: 1. Identifier: Each executor has a unique identifier (the <code>method</code> name in <code>schemas.method</code>) 2. Registry: Executors are registered in an <code>ExecutorRegistry</code> 3. Lookup: When a task is executed, the executor is looked up by <code>schemas.method</code></p> <p>MUST: Implementations MUST provide an <code>ExecutorRegistry</code> mechanism. MUST: Executor identifiers MUST be unique within a registry. SHOULD: Registration SHOULD happen before task execution.</p> <p>Implementation Note: The specific registration mechanism (decorators, function calls, configuration files) is implementation-specific. The protocol only specifies that executors must be registered and accessible via the <code>method</code> identifier.</p>"},{"location":"protocol/02-core-concepts/#executor-execution-contract","title":"Executor Execution Contract","text":"<p>Input Contract: - Executor receives <code>task.inputs</code> as input - Inputs MAY be validated against <code>task.schemas.input_schema</code> (if present) - Executor MAY access <code>task.schemas</code> and <code>task.params</code> for configuration</p> <p>Output Contract: - Executor MUST return a result object (JSON-serializable) - Result is stored in <code>task.result</code> when task completes - If executor raises/throws an error, task status is set to <code>failed</code> and error is stored in <code>task.error</code></p> <p>Error Contract: - Executor errors MUST be captured and stored in <code>task.error</code> - Error messages SHOULD be descriptive and include context</p>"},{"location":"protocol/02-core-concepts/#node","title":"Node","text":"<p>A Node is a participant in the network that implements the protocol.</p>"},{"location":"protocol/02-core-concepts/#node-capabilities","title":"Node Capabilities","text":"<p>A node can: - Submit Flows: Create and submit task flows for execution - Execute Tasks: Execute tasks using registered executors - Store Results: Persist task state and results - Provide API: Expose protocol-compliant API for external clients</p>"},{"location":"protocol/02-core-concepts/#node-interoperability","title":"Node Interoperability","text":"<p>MUST: Nodes running different language implementations (Python, Go, Rust) MUST be able to communicate as long as they adhere to the Data Protocol.</p> <p>MUST: Nodes MUST use standard data formats (JSON) for communication.</p> <p>SHOULD: Nodes SHOULD validate incoming data against protocol schemas.</p>"},{"location":"protocol/02-core-concepts/#taskmanager","title":"TaskManager","text":"<p>TaskManager is the orchestrator that coordinates task execution, dependency resolution, and priority scheduling.</p>"},{"location":"protocol/02-core-concepts/#taskmanager-responsibilities","title":"TaskManager Responsibilities","text":"<ol> <li>Task Execution: Manages task execution lifecycle</li> <li>Dependency Resolution: Ensures tasks wait for dependencies</li> <li>Priority Scheduling: Executes tasks in priority order</li> <li>State Management: Tracks task state transitions</li> <li>Error Handling: Handles execution failures and errors</li> </ol>"},{"location":"protocol/02-core-concepts/#taskmanager-behavior-specification","title":"TaskManager Behavior Specification","text":""},{"location":"protocol/02-core-concepts/#orchestration-algorithm","title":"Orchestration Algorithm","text":"<p>The TaskManager MUST follow this algorithm for task execution:</p> <pre><code>function executeTaskTree(rootTask):\n    // 1. Validate task tree\n    validateTaskTree(rootTask)\n\n    // 2. Initialize all tasks to pending\n    initializeTasks(rootTask)\n\n    // 3. Execute loop\n    while hasPendingTasks(rootTask):\n        // 3.1. Find ready tasks (dependencies satisfied)\n        readyTasks = findReadyTasks(rootTask)\n\n        // 3.2. Sort by priority\n        sortedTasks = sortByPriority(readyTasks)\n\n        // 3.3. Execute ready tasks (concurrently if possible)\n        for task in sortedTasks:\n            executeTask(task)\n\n        // 3.4. Wait for tasks to complete or fail\n        waitForTasks(sortedTasks)\n\n    // 4. Return final state\n    return rootTask\n</code></pre> <p>MUST: Implementations MUST follow this general algorithm. MAY: Implementations MAY optimize or parallelize execution.</p>"},{"location":"protocol/02-core-concepts/#dependency-resolution","title":"Dependency Resolution","text":"<p>TaskManager MUST resolve dependencies before allowing task execution:</p> <pre><code>function canExecute(task):\n    // Check all dependencies\n    for dependency in task.dependencies:\n        dep_task = getTask(dependency.id)\n\n        // Check if dependency is ready\n        if dep_task.status == \"pending\" or dep_task.status == \"in_progress\":\n            return false  // Not ready\n\n        // Check required dependencies\n        if dependency.required == true:\n            if dep_task.status != \"completed\":\n                return false  // Required dependency failed\n\n    return true  // All dependencies satisfied\n</code></pre> <p>See Execution Lifecycle for detailed dependency resolution rules.</p>"},{"location":"protocol/02-core-concepts/#priority-scheduling","title":"Priority Scheduling","text":"<p>TaskManager MUST schedule tasks by priority:</p> <pre><code>function sortByPriority(tasks):\n    // Group by priority\n    groups = {}\n    for task in tasks:\n        priority = task.priority ?? 2  // Default: 2\n        if priority not in groups:\n            groups[priority] = []\n        groups[priority].append(task)\n\n    // Sort groups (ascending: 0, 1, 2, 3)\n    sorted_groups = sorted(groups.keys())\n\n    // Flatten\n    result = []\n    for priority in sorted_groups:\n        result.extend(groups[priority])\n\n    return result\n</code></pre> <p>MUST: Lower priority values (0) MUST execute before higher values (3). SHOULD: Tasks with the same priority SHOULD be executed fairly (FIFO, round-robin, etc.).</p>"},{"location":"protocol/02-core-concepts/#concurrency-control","title":"Concurrency Control","text":"<p>TaskManager MAY execute multiple tasks concurrently:</p> <p>MUST: Implementations MUST ensure only one execution of a task at a time. SHOULD: Implementations SHOULD support concurrent execution of independent tasks. MAY: Implementations MAY limit concurrency based on available resources.</p>"},{"location":"protocol/02-core-concepts/#taskmanager-interface","title":"TaskManager Interface","text":"<p>TaskManager MUST provide the following operations (language-agnostic):</p> <p><code>distributeTaskTree(taskTree: TaskTreeNode) -&gt; TaskTreeNode</code></p> <p>Executes a task tree with dependency management and priority scheduling.</p> <p>Input: - <code>taskTree</code> (TaskTreeNode): Root task with children</p> <p>Output: - Returns: Updated task tree with execution results</p> <p>MUST: TaskManager MUST execute tasks according to dependencies and priorities. MUST: TaskManager MUST handle errors and update task status accordingly.</p> <p><code>cancelTask(taskId: String, errorMessage: String?) -&gt; void</code></p> <p>Cancels a running task.</p> <p>Input: - <code>taskId</code> (string): Task ID to cancel - <code>errorMessage</code> (string, optional): Cancellation message</p> <p>MUST: TaskManager MUST transition task to <code>cancelled</code> status. SHOULD: TaskManager SHOULD attempt to stop executor execution.</p>"},{"location":"protocol/02-core-concepts/#storage-requirements","title":"Storage Requirements","text":"<p>The protocol requires persistent storage for task state and results.</p>"},{"location":"protocol/02-core-concepts/#storage-interface-specification","title":"Storage Interface Specification","text":"<p>Storage MUST provide the following operations (language-agnostic):</p>"},{"location":"protocol/02-core-concepts/#required-operations","title":"Required Operations","text":"<p><code>createTask(task: Task) -&gt; Task</code></p> <p>Creates a new task in storage.</p> <p>MUST: Storage MUST generate unique UUID for task <code>id</code> if not provided. MUST: Storage MUST persist all task fields.</p> <p><code>getTask(taskId: String) -&gt; Task</code></p> <p>Retrieves a task by ID.</p> <p>MUST: Storage MUST return task if exists, or error if not found.</p> <p><code>updateTask(taskId: String, updates: Object) -&gt; Task</code></p> <p>Updates task fields.</p> <p>MUST: Storage MUST validate updates against task schema. MUST: Storage MUST persist updates atomically.</p> <p><code>deleteTask(taskId: String) -&gt; void</code></p> <p>Deletes a task from storage.</p> <p>MUST: Storage MUST only delete tasks with status <code>pending</code>. MUST: Storage MUST reject deletion if task has children or dependents.</p> <p><code>listTasks(filters: Object) -&gt; Array&lt;Task&gt;</code></p> <p>Lists tasks with optional filters.</p> <p>SHOULD: Storage SHOULD support filtering by <code>status</code>, <code>user_id</code>, etc. SHOULD: Storage SHOULD support pagination (<code>limit</code>, <code>offset</code>).</p> <p><code>getTaskTree(rootTaskId: String) -&gt; TaskTreeNode</code></p> <p>Retrieves complete task tree.</p> <p>MUST: Storage MUST return complete tree structure (all descendants).</p>"},{"location":"protocol/02-core-concepts/#query-operations","title":"Query Operations","text":"<p><code>findDependentTasks(taskId: String) -&gt; Array&lt;Task&gt;</code></p> <p>Finds all tasks that depend on a given task.</p> <p>MUST: Storage MUST return all tasks that reference <code>taskId</code> in their <code>dependencies</code>.</p> <p><code>getChildren(parentId: String) -&gt; Array&lt;Task&gt;</code></p> <p>Gets direct children of a task.</p> <p>MUST: Storage MUST return only direct children (not grandchildren).</p>"},{"location":"protocol/02-core-concepts/#storage-consistency-requirements","title":"Storage Consistency Requirements","text":"<p>MUST: Storage MUST ensure data consistency: - Task state transitions follow state machine rules - Field values are consistent with status - Dependencies reference valid tasks - Parent-child relationships are valid</p> <p>SHOULD: Storage SHOULD use transactions for atomic updates.</p> <p>MAY: Storage MAY support different backends (SQL, NoSQL, in-memory).</p>"},{"location":"protocol/02-core-concepts/#storage-schema","title":"Storage Schema","text":"<p>Storage MUST persist the following task fields:</p> <ul> <li>Identity: <code>id</code>, <code>parent_id</code>, <code>user_id</code></li> <li>Definition: <code>name</code>, <code>status</code>, <code>priority</code></li> <li>Configuration: <code>schemas</code>, <code>params</code></li> <li>Data: <code>inputs</code>, <code>result</code>, <code>error</code></li> <li>Structure: <code>dependencies</code></li> <li>Tracking: <code>progress</code>, <code>created_at</code>, <code>started_at</code>, <code>updated_at</code>, <code>completed_at</code></li> </ul> <p>MUST: Storage MUST persist all required fields. SHOULD: Storage SHOULD index frequently queried fields (<code>id</code>, <code>parent_id</code>, <code>user_id</code>, <code>status</code>).</p>"},{"location":"protocol/02-core-concepts/#extension-points","title":"Extension Points","text":"<p>The protocol provides extension points for custom functionality:</p> <ol> <li>Custom Executors: Implement custom executors for specific use cases</li> <li>Storage Backends: Implement custom storage backends</li> <li>Transport Layers: Implement custom transport layers (beyond HTTP)</li> </ol> <p>MUST: Implementations MUST support custom executors. MAY: Implementations MAY support custom storage backends and transport layers.</p>"},{"location":"protocol/02-core-concepts/#see-also","title":"See Also","text":"<ul> <li>Data Model - Complete task schema and data structures</li> <li>Execution Lifecycle - State machine and execution rules</li> <li>Interface Protocol - API specifications</li> <li>Conformance - Implementation requirements</li> </ul>"},{"location":"protocol/03-data-model/","title":"Data Model","text":""},{"location":"protocol/03-data-model/#complete-field-specification","title":"Complete Field Specification","text":"Field Type Required Default Constraints Description <code>id</code> String (UUID v4) Yes - Must be valid UUID v4 Unique identifier for the task. MUST be unique across all tasks. <code>parent_id</code> String (UUID v4) No <code>null</code> Must be valid UUID v4 if present ID of the parent task (if part of a hierarchy). Used for organizational purposes only, does not affect execution order. <code>user_id</code> String No <code>null</code> Non-empty string if present User identifier for multi-tenant scenarios. Used for access control and filtering. <code>name</code> String Yes - Non-empty string, max 255 chars Name or method identifier of the task. MUST match a registered executor's identifier when <code>schemas.method</code> is present. <code>status</code> String (enum) Yes <code>\"pending\"</code> One of: <code>pending</code>, <code>in_progress</code>, <code>completed</code>, <code>failed</code>, <code>cancelled</code> Current execution state. See Execution Lifecycle for state machine details. <code>priority</code> Integer No <code>2</code> Range: 0-3 (0=urgent, 1=high, 2=normal, 3=low) Execution priority. Lower values = higher priority. Used for scheduling when multiple tasks are ready. <code>inputs</code> Object (JSON) No <code>{}</code> Valid JSON object Runtime input parameters for the executor. Contains the actual data to be processed. <code>schemas</code> Object (JSON) No <code>null</code> Valid JSON object Configuration and method definition. Defines which executor to use and how to validate inputs. <code>params</code> Object (JSON) No <code>null</code> Valid JSON object Additional executor-specific parameters. Used for executor configuration beyond <code>schemas</code>. <code>result</code> Object (JSON) No <code>null</code> Valid JSON object Execution result. MUST be <code>null</code> when status is not <code>completed</code>. SHOULD be populated when status is <code>completed</code>. <code>error</code> String No <code>null</code> Non-empty string if present Error message. MUST be <code>null</code> when status is not <code>failed</code> or <code>cancelled</code>. SHOULD be populated when status is <code>failed</code> or <code>cancelled</code>. <code>dependencies</code> Array No <code>[]</code> Array of Dependency objects List of dependencies that must be satisfied before execution. See Dependency Schema for structure. <code>progress</code> Number (float) No <code>0.0</code> Range: 0.0-1.0 Execution progress as a fraction (0.0 = not started, 1.0 = complete). SHOULD be updated during execution. <code>created_at</code> String (ISO 8601) No Current timestamp Valid ISO 8601 datetime Task creation timestamp. SHOULD be set when task is created. <code>started_at</code> String (ISO 8601) No <code>null</code> Valid ISO 8601 datetime or null Task execution start timestamp. MUST be <code>null</code> when status is <code>pending</code>. SHOULD be set when status transitions to <code>in_progress</code>. <code>updated_at</code> String (ISO 8601) No Current timestamp Valid ISO 8601 datetime Last update timestamp. SHOULD be updated whenever task is modified. <code>completed_at</code> String (ISO 8601) No <code>null</code> Valid ISO 8601 datetime or null Task completion timestamp. MUST be <code>null</code> when status is not terminal (<code>completed</code>, <code>failed</code>, <code>cancelled</code>). SHOULD be set when status transitions to terminal state. <code>origin_type</code> String (enum) No <code>null</code> One of: <code>create</code>, <code>link</code>, <code>copy</code>, <code>archive</code> Origin of the task definition. See below for meaning. <code>original_task_id</code> String (UUID v4) No <code>null</code> Must be valid UUID v4 if present Source task ID if this task was copied, linked, or snapshotted. <code>has_references</code> Boolean No <code>false</code> - Whether this task is referenced/copied by others. <code>schedule_type</code> String (enum) No <code>null</code> One of: <code>once</code>, <code>interval</code>, <code>cron</code>, <code>daily</code>, <code>weekly</code>, <code>monthly</code> Scheduling mode for recurring execution. See Scheduling Fields. <code>schedule_expression</code> String No <code>null</code> Non-empty string if present Schedule expression format depends on <code>schedule_type</code> (e.g., cron string, interval seconds). <code>schedule_enabled</code> Boolean No <code>false</code> - Whether scheduling is enabled for this task. <code>schedule_start_at</code> String (ISO 8601) No <code>null</code> Valid ISO 8601 datetime or null Earliest time the schedule can trigger. <code>schedule_end_at</code> String (ISO 8601) No <code>null</code> Valid ISO 8601 datetime or null Latest time the schedule can trigger (after this, scheduling is disabled). <code>next_run_at</code> String (ISO 8601) No <code>null</code> Valid ISO 8601 datetime or null Next scheduled execution time (computed). <code>last_run_at</code> String (ISO 8601) No <code>null</code> Valid ISO 8601 datetime or null Last time this scheduled task was executed. <code>max_runs</code> Integer No <code>null</code> Non-negative integer if present Maximum number of scheduled runs (null = unlimited). <code>run_count</code> Integer No <code>0</code> Non-negative integer Number of times this scheduled task has executed."},{"location":"protocol/03-data-model/#auxiliary-fields-database-optimization","title":"Auxiliary Fields (Database Optimization)","text":"<p>These fields are not required by the protocol, but are recommended for database implementations to optimize queries and tree operations:</p> Field Type Description <code>task_tree_id</code> String Identifier for the task tree. Used to efficiently query all tasks in a tree. <code>has_children</code> Boolean Indicates if the task has child tasks. Used for fast child lookup. <p>Note: These fields are for implementation convenience and do not affect protocol behavior.</p>"},{"location":"protocol/03-data-model/#origin_type-values","title":"<code>origin_type</code> values","text":"Value Description <code>create</code> Task created freshly <code>link</code> Task linked from another. The source task MUST be in <code>completed</code> status (in principle). <code>copy</code> Task copied from another (can be modified) <code>archive</code> Task archive from another (cannot be modified). The source task MUST be in <code>completed</code> status (in principle)."},{"location":"protocol/03-data-model/#field-relationships-and-constraints","title":"Field Relationships and Constraints","text":"<p>MUST:   - If <code>status</code> is <code>completed</code>, <code>result</code> SHOULD NOT be <code>null</code>.   - If <code>status</code> is <code>failed</code> or <code>cancelled</code>, <code>error</code> SHOULD NOT be <code>null</code>.   - If <code>status</code> is <code>pending</code>, <code>started_at</code> MUST be <code>null</code>.   - If <code>status</code> is <code>in_progress</code>, <code>started_at</code> MUST NOT be <code>null</code>.   - If <code>status</code> is terminal (<code>completed</code>, <code>failed</code>, <code>cancelled</code>), <code>completed_at</code> MUST NOT be <code>null</code>.   - If <code>parent_id</code> is present, the referenced task MUST exist in the same flow.   - All dependency IDs in <code>dependencies</code> MUST reference existing tasks in the same flow.   - <code>progress</code> MUST be in range [0.0, 1.0].   - If <code>origin_type</code> is <code>link</code> or <code>archive</code>, the <code>original_task_id</code> MUST reference a task in <code>completed</code> status (in principle).   - If <code>schedule_enabled</code> is <code>true</code>, <code>schedule_type</code> and <code>schedule_expression</code> MUST NOT be <code>null</code>.   - If <code>max_runs</code> is present, <code>run_count</code> MUST be less than or equal to <code>max_runs</code>.   - If <code>schedule_end_at</code> is present, <code>next_run_at</code> MUST be <code>null</code> or not later than <code>schedule_end_at</code>.</p>"},{"location":"protocol/03-data-model/#scheduling-fields","title":"Scheduling Fields","text":"<p>Scheduling allows a task definition to trigger repeatedly. Scheduling fields do not alter dependency rules; they only determine when a task becomes ready to run.</p> <p>Schedule types: - <code>once</code>: Execute at a single specified time. - <code>interval</code>: Execute every fixed number of seconds. - <code>cron</code>: Execute using a cron expression. - <code>daily</code>: Execute once per day at a specific time. - <code>weekly</code>: Execute on specific weekdays at a specific time. - <code>monthly</code>: Execute on specific dates each month.</p> <p>Schedule expressions: - <code>once</code>: ISO 8601 datetime (e.g., <code>2025-01-20T09:00:00Z</code>). - <code>interval</code>: Integer seconds as a string (e.g., <code>\"3600\"</code>). - <code>cron</code>: Standard 5-field cron (e.g., <code>\"0 9 * * 1-5\"</code>). - <code>daily</code>: <code>HH:MM</code> in 24-hour format (e.g., <code>\"09:30\"</code>). - <code>weekly</code>: <code>WEEKDAY@HH:MM</code> (e.g., <code>\"mon,wed@09:30\"</code>). - <code>monthly</code>: <code>DAY@HH:MM</code> (e.g., <code>\"1,15@09:30\"</code>).</p> <p>Notes: - <code>next_run_at</code> is calculated by the scheduler and is read-only for clients. - When <code>schedule_enabled</code> is <code>false</code>, scheduling fields MAY be present but are ignored. - Scheduling scope is hierarchical: if the root task has scheduling enabled, the schedule applies to the entire task tree. If only a child task has scheduling enabled, the schedule applies only to that child task and its dependency chain.</p>"},{"location":"protocol/03-data-model/#json-schema-definition","title":"JSON Schema Definition","text":"<pre><code>{\n  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n  \"type\": \"object\",\n  \"required\": [\"id\", \"name\", \"status\"],\n  \"properties\": {\n    \"id\": {\n      \"type\": \"string\",\n      \"format\": \"uuid\",\n      \"description\": \"Unique identifier for the task (UUID v4)\"\n    },\n    \"parent_id\": {\n      \"type\": [\"string\", \"null\"],\n      \"format\": \"uuid\",\n      \"description\": \"ID of the parent task (organizational only)\"\n    },\n    \"user_id\": {\n      \"type\": [\"string\", \"null\"],\n      \"minLength\": 1,\n      \"description\": \"User identifier for multi-tenant scenarios\"\n    },\n    \"name\": {\n      \"type\": \"string\",\n      \"minLength\": 1,\n      \"maxLength\": 255,\n      \"description\": \"Name or method identifier of the task\"\n    },\n    \"status\": {\n      \"type\": \"string\",\n      \"enum\": [\"pending\", \"in_progress\", \"completed\", \"failed\", \"cancelled\"],\n      \"description\": \"Current execution state\"\n    },\n    \"priority\": {\n      \"type\": \"integer\",\n      \"minimum\": 0,\n      \"maximum\": 3,\n      \"default\": 2,\n      \"description\": \"Execution priority (0=urgent, 1=high, 2=normal, 3=low)\"\n    },\n    \"inputs\": {\n      \"type\": \"object\",\n      \"default\": {},\n      \"description\": \"Runtime input parameters for the executor\"\n    },\n    \"schemas\": {\n      \"type\": [\"object\", \"null\"],\n      \"description\": \"Configuration and method definition\",\n      \"properties\": {\n        \"type\": {\n          \"type\": \"string\",\n          \"enum\": [\"local\", \"remote\", \"external\"],\n          \"description\": \"Executor type\"\n        },\n        \"method\": {\n          \"type\": \"string\",\n          \"minLength\": 1,\n          \"description\": \"Executor identifier (executor.id). MUST match a registered executor identifier in the ExecutorRegistry.\"\n        },\n        \"input_schema\": {\n          \"type\": \"object\",\n          \"description\": \"JSON Schema (draft-07) defining valid inputs\"\n        },\n        \"model\": {\n          \"type\": \"string\",\n          \"description\": \"Model identifier (for LLM executors)\"\n        }\n      },\n      \"required\": [\"method\"],\n      \"additionalProperties\": true\n    },\n    \"params\": {\n      \"type\": [\"object\", \"null\"],\n      \"description\": \"Additional executor-specific parameters\"\n    },\n    \"result\": {\n      \"type\": [\"object\", \"null\"],\n      \"description\": \"Execution result (populated when status is completed)\"\n    },\n    \"error\": {\n      \"type\": \"string\",\n      \"description\": \"Error message (populated when status is failed or cancelled)\"\n    },\n    \"dependencies\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"#/definitions/dependency\"\n      },\n      \"default\": [],\n      \"description\": \"List of dependencies that must be satisfied before execution\"\n    },\n    \"progress\": {\n      \"type\": \"number\",\n      \"minimum\": 0.0,\n      \"maximum\": 1.0,\n      \"default\": 0.0,\n      \"description\": \"Execution progress as a fraction\"\n    },\n    \"created_at\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\",\n      \"description\": \"Task creation timestamp (ISO 8601)\"\n    },\n    \"started_at\": {\n      \"type\": [\"string\", \"null\"],\n      \"format\": \"date-time\",\n      \"description\": \"Task execution start timestamp (ISO 8601)\"\n    },\n    \"updated_at\": {\n      \"type\": \"string\",\n      \"format\": \"date-time\",\n      \"description\": \"Last update timestamp (ISO 8601)\"\n    },\n    \"completed_at\": {\n      \"type\": [\"string\", \"null\"],\n      \"format\": \"date-time\",\n      \"description\": \"Task completion timestamp (ISO 8601)\"\n    },\n    \"origin_type\": {\n      \"type\": [\"string\", \"null\"],\n      \"enum\": [\"create\", \"link\", \"copy\", \"archive\", null],\n      \"description\": \"Origin of the task definition. One of: create, link, copy, archive. For link/archive, the source task MUST be completed.\"\n    },\n    \"original_task_id\": {\n      \"type\": [\"string\", \"null\"],\n      \"format\": \"uuid\",\n      \"description\": \"Source task ID if this task was copied, linked, or snapshotted.\"\n    },\n    \"has_references\": {\n      \"type\": \"boolean\",\n      \"default\": false,\n      \"description\": \"Whether this task is referenced/copied by others.\"\n    },\n    \"schedule_type\": {\n      \"type\": [\"string\", \"null\"],\n      \"enum\": [\"once\", \"interval\", \"cron\", \"daily\", \"weekly\", \"monthly\", null],\n      \"description\": \"Scheduling mode for recurring execution.\"\n    },\n    \"schedule_expression\": {\n      \"type\": [\"string\", \"null\"],\n      \"description\": \"Schedule expression (format depends on schedule_type).\"\n    },\n    \"schedule_enabled\": {\n      \"type\": \"boolean\",\n      \"default\": false,\n      \"description\": \"Whether scheduling is enabled for this task.\"\n    },\n    \"schedule_start_at\": {\n      \"type\": [\"string\", \"null\"],\n      \"format\": \"date-time\",\n      \"description\": \"Earliest time the schedule can trigger (ISO 8601).\"\n    },\n    \"schedule_end_at\": {\n      \"type\": [\"string\", \"null\"],\n      \"format\": \"date-time\",\n      \"description\": \"Latest time the schedule can trigger (ISO 8601).\"\n    },\n    \"next_run_at\": {\n      \"type\": [\"string\", \"null\"],\n      \"format\": \"date-time\",\n      \"description\": \"Next scheduled execution time (computed).\"\n    },\n    \"last_run_at\": {\n      \"type\": [\"string\", \"null\"],\n      \"format\": \"date-time\",\n      \"description\": \"Last scheduled execution time (ISO 8601).\"\n    },\n    \"max_runs\": {\n      \"type\": [\"integer\", \"null\"],\n      \"minimum\": 0,\n      \"description\": \"Maximum number of scheduled runs (null = unlimited).\"\n    },\n    \"run_count\": {\n      \"type\": \"integer\",\n      \"minimum\": 0,\n      \"default\": 0,\n      \"description\": \"Number of times this scheduled task has executed.\"\n    }\n  },\n  \"definitions\": {\n    \"dependency\": {\n      \"type\": \"object\",\n      \"required\": [\"id\"],\n      \"properties\": {\n        \"id\": {\n          \"type\": \"string\",\n          \"format\": \"uuid\",\n          \"description\": \"ID of the task to depend on\"\n        },\n        \"required\": {\n          \"type\": \"boolean\",\n          \"default\": true,\n          \"description\": \"If true, the dependent task must complete successfully\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"protocol/03-data-model/#complete-task-example","title":"Complete Task Example","text":"<pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"parent_id\": \"660e8400-e29b-41d4-a716-446655440001\",\n  \"user_id\": \"user-123\",\n  \"name\": \"Crawl Website\",\n  \"status\": \"pending\",\n  \"priority\": 1,\n  \"inputs\": {\n    \"url\": \"https://example.com\",\n    \"timeout\": 60\n  },\n  \"schemas\": {\n    \"type\": \"local\",\n    \"method\": \"web_crawler\",\n    \"input_schema\": {\n      \"type\": \"object\",\n      \"required\": [\"url\"],\n      \"properties\": {\n        \"url\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        },\n        \"timeout\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"default\": 30\n        }\n      }\n    }\n  },\n  \"params\": {\n    \"retry_count\": 3,\n    \"user_agent\": \"MyCrawler/1.0\"\n  },\n  \"result\": null,\n  \"error\": null,\n  \"dependencies\": [\n    {\n      \"id\": \"550e8400-e29b-41d4-a716-446655440002\",\n      \"required\": true\n    }\n  ],\n  \"progress\": 0.0,\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"started_at\": null,\n  \"updated_at\": \"2025-01-15T10:30:00Z\",\n  \"completed_at\": null,\n  \"origin_type\": \"copy\",\n  \"original_task_id\": \"550e8400-e29b-41d4-a716-446655440003\",\n  \"has_references\": false,\n  \"schedule_type\": \"weekly\",\n  \"schedule_expression\": \"mon,wed@09:30\",\n  \"schedule_enabled\": true,\n  \"schedule_start_at\": \"2025-01-20T00:00:00Z\",\n  \"schedule_end_at\": null,\n  \"next_run_at\": \"2025-01-22T09:30:00Z\",\n  \"last_run_at\": null,\n  \"max_runs\": null,\n  \"run_count\": 0\n}\n</code></pre>"},{"location":"protocol/03-data-model/#inputs-vs-schemas","title":"Inputs vs Schemas","text":"<p>It is critical to distinguish between <code>inputs</code> and <code>schemas</code>:</p>"},{"location":"protocol/03-data-model/#schemas-static-configuration","title":"<code>schemas</code> - Static Configuration","text":"<p>Purpose: Defines the Method and Static Configuration. It tells the system which executor to use and provides configuration that doesn't change between runs.</p> <p>Structure: <pre><code>{\n  \"type\": \"local\" | \"remote\" | \"external\",\n  \"method\": \"executor_identifier\",\n  \"input_schema\": {\n    // JSON Schema defining valid inputs\n  }\n}\n</code></pre></p> <p>Fields: - <code>type</code> (string, optional): Executor type. Values: <code>\"local\"</code> (same process), <code>\"remote\"</code> (different process), <code>\"external\"</code> (external service). - <code>method</code> (string, required): The executor identifier. MUST be the executor.id (registered executor identifier) used to look up the executor in the <code>ExecutorRegistry</code>. MUST match a registered executor identifier. - <code>input_schema</code> (object, optional): JSON Schema (draft-07) defining what <code>inputs</code> are valid. Used for validation before execution.</p> <p>Example: <pre><code>{\n  \"schemas\": {\n    \"type\": \"local\",\n    \"method\": \"web_crawler\",\n    \"input_schema\": {\n      \"type\": \"object\",\n      \"required\": [\"url\"],\n      \"properties\": {\n        \"url\": {\n          \"type\": \"string\",\n          \"format\": \"uri\",\n          \"description\": \"URL to crawl\"\n        },\n        \"timeout\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"default\": 30,\n          \"description\": \"Timeout in seconds\"\n        }\n      }\n    }\n  }\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#inputs-runtime-data","title":"<code>inputs</code> - Runtime Data","text":"<p>Purpose: Defines the Runtime Data. It contains the actual data to be processed in this specific execution.</p> <p>Structure: Any valid JSON object that conforms to <code>schemas.input_schema</code> (if present).</p> <p>Example: <pre><code>{\n  \"inputs\": {\n    \"url\": \"https://example.com\",\n    \"timeout\": 60\n  }\n}\n</code></pre></p> <p>Validation Rules: - MUST: If <code>schemas.input_schema</code> is present, <code>inputs</code> MUST conform to the schema. - SHOULD: Implementations SHOULD validate <code>inputs</code> against <code>schemas.input_schema</code> before execution. - MAY: If <code>schemas.input_schema</code> is not present, <code>inputs</code> can be any valid JSON object.</p>"},{"location":"protocol/03-data-model/#complete-task-example_1","title":"Complete Task Example","text":"<pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"parent_id\": \"660e8400-e29b-41d4-a716-446655440001\",\n  \"user_id\": \"user-123\",\n  \"name\": \"Crawl Website\",\n  \"status\": \"pending\",\n  \"priority\": 1,\n  \"inputs\": {\n    \"url\": \"https://example.com\",\n    \"timeout\": 60\n  },\n  \"schemas\": {\n    \"type\": \"local\",\n    \"method\": \"web_crawler\",\n    \"input_schema\": {\n      \"type\": \"object\",\n      \"required\": [\"url\"],\n      \"properties\": {\n        \"url\": {\n          \"type\": \"string\",\n          \"format\": \"uri\"\n        },\n        \"timeout\": {\n          \"type\": \"integer\",\n          \"minimum\": 1,\n          \"default\": 30\n        }\n      }\n    }\n  },\n  \"params\": {\n    \"retry_count\": 3,\n    \"user_agent\": \"MyCrawler/1.0\"\n  },\n  \"result\": null,\n  \"error\": null,\n  \"dependencies\": [\n    {\n      \"id\": \"550e8400-e29b-41d4-a716-446655440002\",\n      \"required\": true\n    }\n  ],\n  \"progress\": 0.0,\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"started_at\": null,\n  \"updated_at\": \"2025-01-15T10:30:00Z\",\n  \"completed_at\": null,\n  \"schedule_type\": \"interval\",\n  \"schedule_expression\": \"3600\",\n  \"schedule_enabled\": true,\n  \"schedule_start_at\": \"2025-01-15T10:30:00Z\",\n  \"schedule_end_at\": null,\n  \"next_run_at\": \"2025-01-15T11:30:00Z\",\n  \"last_run_at\": null,\n  \"max_runs\": null,\n  \"run_count\": 0\n}\n</code></pre>"},{"location":"protocol/03-data-model/#dependency-schema","title":"Dependency Schema","text":"<p>Dependencies define the execution order. A task with dependencies will wait for those tasks to complete before executing.</p>"},{"location":"protocol/03-data-model/#dependency-object-specification","title":"Dependency Object Specification","text":"Field Type Required Default Constraints Description <code>id</code> String (UUID v4) Yes - Must be valid UUID v4, must reference existing task ID of the task to depend on. MUST reference a task in the same flow. <code>required</code> Boolean No <code>true</code> - If <code>true</code>, the dependent task MUST complete successfully. If <code>false</code>, the task can execute even if the dependency fails."},{"location":"protocol/03-data-model/#dependency-validation-rules","title":"Dependency Validation Rules","text":"<p>MUST: - All dependency IDs MUST reference existing tasks in the same flow. - A task MUST NOT depend on itself (self-reference). - Dependencies MUST NOT create circular dependencies (see Validation Rules).</p> <p>SHOULD: - Implementations SHOULD validate dependencies when a task is created or updated. - Implementations SHOULD detect circular dependencies and reject invalid task definitions.</p>"},{"location":"protocol/03-data-model/#required-vs-optional-dependencies","title":"Required vs Optional Dependencies","text":""},{"location":"protocol/03-data-model/#required-dependencies-required-true","title":"Required Dependencies (<code>required: true</code>)","text":"<p>Behavior: - The task waits for the dependency to complete. - If the dependency completes successfully (<code>status: \"completed\"</code>), the task can proceed. - If the dependency fails (<code>status: \"failed\"</code>), the task MUST NOT execute (unless explicitly allowed by error handling).</p> <p>Example: <pre><code>{\n  \"dependencies\": [\n    {\n      \"id\": \"task-123\",\n      \"required\": true\n    }\n  ]\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#optional-dependencies-required-false","title":"Optional Dependencies (<code>required: false</code>)","text":"<p>Behavior: - The task waits for the dependency to complete (or fail). - The task can execute regardless of whether the dependency succeeds or fails. - Useful for fallback scenarios or optional data sources.</p> <p>Example: <pre><code>{\n  \"dependencies\": [\n    {\n      \"id\": \"primary-task\",\n      \"required\": false\n    },\n    {\n      \"id\": \"fallback-task\",\n      \"required\": false\n    }\n  ]\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#dependency-resolution-algorithm","title":"Dependency Resolution Algorithm","text":"<p>When determining if a task can execute:</p> <ol> <li>Collect all dependencies: Get all tasks referenced in the <code>dependencies</code> array.</li> <li>Check each dependency:</li> <li>If <code>required: true</code>:<ul> <li>Dependency MUST have <code>status: \"completed\"</code> for the task to proceed.</li> <li>If dependency has <code>status: \"failed\"</code>, the task MUST NOT proceed.</li> </ul> </li> <li>If <code>required: false</code>:<ul> <li>Dependency MUST have a terminal status (<code>completed</code>, <code>failed</code>, or <code>cancelled</code>).</li> <li>Task can proceed regardless of dependency outcome.</li> </ul> </li> <li>All dependencies satisfied: Task can transition from <code>pending</code> to <code>in_progress</code>.</li> </ol> <p>See Execution Lifecycle for detailed state machine behavior.</p>"},{"location":"protocol/03-data-model/#task-tree-structure","title":"Task Tree Structure","text":"<p>Tasks are organized in a hierarchical tree structure. The tree represents both organizational relationships (parent-child) and execution dependencies.</p>"},{"location":"protocol/03-data-model/#tasktreenode-specification","title":"TaskTreeNode Specification","text":"<p>A TaskTreeNode wraps a Task and its children in a recursive structure.</p> <p>Structure: <pre><code>{\n  \"task\": {\n    // Complete Task object (see Task Schema above)\n  },\n  \"children\": [\n    // Array of TaskTreeNode objects (recursive)\n  ]\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#field-specification","title":"Field Specification","text":"Field Type Required Description <code>task</code> Task Object Yes The task object. MUST conform to Task Schema. <code>children</code> Array of TaskTreeNode No Child nodes. Empty array <code>[]</code> if no children."},{"location":"protocol/03-data-model/#tree-validation-rules","title":"Tree Validation Rules","text":"<p>MUST: - The root task MUST have <code>parent_id: null</code>. - All child tasks MUST have <code>parent_id</code> matching their parent's <code>id</code>. - The tree MUST be acyclic (no circular parent-child relationships). - All tasks in the tree MUST belong to the same flow (same root).</p> <p>SHOULD: - Implementations SHOULD validate tree structure when building or updating task trees. - Implementations SHOULD detect and reject invalid tree structures.</p>"},{"location":"protocol/03-data-model/#parent-child-vs-dependencies","title":"Parent-Child vs Dependencies","text":"<p>Critical Distinction:</p> <ul> <li><code>parent_id</code> (Parent-Child): Organizational relationship only. Used for grouping and visualization. Does NOT affect execution order.</li> <li><code>dependencies</code>: Execution control. Determines when tasks run. Controls actual execution sequence.</li> </ul> <p>Example: <pre><code>{\n  \"task\": {\n    \"id\": \"task-a\",\n    \"name\": \"Task A\",\n    \"parent_id\": \"root-task\"\n  },\n  \"children\": [\n    {\n      \"task\": {\n        \"id\": \"task-b\",\n        \"name\": \"Task B\",\n        \"parent_id\": \"task-a\",  // Organizational: B is child of A\n        \"dependencies\": [\n          {\n            \"id\": \"task-c\",  // Execution: B waits for C (not A!)\n            \"required\": true\n          }\n        ]\n      },\n      \"children\": []\n    }\n  ]\n}\n</code></pre></p> <p>In this example: - Task B is organizationally a child of Task A. - Task B executionally depends on Task C (not Task A). - Execution order: Task C runs first, then Task B (regardless of parent-child relationship).</p>"},{"location":"protocol/03-data-model/#tree-serialization-format","title":"Tree Serialization Format","text":"<p>When serializing a task tree for transmission:</p> <p>MUST: - Include complete Task objects (all fields). - Preserve parent-child relationships. - Preserve dependency relationships. - Maintain tree structure integrity.</p> <p>SHOULD: - Use recursive JSON structure (TaskTreeNode with nested children). - Include all tasks in the tree, not just the root.</p> <p>Example: <pre><code>{\n  \"task\": {\n    \"id\": \"root-task\",\n    \"name\": \"Root Task\",\n    \"status\": \"pending\",\n    \"parent_id\": null\n  },\n  \"children\": [\n    {\n      \"task\": {\n        \"id\": \"child-1\",\n        \"name\": \"Child Task 1\",\n        \"status\": \"pending\",\n        \"parent_id\": \"root-task\"\n      },\n      \"children\": []\n    },\n    {\n      \"task\": {\n        \"id\": \"child-2\",\n        \"name\": \"Child Task 2\",\n        \"status\": \"pending\",\n        \"parent_id\": \"root-task\",\n        \"dependencies\": [\n          {\n            \"id\": \"child-1\",\n            \"required\": true\n          }\n        ]\n      },\n      \"children\": []\n    }\n  ]\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#result-and-error-formats","title":"Result and Error Formats","text":""},{"location":"protocol/03-data-model/#result-format","title":"Result Format","text":"<p>When a task completes successfully (<code>status: \"completed\"</code>), the <code>result</code> field contains the execution result.</p> <p>Structure: Any valid JSON object. The structure is executor-specific.</p> <p>MUST: - <code>result</code> MUST be a valid JSON object (not a primitive value). - <code>result</code> MUST be <code>null</code> when status is not <code>completed</code>. - <code>result</code> SHOULD be populated when status is <code>completed</code>.</p> <p>Example: <pre><code>{\n  \"status\": \"completed\",\n  \"result\": {\n    \"output\": \"Processed data\",\n    \"metadata\": {\n      \"processing_time\": 1.5,\n      \"items_processed\": 100\n    }\n  }\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#error-format","title":"Error Format","text":"<p>When a task fails (<code>status: \"failed\"</code> or <code>status: \"cancelled\"</code>), the <code>error</code> field contains the error message.</p> <p>Structure: String containing the error message.</p> <p>MUST: - <code>error</code> MUST be a non-empty string when status is <code>failed</code> or <code>cancelled</code>. - <code>error</code> MUST be <code>null</code> when status is not <code>failed</code> or <code>cancelled</code>.</p> <p>SHOULD: - Error messages SHOULD be human-readable. - Error messages SHOULD include context about what failed.</p> <p>Example: <pre><code>{\n  \"status\": \"failed\",\n  \"error\": \"Executor 'web_crawler' not found in registry\"\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#progress-format","title":"Progress Format","text":"<p>The <code>progress</code> field tracks execution progress as a fraction.</p> <p>Type: Number (float) Range: 0.0 to 1.0 Default: 0.0</p> <p>MUST: - <code>progress</code> MUST be in range [0.0, 1.0]. - <code>progress</code> SHOULD be updated during execution to reflect current progress.</p> <p>Example: <pre><code>{\n  \"status\": \"in_progress\",\n  \"progress\": 0.65\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#executor-registration","title":"Executor Registration","text":"<p>The protocol supports extending functionality via custom executors. Executors are registered with an <code>ExecutorRegistry</code> using a unique identifier (the <code>method</code> name).</p>"},{"location":"protocol/03-data-model/#executor-categories","title":"Executor Categories","text":"<p>Executors can be categorized by their source and availability:</p> <ol> <li>Built-in Executors: Provided by the framework implementation</li> <li>System executors (e.g., <code>system_info_executor</code>, <code>command_executor</code>)</li> <li>Data processing executors (e.g., <code>aggregate_results_executor</code>)</li> <li>Tool executors (e.g., GitHub tools, web scraping tools)</li> <li> <p>These are available without additional dependencies</p> </li> <li> <p>Optional Executors: Provided by optional extensions</p> </li> <li>LLM executors (e.g., <code>crew_manager</code> for CrewAI) - requires AI/LLM dependencies</li> <li>HTTP executors - may require additional dependencies</li> <li> <p>Specialized executors for specific use cases</p> </li> <li> <p>Custom Executors: Created by users</p> </li> <li>User-defined executors implementing the Executor interface</li> <li>Registered with ExecutorRegistry using unique identifiers</li> <li>Can implement any business logic or integration</li> </ol> <p>MUST: Implementations MUST provide at least one executor type (built-in or custom).</p> <p>SHOULD: Implementations SHOULD provide common built-in executors for system operations.</p> <p>MAY: Implementations MAY provide optional executors for specialized use cases.</p> <p>Note: The specific executor identifiers (e.g., <code>system_info_executor</code>, <code>command_executor</code>) are implementation-specific. The protocol only specifies the registration and lookup mechanism, not the specific executor names that must be provided.</p>"},{"location":"protocol/03-data-model/#executor-identifier","title":"Executor Identifier","text":"<p>The <code>method</code> field in <code>schemas</code> is used to look up the executor in the registry. The value of <code>schemas.method</code> MUST be the executor.id (the registered executor identifier).</p> <p>MUST: - <code>method</code> MUST be a non-empty string. - <code>method</code> MUST be the executor.id (registered executor identifier). - <code>method</code> MUST match a registered executor identifier in the ExecutorRegistry. - Executor identifiers MUST be unique within a registry.</p> <p>SHOULD: - Executor identifiers SHOULD be descriptive (e.g., <code>\"web_crawler\"</code> not <code>\"exec1\"</code>). - Executor identifiers SHOULD follow a consistent naming convention.</p>"},{"location":"protocol/03-data-model/#registration-mechanism","title":"Registration Mechanism","text":"<p>Language-Agnostic Specification:</p> <ol> <li>Registry: Implementations MUST provide an <code>ExecutorRegistry</code> that maps executor identifiers to executor instances.</li> <li>Registration: Executors MUST be registered before they can be used in tasks.</li> <li>Lookup: When a task is executed, the system MUST look up the executor using <code>schemas.method</code>.</li> <li>Error Handling: If an executor is not found, the task MUST fail with an appropriate error.</li> </ol> <p>Implementation Note: The specific registration mechanism (decorators, function calls, configuration files) is implementation-specific. The protocol only specifies that executors must be registered and accessible via the <code>method</code> identifier.</p>"},{"location":"protocol/03-data-model/#example-registration-conceptual","title":"Example Registration (Conceptual)","text":"<pre><code># Python example (for reference only - protocol is language-agnostic)\n@executor_registry.register(\"my_custom_analyzer\")\nclass MyCustomAnalyzer:\n    async def execute(self, inputs):\n        # Implementation\n        pass\n</code></pre> <p>In the Task JSON: <pre><code>{\n  \"schemas\": {\n    \"method\": \"my_custom_analyzer\"\n  }\n}\n</code></pre></p>"},{"location":"protocol/03-data-model/#validation-rules","title":"Validation Rules","text":""},{"location":"protocol/03-data-model/#task-validation","title":"Task Validation","text":"<p>When creating or updating a task, implementations MUST validate:</p> <ol> <li>Required Fields: <code>id</code>, <code>name</code>, <code>status</code> are present.</li> <li>Field Types: All fields match their specified types.</li> <li>Field Constraints: All constraints are satisfied (e.g., <code>priority</code> in range 0-3).</li> <li>UUID Format: <code>id</code>, <code>parent_id</code>, and dependency IDs are valid UUIDs.</li> <li>Status Consistency: Status matches field values (e.g., <code>result</code> is null when status is not <code>completed</code>).</li> <li>Dependency References: All dependency IDs reference existing tasks.</li> <li>Circular Dependencies: No circular dependency chains exist.</li> <li>Input Schema: If <code>schemas.input_schema</code> is present, <code>inputs</code> conforms to it.</li> </ol>"},{"location":"protocol/03-data-model/#dependency-validation","title":"Dependency Validation","text":"<p>When validating dependencies:</p> <ol> <li>Reference Validation: All dependency IDs MUST reference existing tasks.</li> <li>Self-Reference: A task MUST NOT depend on itself.</li> <li>Circular Dependency Detection: </li> <li>Build dependency graph.</li> <li>Check for cycles using depth-first search (DFS).</li> <li>Reject if cycles are detected.</li> </ol> <p>Algorithm for Circular Dependency Detection:</p> <pre><code>function hasCircularDependency(task, visited, recStack):\n    visited[task.id] = true\n    recStack[task.id] = true\n\n    for each dependency in task.dependencies:\n        dep_task = getTask(dependency.id)\n        if not visited[dep_task.id]:\n            if hasCircularDependency(dep_task, visited, recStack):\n                return true\n        else if recStack[dep_task.id]:\n            return true  // Cycle detected\n\n    recStack[task.id] = false\n    return false\n</code></pre>"},{"location":"protocol/03-data-model/#tree-validation","title":"Tree Validation","text":"<p>When validating a task tree:</p> <ol> <li>Root Task: Exactly one root task exists (<code>parent_id: null</code>).</li> <li>Parent-Child Consistency: All <code>parent_id</code> values reference valid parent tasks in the tree.</li> <li>Acyclic Structure: No circular parent-child relationships.</li> <li>Dependency References: All dependency IDs reference tasks within the same tree.</li> </ol>"},{"location":"protocol/03-data-model/#see-also","title":"See Also","text":"<ul> <li>Execution Lifecycle - State machine and execution rules</li> <li>Core Concepts - Fundamental protocol concepts</li> <li>Validation - Detailed validation algorithms</li> <li>Error Handling - Error codes and handling</li> </ul>"},{"location":"protocol/04-execution-lifecycle/","title":"Execution Lifecycle","text":"<p>The Execution Lifecycle defines how a Task transitions between states and how the system manages execution order. This specification is mandatory for all implementations.</p>"},{"location":"protocol/04-execution-lifecycle/#provenance-constraints","title":"Provenance Constraints","text":"<p>For tasks with <code>origin_type</code> of <code>link</code> or <code>archive</code>, the referenced source task (<code>original_task_id</code>) MUST be in <code>completed</code> status (in principle) before the new task is created or executed. This ensures that only completed results are linked or snapshotted for further use.</p>"},{"location":"protocol/04-execution-lifecycle/#state-machine","title":"State Machine","text":"<p>A Task transitions through a well-defined set of states. Implementations MUST enforce valid state transitions and MUST NOT allow invalid transitions.</p>"},{"location":"protocol/04-execution-lifecycle/#states","title":"States","text":"State Value Description Pending <code>pending</code> The task is created but not yet started. It may be waiting for dependencies or a worker slot. In Progress <code>in_progress</code> The task is currently being executed by an Executor. Completed <code>completed</code> The task finished successfully. <code>result</code> field is populated. Failed <code>failed</code> The task encountered an error. <code>error</code> field is populated. Cancelled <code>cancelled</code> The task was manually stopped. <code>error</code> field may contain cancellation reason."},{"location":"protocol/04-execution-lifecycle/#state-categories","title":"State Categories","text":"<ul> <li>Initial State: <code>pending</code> (tasks start in this state)</li> <li>Active State: <code>in_progress</code> (task is executing)</li> <li>Terminal States: <code>completed</code>, <code>failed</code>, <code>cancelled</code> (task execution has ended)</li> </ul>"},{"location":"protocol/04-execution-lifecycle/#formal-state-transition-diagram","title":"Formal State Transition Diagram","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; pending: Task Created\n    pending --&gt; in_progress: Dependencies Satisfied&lt;br/&gt;Executor Available\n    pending --&gt; cancelled: Cancel Requested\n    in_progress --&gt; completed: Execution Succeeds\n    in_progress --&gt; failed: Execution Fails\n    in_progress --&gt; cancelled: Cancel Requested\n    failed --&gt; pending: Re-execution Requested\n    cancelled --&gt; [*]: Terminal State\n    completed --&gt; [*]: Terminal State\n    failed --&gt; [*]: Terminal State (if not re-executed)</code></pre>"},{"location":"protocol/04-execution-lifecycle/#valid-state-transitions","title":"Valid State Transitions","text":"<p>MUST: Implementations MUST only allow the following transitions:</p> From To Condition <code>pending</code> <code>in_progress</code> All dependencies satisfied, executor available <code>pending</code> <code>cancelled</code> Cancel requested <code>in_progress</code> <code>completed</code> Execution succeeds <code>in_progress</code> <code>failed</code> Execution fails <code>in_progress</code> <code>cancelled</code> Cancel requested <code>failed</code> <code>pending</code> Re-execution requested <p>MUST NOT: Implementations MUST NOT allow: - Direct transition from <code>pending</code> to <code>completed</code> or <code>failed</code> (must go through <code>in_progress</code>) - Transition from <code>completed</code> to any other state (terminal state) - Transition from <code>cancelled</code> to any other state (terminal state, unless re-execution is explicitly supported) - Any transition not listed above</p>"},{"location":"protocol/04-execution-lifecycle/#state-transition-conditions","title":"State Transition Conditions","text":""},{"location":"protocol/04-execution-lifecycle/#transition-pending-in_progress","title":"Transition: <code>pending</code> \u2192 <code>in_progress</code>","text":"<p>Conditions (ALL must be true): 1. Task status is <code>pending</code> 2. All dependencies are satisfied (see Dependency Resolution) 3. An executor is available (or can be instantiated) 4. No cancellation request is pending</p> <p>Actions (MUST be performed): 1. Set <code>status</code> to <code>in_progress</code> 2. Set <code>started_at</code> to current timestamp (ISO 8601, UTC) 3. Update <code>updated_at</code> to current timestamp 4. Invoke executor's <code>execute()</code> method with task <code>inputs</code></p>"},{"location":"protocol/04-execution-lifecycle/#transition-in_progress-completed","title":"Transition: <code>in_progress</code> \u2192 <code>completed</code>","text":"<p>Conditions: 1. Task status is <code>in_progress</code> 2. Executor's <code>execute()</code> method returns successfully 3. Result is available</p> <p>Actions (MUST be performed): 1. Set <code>status</code> to <code>completed</code> 2. Set <code>result</code> to executor's return value 3. Set <code>progress</code> to <code>1.0</code> 4. Set <code>completed_at</code> to current timestamp (ISO 8601, UTC) 5. Update <code>updated_at</code> to current timestamp 6. Trigger dependent tasks to check if they can proceed</p>"},{"location":"protocol/04-execution-lifecycle/#transition-in_progress-failed","title":"Transition: <code>in_progress</code> \u2192 <code>failed</code>","text":"<p>Conditions: 1. Task status is <code>in_progress</code> 2. Executor's <code>execute()</code> method raises an exception or returns an error</p> <p>Actions (MUST be performed): 1. Set <code>status</code> to <code>failed</code> 2. Set <code>error</code> to error message (string) 3. Set <code>progress</code> to current progress (if available) or <code>0.0</code> 4. Set <code>completed_at</code> to current timestamp (ISO 8601, UTC) 5. Update <code>updated_at</code> to current timestamp 6. Handle dependent tasks according to dependency <code>required</code> flag</p>"},{"location":"protocol/04-execution-lifecycle/#transition-in_progress-cancelled","title":"Transition: <code>in_progress</code> \u2192 <code>cancelled</code>","text":"<p>Conditions: 1. Task status is <code>in_progress</code> 2. Cancel request is received</p> <p>Actions (MUST be performed): 1. Set <code>status</code> to <code>cancelled</code> 2. Set <code>error</code> to cancellation message (if provided) 3. Set <code>progress</code> to current progress (if available) 4. Set <code>completed_at</code> to current timestamp (ISO 8601, UTC) 5. Update <code>updated_at</code> to current timestamp 6. If executor supports cancellation, invoke <code>cancel()</code> method 7. Handle dependent tasks according to dependency <code>required</code> flag</p>"},{"location":"protocol/04-execution-lifecycle/#transition-pending-cancelled","title":"Transition: <code>pending</code> \u2192 <code>cancelled</code>","text":"<p>Conditions: 1. Task status is <code>pending</code> 2. Cancel request is received</p> <p>Actions (MUST be performed): 1. Set <code>status</code> to <code>cancelled</code> 2. Set <code>error</code> to cancellation message (if provided) 3. Set <code>completed_at</code> to current timestamp (ISO 8601, UTC) 4. Update <code>updated_at</code> to current timestamp 5. Handle dependent tasks according to dependency <code>required</code> flag</p>"},{"location":"protocol/04-execution-lifecycle/#transition-failed-pending-re-execution","title":"Transition: <code>failed</code> \u2192 <code>pending</code> (Re-execution)","text":"<p>Conditions: 1. Task status is <code>failed</code> 2. Re-execution is requested</p> <p>Actions (MUST be performed): 1. Set <code>status</code> to <code>pending</code> 2. Set <code>error</code> to <code>null</code> 3. Set <code>progress</code> to <code>0.0</code> 4. Set <code>started_at</code> to <code>null</code> 5. Set <code>completed_at</code> to <code>null</code> 6. Update <code>updated_at</code> to current timestamp 7. Optionally: Reset <code>result</code> to <code>null</code> (implementation-specific)</p> <p>SHOULD: Implementations SHOULD also handle cascading re-execution (see Re-execution).</p>"},{"location":"protocol/04-execution-lifecycle/#invalid-transition-handling","title":"Invalid Transition Handling","text":"<p>MUST: If an invalid transition is attempted, implementations MUST: 1. Reject the transition 2. Keep the task in its current state 3. Return an error indicating the invalid transition</p> <p>Example Error: <pre><code>{\n  \"error\": \"Invalid state transition: cannot transition from 'completed' to 'in_progress'\"\n}\n</code></pre></p>"},{"location":"protocol/04-execution-lifecycle/#execution-rules","title":"Execution Rules","text":""},{"location":"protocol/04-execution-lifecycle/#scheduling-triggering-only","title":"Scheduling (Triggering Only)","text":"<p>Scheduling only determines when tasks are triggered. It does not change dependency resolution, priority ordering, or state transition rules in this document.</p> <p>Scope rule: - If the root task has scheduling enabled, the schedule applies to the entire task tree. - If only a child task has scheduling enabled, the schedule applies only to that child task and its dependency chain.</p>"},{"location":"protocol/04-execution-lifecycle/#priority-scheduling","title":"Priority Scheduling","text":"<p>Tasks are executed based on their <code>priority</code> field. Lower values indicate higher priority.</p>"},{"location":"protocol/04-execution-lifecycle/#priority-values","title":"Priority Values","text":"Priority Value Description Urgent <code>0</code> Highest priority. Execute first when ready. High <code>1</code> High priority. Execute after urgent tasks. Normal <code>2</code> Default priority. Execute after high priority tasks. Low <code>3</code> Lowest priority. Execute last when ready."},{"location":"protocol/04-execution-lifecycle/#priority-scheduling-algorithm","title":"Priority Scheduling Algorithm","text":"<p>When multiple tasks are ready to execute (dependencies satisfied), they MUST be scheduled according to priority:</p> <ol> <li>Group by Priority: Group ready tasks by priority value.</li> <li>Sort Groups: Process groups in ascending order (0, then 1, then 2, then 3).</li> <li>Within Group: Tasks with the same priority MAY be executed in any order (implementation-specific, e.g., FIFO, round-robin).</li> </ol> <p>Algorithm (pseudocode): <pre><code>function scheduleTasks(readyTasks):\n    // Group by priority\n    groups = {}\n    for task in readyTasks:\n        priority = task.priority ?? 2  // Default: 2\n        if priority not in groups:\n            groups[priority] = []\n        groups[priority].append(task)\n\n    // Execute in priority order\n    for priority in sorted(groups.keys()):\n        tasks = groups[priority]\n        // Execute tasks in this priority group\n        // (order within group is implementation-specific)\n        for task in tasks:\n            executeTask(task)\n</code></pre></p> <p>MUST: Implementations MUST respect priority ordering when scheduling tasks.</p> <p>SHOULD: Implementations SHOULD execute tasks with the same priority fairly (e.g., FIFO, round-robin).</p> <p>Note: Priority only affects scheduling when tasks are ready (dependencies satisfied). Dependencies take precedence over priority.</p>"},{"location":"protocol/04-execution-lifecycle/#dependency-resolution","title":"Dependency Resolution","text":"<p>A task cannot start until all its dependencies are satisfied. Dependency resolution determines when a task is ready to execute.</p>"},{"location":"protocol/04-execution-lifecycle/#dependency-satisfaction-rules","title":"Dependency Satisfaction Rules","text":"<p>For a dependency to be satisfied:</p> <ol> <li>Required Dependency (<code>required: true</code>):</li> <li>Dependent task MUST have <code>status: \"completed\"</code></li> <li> <p>If dependent task has <code>status: \"failed\"</code>, dependency is NOT satisfied, and the task MUST NOT execute.</p> </li> <li> <p>Optional Dependency (<code>required: false</code>):</p> </li> <li>Dependent task MUST have a terminal status (<code>completed</code>, <code>failed</code>, or <code>cancelled</code>)</li> <li>Task can execute regardless of whether dependency succeeded or failed.</li> </ol>"},{"location":"protocol/04-execution-lifecycle/#dependency-resolution-algorithm","title":"Dependency Resolution Algorithm","text":"<pre><code>function canExecute(task):\n    if task.status != \"pending\":\n        return false  // Only pending tasks can start\n\n    for dependency in task.dependencies:\n        dep_task = getTask(dependency.id)\n\n        if dep_task.status == \"pending\" or dep_task.status == \"in_progress\":\n            return false  // Dependency not ready\n\n        if dependency.required == true:\n            if dep_task.status != \"completed\":\n                return false  // Required dependency failed\n\n    return true  // All dependencies satisfied\n</code></pre> <p>MUST: Implementations MUST check all dependencies before allowing a task to transition from <code>pending</code> to <code>in_progress</code>.</p> <p>MUST: Implementations MUST respect the <code>required</code> flag for each dependency.</p>"},{"location":"protocol/04-execution-lifecycle/#dependency-failure-handling","title":"Dependency Failure Handling","text":"<p>When a required dependency fails:</p> <ol> <li>Block Execution: The dependent task MUST NOT execute.</li> <li>Status: The dependent task remains in <code>pending</code> status.</li> <li>Error Propagation: The dependent task MAY inherit error information (implementation-specific).</li> </ol> <p>When an optional dependency fails:</p> <ol> <li>Allow Execution: The dependent task CAN execute.</li> <li>Status: The dependent task can transition to <code>in_progress</code> when other dependencies are satisfied.</li> </ol>"},{"location":"protocol/04-execution-lifecycle/#concurrent-execution","title":"Concurrent Execution","text":"<p>Multiple tasks MAY execute concurrently if: 1. Their dependencies are satisfied 2. Executors are available 3. No explicit serialization constraint exists</p> <p>MUST: Implementations MUST support concurrent execution of independent tasks.</p> <p>SHOULD: Implementations SHOULD limit concurrent execution based on available resources (e.g., thread pool size, executor capacity).</p> <p>MAY: Implementations MAY provide configuration for concurrency limits.</p>"},{"location":"protocol/04-execution-lifecycle/#task-cancellation","title":"Task Cancellation","text":"<p>Tasks can be cancelled at any point during execution.</p>"},{"location":"protocol/04-execution-lifecycle/#cancellation-rules","title":"Cancellation Rules","text":"<p>MUST: - Implementations MUST support cancellation of <code>pending</code> and <code>in_progress</code> tasks. - Cancellation MUST transition the task to <code>cancelled</code> status. - Cancellation MUST set the <code>error</code> field (if a cancellation message is provided).</p> <p>SHOULD: - Implementations SHOULD attempt to stop executor execution when cancelling <code>in_progress</code> tasks. - Implementations SHOULD handle dependent tasks appropriately (see Dependency Failure Handling).</p> <p>MAY: - Implementations MAY support cancellation of terminal states (no-op or error).</p>"},{"location":"protocol/04-execution-lifecycle/#cancellation-behavior","title":"Cancellation Behavior","text":"<ol> <li>Pending Tasks: Immediate cancellation (no executor running).</li> <li>In Progress Tasks: </li> <li>If executor supports cancellation, invoke <code>cancel()</code> method.</li> <li>If executor does not support cancellation, mark as cancelled but executor may continue running (implementation-specific).</li> </ol>"},{"location":"protocol/04-execution-lifecycle/#cancellation-impact-on-dependencies","title":"Cancellation Impact on Dependencies","text":"<p>When a task is cancelled:</p> <ol> <li>Required Dependencies: Tasks depending on a cancelled task with <code>required: true</code> MUST NOT execute.</li> <li>Optional Dependencies: Tasks depending on a cancelled task with <code>required: false</code> CAN execute.</li> </ol>"},{"location":"protocol/04-execution-lifecycle/#re-execution","title":"Re-execution","text":"<p>The protocol supports re-execution of tasks. Re-execution allows failed tasks to be retried or tasks to be re-run with different inputs.</p>"},{"location":"protocol/04-execution-lifecycle/#re-execution-rules","title":"Re-execution Rules","text":""},{"location":"protocol/04-execution-lifecycle/#when-re-execution-is-allowed","title":"When Re-execution is Allowed","text":"<p>MUST: Implementations MUST support re-execution of tasks with status <code>failed</code>.</p> <p>SHOULD: Implementations SHOULD support re-execution of tasks with status <code>completed</code> (for testing, debugging, or re-processing).</p> <p>MAY: Implementations MAY support re-execution of tasks with status <code>cancelled</code>.</p>"},{"location":"protocol/04-execution-lifecycle/#re-execution-process","title":"Re-execution Process","text":"<ol> <li>Reset State: Task status transitions from terminal state to <code>pending</code>.</li> <li>Clear Results: <code>result</code> and <code>error</code> fields are reset (set to <code>null</code>).</li> <li>Reset Progress: <code>progress</code> is reset to <code>0.0</code>.</li> <li>Reset Timestamps: <code>started_at</code> and <code>completed_at</code> are reset to <code>null</code>.</li> <li>Preserve Definition: Task definition (<code>name</code>, <code>inputs</code>, <code>schemas</code>, <code>dependencies</code>) is preserved.</li> </ol>"},{"location":"protocol/04-execution-lifecycle/#cascading-re-execution","title":"Cascading Re-execution","text":"<p>When a task is re-executed, tasks that depend on it MAY need to be re-executed to ensure consistency.</p> <p>Rules: 1. Direct Dependents: Tasks that directly depend on the re-executed task SHOULD be marked for re-execution if they have already completed. 2. Transitive Dependents: Tasks that transitively depend on the re-executed task SHOULD be marked for re-execution. 3. Status Check: Only tasks with terminal status (<code>completed</code>, <code>failed</code>, <code>cancelled</code>) are candidates for cascading re-execution.</p> <p>Algorithm (pseudocode): <pre><code>function cascadeReexecution(task):\n    visited = set()\n\n    function markForReexecution(current_task):\n        if current_task.id in visited:\n            return\n        visited.add(current_task.id)\n\n        if current_task.status in [\"completed\", \"failed\", \"cancelled\"]:\n            // Reset to pending\n            current_task.status = \"pending\"\n            current_task.result = null\n            current_task.error = null\n            current_task.progress = 0.0\n            current_task.started_at = null\n            current_task.completed_at = null\n\n        // Recursively mark dependents\n        dependents = findDependentTasks(current_task.id)\n        for dependent in dependents:\n            markForReexecution(dependent)\n\n    markForReexecution(task)\n</code></pre></p> <p>SHOULD: Implementations SHOULD support cascading re-execution to maintain data consistency.</p> <p>MAY: Implementations MAY provide configuration to disable cascading re-execution.</p>"},{"location":"protocol/04-execution-lifecycle/#copy-execution","title":"Copy Execution","text":"<p>Copy execution creates a new copy of an existing task (and optionally its children) for re-execution without modifying the original task.</p>"},{"location":"protocol/04-execution-lifecycle/#copy-execution-rules","title":"Copy Execution Rules","text":"<p>MUST: Implementations MUST support copying tasks before execution.</p> <p>SHOULD: Implementations SHOULD support copying task trees (task + children).</p>"},{"location":"protocol/04-execution-lifecycle/#what-gets-copied","title":"What Gets Copied","text":"<p>Copied: - Task definition (<code>name</code>, <code>inputs</code>, <code>schemas</code>, <code>params</code>, <code>dependencies</code>) - Task hierarchy (parent-child relationships) - Dependencies (when <code>copy_children: true</code>)</p> <p>Reset: - <code>status</code>: Set to <code>pending</code> - <code>result</code>: Set to <code>null</code> - <code>error</code>: Set to <code>null</code> - <code>progress</code>: Set to <code>0.0</code> - <code>started_at</code>: Set to <code>null</code> - <code>completed_at</code>: Set to <code>null</code></p> <p>New: - <code>id</code>: New UUID generated - <code>created_at</code>: Current timestamp - <code>updated_at</code>: Current timestamp</p> <p>Preserved (in original): - Original task remains completely unchanged - Original task's execution history is preserved</p>"},{"location":"protocol/04-execution-lifecycle/#copy-children-behavior","title":"Copy Children Behavior","text":"<p>When <code>copy_children: true</code>: 1. Each direct child of the original task is copied. 2. All dependencies of copied children are included. 3. Tasks that depend on multiple copied children are only copied once (deduplication).</p> <p>Example: <pre><code>Original Tree:\n- Parent (id: parent-1)\n  - Child 1 (id: child-1, depends on: dep-1)\n  - Child 2 (id: child-2, depends on: dep-1, dep-2)\n  - Dependency 1 (id: dep-1)\n  - Dependency 2 (id: dep-2)\n\nAfter copy_execution=true, copy_children=true:\n- Copied Parent (id: parent-1-copy)\n  - Copied Child 1 (id: child-1-copy, depends on: dep-1-copy)\n  - Copied Child 2 (id: child-2-copy, depends on: dep-1-copy, dep-2-copy)\n  - Copied Dependency 1 (id: dep-1-copy) - copied only once!\n  - Copied Dependency 2 (id: dep-2-copy)\n</code></pre></p>"},{"location":"protocol/04-execution-lifecycle/#error-handling","title":"Error Handling","text":""},{"location":"protocol/04-execution-lifecycle/#error-propagation","title":"Error Propagation","text":"<p>When a task fails, the error information is stored in the <code>error</code> field.</p> <p>MUST: Implementations MUST set the <code>error</code> field when a task fails.</p> <p>SHOULD: Error messages SHOULD be descriptive and include context about what failed.</p>"},{"location":"protocol/04-execution-lifecycle/#failure-handling-strategies","title":"Failure Handling Strategies","text":""},{"location":"protocol/04-execution-lifecycle/#required-dependency-failure","title":"Required Dependency Failure","text":"<p>When a required dependency fails:</p> <ol> <li>Block Execution: Dependent task MUST NOT execute.</li> <li>Status: Dependent task remains in <code>pending</code> status.</li> <li>Error Information: Dependent task MAY inherit error information (implementation-specific).</li> </ol>"},{"location":"protocol/04-execution-lifecycle/#optional-dependency-failure","title":"Optional Dependency Failure","text":"<p>When an optional dependency fails:</p> <ol> <li>Allow Execution: Dependent task CAN execute.</li> <li>Status: Dependent task can transition to <code>in_progress</code> when other dependencies are satisfied.</li> <li>Error Information: Dependent task MAY access dependency error information (implementation-specific).</li> </ol>"},{"location":"protocol/04-execution-lifecycle/#retry-mechanisms","title":"Retry Mechanisms","text":"<p>MAY: Implementations MAY support automatic retry of failed tasks.</p> <p>SHOULD: If retry is supported, implementations SHOULD: - Limit the number of retries (e.g., max 3 attempts). - Use exponential backoff between retries. - Provide configuration for retry behavior.</p> <p>Example Retry Configuration: <pre><code>{\n  \"retry\": {\n    \"max_attempts\": 3,\n    \"backoff\": \"exponential\",\n    \"initial_delay\": 1.0,\n    \"max_delay\": 60.0\n  }\n}\n</code></pre></p>"},{"location":"protocol/04-execution-lifecycle/#error-recovery-procedures","title":"Error Recovery Procedures","text":"<p>MAY: Implementations MAY provide error recovery mechanisms:</p> <ol> <li>Automatic Recovery: Retry failed tasks automatically.</li> <li>Manual Recovery: Allow users to manually trigger re-execution.</li> <li>Partial Recovery: Resume execution from a checkpoint (if supported by executor).</li> </ol>"},{"location":"protocol/04-execution-lifecycle/#implementation-requirements","title":"Implementation Requirements","text":""},{"location":"protocol/04-execution-lifecycle/#state-persistence","title":"State Persistence","text":"<p>MUST: Implementations MUST persist task state to durable storage.</p> <p>SHOULD: State persistence SHOULD be atomic (all-or-nothing).</p> <p>MAY: Implementations MAY use transactions for state updates.</p>"},{"location":"protocol/04-execution-lifecycle/#state-consistency","title":"State Consistency","text":"<p>MUST: Implementations MUST ensure state consistency: - Status transitions follow the state machine rules. - Field values are consistent with status (e.g., <code>result</code> is null when status is not <code>completed</code>). - Timestamps are consistent (e.g., <code>started_at</code> &lt;= <code>completed_at</code>).</p>"},{"location":"protocol/04-execution-lifecycle/#concurrency-control","title":"Concurrency Control","text":"<p>MUST: Implementations MUST handle concurrent state updates correctly: - Prevent race conditions when updating task status. - Ensure only one execution of a task at a time. - Handle concurrent cancellation requests.</p> <p>SHOULD: Implementations SHOULD use appropriate concurrency primitives (locks, transactions, etc.).</p>"},{"location":"protocol/04-execution-lifecycle/#see-also","title":"See Also","text":"<ul> <li>Data Model - Task schema and field definitions</li> <li>Core Concepts - Fundamental protocol concepts</li> <li>Validation - Validation rules and algorithms</li> <li>Error Handling - Error codes and handling</li> </ul>"},{"location":"protocol/05-examples/","title":"Complete Task Example","text":"<p>This example shows a fully populated Task object with all standard fields. Use this as a reference for constructing valid tasks.</p> <pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"parent_id\": null,\n  \"user_id\": \"user-123\",\n  \"name\": \"system resources\",\n  \"status\": \"pending\",\n  \"priority\": 2,\n  \"inputs\": {\n    \"resource\": \"cpu\"\n  },\n  \"schemas\": {\n    \"method\": \"system_info_executor\",\n    \"input_schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"resource\": {\n          \"type\": \"string\",\n          \"enum\": [\"cpu\", \"memory\", \"disk\", \"all\"]\n        }\n      }\n    }\n  },\n  \"params\": null,\n  \"result\": null,\n  \"error\": null,\n  \"dependencies\": [],\n  \"progress\": 0.0,\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"started_at\": null,\n  \"updated_at\": \"2025-01-15T10:30:00Z\",\n  \"completed_at\": null,\n  \"origin_type\": \"create\",\n  \"original_task_id\": null,\n  \"has_references\": false\n}\n</code></pre>"},{"location":"protocol/05-examples/#example-10-linked-and-snapshotted-tasks","title":"Example 10: Linked and Snapshotted Tasks","text":"<p>Demonstrates a task created by linking to a completed source task.</p>"},{"location":"protocol/05-examples/#source-task-completed","title":"Source Task (Completed)","text":"<pre><code>{\n  \"id\": \"source-task-uuid\",\n  \"name\": \"Generate Report\",\n  \"status\": \"completed\",\n  \"result\": {\"report\": \"...\"},\n  \"created_at\": \"2025-01-15T09:00:00Z\"\n}\n</code></pre>"},{"location":"protocol/05-examples/#linked-task","title":"Linked Task","text":"<pre><code>{\n  \"id\": \"linked-task-uuid\",\n  \"name\": \"Generate Report\",\n  \"status\": \"pending\",\n  \"origin_type\": \"link\",\n  \"original_task_id\": \"source-task-uuid\",\n  \"created_at\": \"2025-01-15T12:00:00Z\"\n}\n</code></pre> <p>Note: For <code>origin_type: link</code> or <code>archive</code>, the <code>original_task_id</code> MUST reference a task in <code>completed</code> status (in principle).</p>"},{"location":"protocol/05-examples/#protocol-examples","title":"Protocol Examples","text":"<p>This section provides comprehensive examples demonstrating the AI Partner Up Flow Protocol. These examples are language-agnostic and can be implemented in any programming language.</p>"},{"location":"protocol/05-examples/#example-1-simple-task-execution","title":"Example 1: Simple Task Execution","text":"<p>The simplest possible task execution - a single task with no dependencies.</p>"},{"location":"protocol/05-examples/#task-definition","title":"Task Definition","text":"<pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"parent_id\": null,\n  \"user_id\": \"user-123\",\n  \"name\": \"system resources\",\n  \"status\": \"pending\",\n  \"priority\": 2,\n  \"inputs\": {\n    \"resource\": \"cpu\"\n  },\n  \"schemas\": {\n    \"method\": \"system_info_executor\",\n    \"input_schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"resource\": {\n          \"type\": \"string\",\n          \"enum\": [\"cpu\", \"memory\", \"disk\", \"all\"]\n        }\n      }\n    }\n  },\n  \"params\": null,\n  \"result\": null,\n  \"error\": null,\n  \"dependencies\": [],\n  \"progress\": 0.0,\n  \"created_at\": \"2025-01-15T10:30:00Z\",\n  \"started_at\": null,\n  \"updated_at\": \"2025-01-15T10:30:00Z\",\n  \"completed_at\": null,\n  \"origin_type\": \"create\",\n  \"original_task_id\": null,\n  \"has_references\": false\n}\n</code></pre>"},{"location":"protocol/05-examples/#execution-flow","title":"Execution Flow","text":"<ol> <li>Task is created with <code>status: \"pending\"</code></li> <li>TaskManager checks dependencies (none) \u2192 task is ready</li> <li>TaskManager checks priority (2 = normal) \u2192 schedules execution</li> <li>Executor is looked up by <code>schemas.method: \"system_info_executor\"</code></li> <li>Executor's <code>execute()</code> is called with <code>inputs: {\"resource\": \"cpu\"}</code></li> <li>Task status transitions: <code>pending</code> \u2192 <code>in_progress</code></li> <li>Executor returns result</li> <li>Task status transitions: <code>in_progress</code> \u2192 <code>completed</code></li> <li>Result is stored in <code>task.result</code></li> </ol>"},{"location":"protocol/05-examples/#expected-result","title":"Expected Result","text":"<pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"status\": \"completed\",\n  \"result\": {\n    \"cpu\": {\n      \"cores\": 8,\n      \"usage\": 45.2\n    }\n  },\n  \"progress\": 1.0,\n  \"started_at\": \"2025-01-15T10:30:00Z\",\n  \"completed_at\": \"2025-01-15T10:30:01Z\"\n}\n</code></pre>"},{"location":"protocol/05-examples/#example-2-sequential-tasks-dependencies","title":"Example 2: Sequential Tasks (Dependencies)","text":"<p>Two tasks where the second depends on the first.</p>"},{"location":"protocol/05-examples/#task-definitions","title":"Task Definitions","text":"<pre><code>{\n  \"tasks\": [\n    {\n      \"id\": \"task-1\",\n      \"name\": \"fetch_data\",\n      \"status\": \"pending\",\n      \"priority\": 1,\n      \"inputs\": {\n        \"url\": \"https://api.example.com/data\"\n      },\n      \"schemas\": {\n        \"method\": \"http_fetcher\"\n      },\n      \"dependencies\": []\n    },\n    {\n      \"id\": \"task-2\",\n      \"name\": \"process_data\",\n      \"status\": \"pending\",\n      \"priority\": 2,\n      \"inputs\": {\n        \"operation\": \"analyze\"\n      },\n      \"schemas\": {\n        \"method\": \"data_processor\"\n      },\n      \"dependencies\": [\n        {\n          \"id\": \"task-1\",\n          \"required\": true\n        }\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"protocol/05-examples/#execution-flow_1","title":"Execution Flow","text":"<ol> <li>Both tasks are created with <code>status: \"pending\"</code></li> <li>Task 1 has no dependencies \u2192 ready to execute</li> <li>Task 1 executes: <code>pending</code> \u2192 <code>in_progress</code> \u2192 <code>completed</code></li> <li>Task 2 checks dependencies:</li> <li>Dependency <code>task-1</code> has <code>status: \"completed\"</code> \u2192 satisfied</li> <li>Task 2 is ready to execute</li> <li>Task 2 executes: <code>pending</code> \u2192 <code>in_progress</code> \u2192 <code>completed</code></li> </ol>"},{"location":"protocol/05-examples/#expected-results","title":"Expected Results","text":"<p>Task 1: <pre><code>{\n  \"id\": \"task-1\",\n  \"status\": \"completed\",\n  \"result\": {\n    \"data\": [1, 2, 3, 4, 5]\n  }\n}\n</code></pre></p> <p>Task 2: <pre><code>{\n  \"id\": \"task-2\",\n  \"status\": \"completed\",\n  \"result\": {\n    \"analysis\": {\n      \"count\": 5,\n      \"sum\": 15,\n      \"average\": 3.0\n    }\n  }\n}\n</code></pre></p>"},{"location":"protocol/05-examples/#example-3-parallel-tasks","title":"Example 3: Parallel Tasks","text":"<p>Multiple tasks with no dependencies between them, executing in parallel.</p>"},{"location":"protocol/05-examples/#task-definitions_1","title":"Task Definitions","text":"<pre><code>{\n  \"tasks\": [\n    {\n      \"id\": \"root-task\",\n      \"name\": \"root\",\n      \"status\": \"pending\",\n      \"parent_id\": null\n    },\n    {\n      \"id\": \"task-1\",\n      \"name\": \"fetch_user_data\",\n      \"status\": \"pending\",\n      \"parent_id\": \"root-task\",\n      \"priority\": 2,\n      \"inputs\": {\"user_id\": \"user-123\"}\n    },\n    {\n      \"id\": \"task-2\",\n      \"name\": \"fetch_product_data\",\n      \"status\": \"pending\",\n      \"parent_id\": \"root-task\",\n      \"priority\": 2,\n      \"inputs\": {\"product_id\": \"prod-456\"}\n    },\n    {\n      \"id\": \"task-3\",\n      \"name\": \"fetch_order_data\",\n      \"status\": \"pending\",\n      \"parent_id\": \"root-task\",\n      \"priority\": 2,\n      \"inputs\": {\"order_id\": \"order-789\"}\n    }\n  ]\n}\n</code></pre>"},{"location":"protocol/05-examples/#execution-flow_2","title":"Execution Flow","text":"<ol> <li>All tasks are created with <code>status: \"pending\"</code></li> <li>All tasks have no dependencies \u2192 all ready to execute</li> <li>All tasks have same priority (2) \u2192 can execute in parallel</li> <li>TaskManager executes all three tasks concurrently</li> <li>All tasks complete independently</li> </ol>"},{"location":"protocol/05-examples/#expected-results_1","title":"Expected Results","text":"<p>All three tasks complete in parallel, with results stored independently.</p>"},{"location":"protocol/05-examples/#example-4-complex-task-tree-with-dependencies","title":"Example 4: Complex Task Tree with Dependencies","text":"<p>A complex workflow with multiple levels of dependencies.</p>"},{"location":"protocol/05-examples/#task-tree-structure","title":"Task Tree Structure","text":"<pre><code>Root Task\n\u2502\n\u251c\u2500\u2500 Task A (no dependencies)\n\u2502\n\u251c\u2500\u2500 Task B (depends on A)\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 Task D (depends on B)\n\u2502\n\u2514\u2500\u2500 Task C (depends on A)\n    \u2502\n    \u2514\u2500\u2500 Task E (depends on C and D)\n</code></pre>"},{"location":"protocol/05-examples/#task-definitions_2","title":"Task Definitions","text":"<pre><code>{\n  \"tasks\": [\n    {\n      \"id\": \"root\",\n      \"name\": \"root\",\n      \"status\": \"pending\",\n      \"parent_id\": null\n    },\n    {\n      \"id\": \"task-a\",\n      \"name\": \"Task A\",\n      \"status\": \"pending\",\n      \"parent_id\": \"root\",\n      \"priority\": 1,\n      \"dependencies\": []\n    },\n    {\n      \"id\": \"task-b\",\n      \"name\": \"Task B\",\n      \"status\": \"pending\",\n      \"parent_id\": \"root\",\n      \"priority\": 2,\n      \"dependencies\": [\n        {\"id\": \"task-a\", \"required\": true}\n      ]\n    },\n    {\n      \"id\": \"task-c\",\n      \"name\": \"Task C\",\n      \"status\": \"pending\",\n      \"parent_id\": \"root\",\n      \"priority\": 2,\n      \"dependencies\": [\n        {\"id\": \"task-a\", \"required\": true}\n      ]\n    },\n    {\n      \"id\": \"task-d\",\n      \"name\": \"Task D\",\n      \"status\": \"pending\",\n      \"parent_id\": \"root\",\n      \"priority\": 3,\n      \"dependencies\": [\n        {\"id\": \"task-b\", \"required\": true}\n      ]\n    },\n    {\n      \"id\": \"task-e\",\n      \"name\": \"Task E\",\n      \"status\": \"pending\",\n      \"parent_id\": \"root\",\n      \"priority\": 4,\n      \"dependencies\": [\n        {\"id\": \"task-c\", \"required\": true},\n        {\"id\": \"task-d\", \"required\": true}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"protocol/05-examples/#execution-flow_3","title":"Execution Flow","text":"<ol> <li>Phase 1: Task A executes (no dependencies)</li> <li>Phase 2: Tasks B and C execute in parallel (both depend on A)</li> <li>Phase 3: Task D executes (depends on B)</li> <li>Phase 4: Task E executes (depends on C and D)</li> </ol>"},{"location":"protocol/05-examples/#execution-timeline","title":"Execution Timeline","text":"<pre><code>Time 0:  Task A starts\nTime 1:  Task A completes\nTime 1:  Tasks B and C start (parallel)\nTime 2:  Tasks B and C complete\nTime 2:  Task D starts\nTime 3:  Task D completes\nTime 3:  Task E starts\nTime 4:  Task E completes\n</code></pre>"},{"location":"protocol/05-examples/#example-5-error-handling","title":"Example 5: Error Handling","text":"<p>Demonstrates error handling when a task fails.</p>"},{"location":"protocol/05-examples/#task-definitions_3","title":"Task Definitions","text":"<pre><code>{\n  \"tasks\": [\n    {\n      \"id\": \"task-1\",\n      \"name\": \"fetch_data\",\n      \"status\": \"pending\",\n      \"inputs\": {\"url\": \"https://invalid-url.example.com\"}\n    },\n    {\n      \"id\": \"task-2\",\n      \"name\": \"process_data\",\n      \"status\": \"pending\",\n      \"dependencies\": [\n        {\"id\": \"task-1\", \"required\": true}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"protocol/05-examples/#execution-flow_4","title":"Execution Flow","text":"<ol> <li>Task 1 executes and fails (invalid URL)</li> <li>Task 1 status: <code>pending</code> \u2192 <code>in_progress</code> \u2192 <code>failed</code></li> <li>Task 2 checks dependencies:</li> <li>Dependency <code>task-1</code> has <code>status: \"failed\"</code> and <code>required: true</code></li> <li>Task 2 cannot execute</li> <li>Task 2 remains in <code>pending</code> status</li> </ol>"},{"location":"protocol/05-examples/#expected-results_2","title":"Expected Results","text":"<p>Task 1 (Failed): <pre><code>{\n  \"id\": \"task-1\",\n  \"status\": \"failed\",\n  \"error\": \"Connection failed: Unable to resolve host 'invalid-url.example.com'\",\n  \"completed_at\": \"2025-01-15T10:30:05Z\"\n}\n</code></pre></p> <p>Task 2 (Blocked): <pre><code>{\n  \"id\": \"task-2\",\n  \"status\": \"pending\",\n  \"dependencies\": [\n    {\"id\": \"task-1\", \"required\": true}\n  ]\n}\n</code></pre></p>"},{"location":"protocol/05-examples/#example-6-optional-dependencies","title":"Example 6: Optional Dependencies","text":"<p>Demonstrates optional dependencies where a task can execute even if dependency fails.</p>"},{"location":"protocol/05-examples/#task-definitions_4","title":"Task Definitions","text":"<pre><code>{\n  \"tasks\": [\n    {\n      \"id\": \"primary\",\n      \"name\": \"primary_source\",\n      \"status\": \"pending\"\n    },\n    {\n      \"id\": \"fallback\",\n      \"name\": \"fallback_source\",\n      \"status\": \"pending\"\n    },\n    {\n      \"id\": \"aggregate\",\n      \"name\": \"aggregate_data\",\n      \"status\": \"pending\",\n      \"dependencies\": [\n        {\"id\": \"primary\", \"required\": false},\n        {\"id\": \"fallback\", \"required\": false}\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"protocol/05-examples/#execution-flow_5","title":"Execution Flow","text":"<ol> <li>Primary task fails</li> <li>Fallback task completes successfully</li> <li>Aggregate task checks dependencies:</li> <li>Primary: <code>failed</code>, <code>required: false</code> \u2192 can proceed</li> <li>Fallback: <code>completed</code>, <code>required: false</code> \u2192 can proceed</li> <li>Aggregate task executes (uses fallback data)</li> </ol>"},{"location":"protocol/05-examples/#example-7-priority-scheduling","title":"Example 7: Priority Scheduling","text":"<p>Demonstrates priority-based scheduling.</p>"},{"location":"protocol/05-examples/#task-definitions_5","title":"Task Definitions","text":"<pre><code>{\n  \"tasks\": [\n    {\n      \"id\": \"urgent-task\",\n      \"name\": \"Urgent Task\",\n      \"status\": \"pending\",\n      \"priority\": 0\n    },\n    {\n      \"id\": \"normal-task\",\n      \"name\": \"Normal Task\",\n      \"status\": \"pending\",\n      \"priority\": 2\n    },\n    {\n      \"id\": \"low-task\",\n      \"name\": \"Low Priority Task\",\n      \"status\": \"pending\",\n      \"priority\": 3\n    }\n  ]\n}\n</code></pre>"},{"location":"protocol/05-examples/#execution-flow_6","title":"Execution Flow","text":"<p>All tasks are ready (no dependencies). Execution order: 1. Urgent task (priority 0) 2. Normal task (priority 2) 3. Low priority task (priority 3)</p>"},{"location":"protocol/05-examples/#example-8-re-execution","title":"Example 8: Re-execution","text":"<p>Demonstrates re-executing a failed task.</p>"},{"location":"protocol/05-examples/#initial-state","title":"Initial State","text":"<pre><code>{\n  \"id\": \"task-1\",\n  \"status\": \"failed\",\n  \"error\": \"Temporary network error\",\n  \"result\": null,\n  \"progress\": 0.0\n}\n</code></pre>"},{"location":"protocol/05-examples/#re-execution-request","title":"Re-execution Request","text":"<p>Reset task to <code>pending</code> status:</p> <pre><code>{\n  \"id\": \"task-1\",\n  \"status\": \"pending\",\n  \"error\": null,\n  \"result\": null,\n  \"progress\": 0.0,\n  \"started_at\": null,\n  \"completed_at\": null\n}\n</code></pre>"},{"location":"protocol/05-examples/#execution-flow_7","title":"Execution Flow","text":"<ol> <li>Task status reset: <code>failed</code> \u2192 <code>pending</code></li> <li>Task executes again: <code>pending</code> \u2192 <code>in_progress</code> \u2192 <code>completed</code></li> <li>Task completes successfully</li> </ol>"},{"location":"protocol/05-examples/#example-9-copy-execution","title":"Example 9: Copy Execution","text":"<p>Demonstrates copying a task before execution.</p>"},{"location":"protocol/05-examples/#original-task","title":"Original Task","text":"<pre><code>{\n  \"id\": \"original-task\",\n  \"name\": \"My Task\",\n  \"status\": \"completed\",\n  \"result\": {\"output\": \"result\"},\n  \"created_at\": \"2025-01-15T10:00:00Z\"\n}\n</code></pre>"},{"location":"protocol/05-examples/#copied-task","title":"Copied Task","text":"<pre><code>{\n  \"id\": \"copied-task\",\n  \"name\": \"My Task\",\n  \"status\": \"pending\",\n  \"result\": null,\n  \"created_at\": \"2025-01-15T11:00:00Z\"\n}\n</code></pre>"},{"location":"protocol/05-examples/#key-differences","title":"Key Differences","text":"<ul> <li>New <code>id</code> (UUID)</li> <li><code>status</code> reset to <code>pending</code></li> <li><code>result</code> reset to <code>null</code></li> <li>New <code>created_at</code> timestamp</li> <li>Original task unchanged</li> </ul>"},{"location":"protocol/05-examples/#executor-type-examples","title":"Executor Type Examples","text":"<p>The protocol supports various executor types. Here are examples of common executor patterns:</p>"},{"location":"protocol/05-examples/#system-information-executor","title":"System Information Executor","text":"<pre><code>{\n  \"id\": \"task-1\",\n  \"name\": \"system_info_executor\",\n  \"schemas\": {\n    \"method\": \"system_info_executor\"\n  },\n  \"inputs\": {\n    \"resource\": \"cpu\"\n  }\n}\n</code></pre> <p>Purpose: Query system resources (CPU, memory, disk).</p>"},{"location":"protocol/05-examples/#command-executor","title":"Command Executor","text":"<pre><code>{\n  \"id\": \"task-2\",\n  \"name\": \"command_executor\",\n  \"schemas\": {\n    \"method\": \"command_executor\"\n  },\n  \"inputs\": {\n    \"command\": \"ls -la\",\n    \"cwd\": \"/tmp\"\n  }\n}\n</code></pre> <p>Purpose: Execute shell commands or system operations.</p>"},{"location":"protocol/05-examples/#data-aggregation-executor","title":"Data Aggregation Executor","text":"<pre><code>{\n  \"id\": \"task-3\",\n  \"name\": \"aggregate_results_executor\",\n  \"schemas\": {\n    \"method\": \"aggregate_results_executor\"\n  },\n  \"inputs\": {},\n  \"dependencies\": [\n    {\"id\": \"task-1\", \"required\": true},\n    {\"id\": \"task-2\", \"required\": true}\n  ]\n}\n</code></pre> <p>Purpose: Combine results from multiple dependent tasks.</p>"},{"location":"protocol/05-examples/#llm-executor-crewai","title":"LLM Executor (CrewAI)","text":"<pre><code>{\n  \"id\": \"task-4\",\n  \"name\": \"crew_manager\",\n  \"schemas\": {\n    \"method\": \"crew_manager\"\n  },\n  \"inputs\": {\n    \"text\": \"Analyze this data...\"\n  },\n  \"params\": {\n    \"agents\": [...],\n    \"tasks\": [...]\n  }\n}\n</code></pre> <p>Purpose: Execute AI/LLM-powered tasks (requires AI dependencies).</p> <p>Note: LLM executors typically require additional configuration in <code>params</code> (agents, tasks, etc.).</p>"},{"location":"protocol/05-examples/#httpapi-executor","title":"HTTP/API Executor","text":"<pre><code>{\n  \"id\": \"task-5\",\n  \"name\": \"http_request_executor\",\n  \"schemas\": {\n    \"method\": \"http_request_executor\"\n  },\n  \"inputs\": {\n    \"url\": \"https://api.example.com/data\",\n    \"method\": \"GET\",\n    \"headers\": {\n      \"Authorization\": \"Bearer token\"\n    }\n  }\n}\n</code></pre> <p>Purpose: Make HTTP requests to external services.</p>"},{"location":"protocol/05-examples/#custom-executor","title":"Custom Executor","text":"<pre><code>{\n  \"id\": \"task-6\",\n  \"name\": \"my_custom_executor\",\n  \"schemas\": {\n    \"method\": \"my_custom_executor\"\n  },\n  \"inputs\": {\n    \"custom_param\": \"value\"\n  }\n}\n</code></pre> <p>Purpose: User-defined executor for specific business logic.</p> <p>Note: Custom executors must be registered with ExecutorRegistry before use.</p>"},{"location":"protocol/05-examples/#multi-language-implementation-examples","title":"Multi-Language Implementation Examples","text":""},{"location":"protocol/05-examples/#python-reference-implementation","title":"Python Reference Implementation","text":"<pre><code># Python example (reference implementation)\nimport asyncio\nfrom uuid import uuid4\n\nasync def execute_task(task, executor_registry):\n    # Look up executor\n    executor = executor_registry.get(task.schemas.method)\n    if not executor:\n        raise ValueError(f\"Executor not found: {task.schemas.method}\")\n\n    # Execute\n    result = await executor.execute(task.inputs)\n\n    # Update task\n    task.status = \"completed\"\n    task.result = result\n    task.progress = 1.0\n\n    return task\n</code></pre>"},{"location":"protocol/05-examples/#go-implementation-example","title":"Go Implementation Example","text":"<pre><code>// Go example (conceptual)\npackage main\n\ntype Task struct {\n    ID     string                 `json:\"id\"`\n    Name   string                 `json:\"name\"`\n    Status string                 `json:\"status\"`\n    Inputs map[string]interface{} `json:\"inputs\"`\n    Result map[string]interface{} `json:\"result\"`\n}\n\nfunc ExecuteTask(task Task, registry ExecutorRegistry) (Task, error) {\n    executor, err := registry.Get(task.Schemas.Method)\n    if err != nil {\n        return task, err\n    }\n\n    result, err := executor.Execute(task.Inputs)\n    if err != nil {\n        task.Status = \"failed\"\n        task.Error = err.Error()\n        return task, err\n    }\n\n    task.Status = \"completed\"\n    task.Result = result\n    task.Progress = 1.0\n\n    return task, nil\n}\n</code></pre>"},{"location":"protocol/05-examples/#rust-implementation-example","title":"Rust Implementation Example","text":"<pre><code>// Rust example (conceptual)\nuse serde::{Deserialize, Serialize};\nuse uuid::Uuid;\n\n#[derive(Debug, Serialize, Deserialize)]\nstruct Task {\n    id: Uuid,\n    name: String,\n    status: String,\n    inputs: serde_json::Value,\n    result: Option&lt;serde_json::Value&gt;,\n}\n\nasync fn execute_task(\n    task: Task,\n    registry: &amp;ExecutorRegistry,\n) -&gt; Result&lt;Task, Box&lt;dyn std::error::Error&gt;&gt; {\n    let executor = registry.get(&amp;task.schemas.method)?;\n    let result = executor.execute(task.inputs).await?;\n\n    Ok(Task {\n        status: \"completed\".to_string(),\n        result: Some(result),\n        ..task\n    })\n}\n</code></pre>"},{"location":"protocol/05-examples/#javascripttypescript-implementation-example","title":"JavaScript/TypeScript Implementation Example","text":"<pre><code>// TypeScript example (conceptual)\ninterface Task {\n  id: string;\n  name: string;\n  status: string;\n  inputs: Record&lt;string, any&gt;;\n  result?: Record&lt;string, any&gt;;\n}\n\nasync function executeTask(\n  task: Task,\n  registry: ExecutorRegistry\n): Promise&lt;Task&gt; {\n  const executor = registry.get(task.schemas.method);\n  if (!executor) {\n    throw new Error(`Executor not found: ${task.schemas.method}`);\n  }\n\n  const result = await executor.execute(task.inputs);\n\n  return {\n    ...task,\n    status: \"completed\",\n    result,\n    progress: 1.0,\n  };\n}\n</code></pre>"},{"location":"protocol/05-examples/#protocol-compliance-examples","title":"Protocol Compliance Examples","text":""},{"location":"protocol/05-examples/#valid-task-definition","title":"Valid Task Definition","text":"<pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"name\": \"valid_task\",\n  \"status\": \"pending\",\n  \"priority\": 2,\n  \"inputs\": {\"key\": \"value\"},\n  \"schemas\": {\n    \"method\": \"executor_id\"\n  },\n  \"dependencies\": []\n}\n</code></pre> <p>\u2705 Valid: All required fields present, valid types, no circular dependencies.</p>"},{"location":"protocol/05-examples/#invalid-task-definitions","title":"Invalid Task Definitions","text":""},{"location":"protocol/05-examples/#missing-required-field","title":"Missing Required Field","text":"<pre><code>{\n  \"name\": \"invalid_task\"\n  // Missing: id, status\n}\n</code></pre> <p>\u274c Invalid: Missing required fields <code>id</code> and <code>status</code>.</p>"},{"location":"protocol/05-examples/#invalid-uuid-format","title":"Invalid UUID Format","text":"<pre><code>{\n  \"id\": \"not-a-uuid\",\n  \"name\": \"invalid_task\",\n  \"status\": \"pending\"\n}\n</code></pre> <p>\u274c Invalid: <code>id</code> must be valid UUID v4 format.</p>"},{"location":"protocol/05-examples/#invalid-status-value","title":"Invalid Status Value","text":"<pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"name\": \"invalid_task\",\n  \"status\": \"invalid_status\"\n}\n</code></pre> <p>\u274c Invalid: <code>status</code> must be one of: <code>pending</code>, <code>in_progress</code>, <code>completed</code>, <code>failed</code>, <code>cancelled</code>.</p>"},{"location":"protocol/05-examples/#priority-out-of-range","title":"Priority Out of Range","text":"<pre><code>{\n  \"id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"name\": \"invalid_task\",\n  \"status\": \"pending\",\n  \"priority\": 10\n}\n</code></pre> <p>\u274c Invalid: <code>priority</code> must be in range 0-3.</p>"},{"location":"protocol/05-examples/#circular-dependency","title":"Circular Dependency","text":"<pre><code>{\n  \"tasks\": [\n    {\n      \"id\": \"task-a\",\n      \"dependencies\": [{\"id\": \"task-b\", \"required\": true}]\n    },\n    {\n      \"id\": \"task-b\",\n      \"dependencies\": [{\"id\": \"task-a\", \"required\": true}]\n    }\n  ]\n}\n</code></pre> <p>\u274c Invalid: Circular dependency detected (A depends on B, B depends on A).</p>"},{"location":"protocol/05-examples/#edge-cases","title":"Edge Cases","text":""},{"location":"protocol/05-examples/#empty-task-tree","title":"Empty Task Tree","text":"<pre><code>{\n  \"task\": {\n    \"id\": \"root\",\n    \"name\": \"root\",\n    \"status\": \"pending\"\n  },\n  \"children\": []\n}\n</code></pre> <p>\u2705 Valid: Single root task with no children.</p>"},{"location":"protocol/05-examples/#task-with-many-dependencies","title":"Task with Many Dependencies","text":"<pre><code>{\n  \"id\": \"task-1\",\n  \"dependencies\": [\n    {\"id\": \"dep-1\", \"required\": true},\n    {\"id\": \"dep-2\", \"required\": true},\n    {\"id\": \"dep-3\", \"required\": true},\n    {\"id\": \"dep-4\", \"required\": false},\n    {\"id\": \"dep-5\", \"required\": false}\n  ]\n}\n</code></pre> <p>\u2705 Valid: Task can have multiple dependencies (both required and optional).</p>"},{"location":"protocol/05-examples/#task-with-deep-nesting","title":"Task with Deep Nesting","text":"<pre><code>Root\n\u2514\u2500\u2500 Level 1\n    \u2514\u2500\u2500 Level 2\n        \u2514\u2500\u2500 Level 3\n            \u2514\u2500\u2500 Level 4\n</code></pre> <p>\u2705 Valid: Tasks can be nested to any depth (implementation may have limits).</p>"},{"location":"protocol/05-examples/#see-also","title":"See Also","text":"<ul> <li>Data Model - Complete task schema</li> <li>Execution Lifecycle - State machine and execution rules</li> <li>Core Concepts - Fundamental concepts</li> <li>Validation - Validation rules</li> </ul>"},{"location":"protocol/06-interfaces/","title":"Interface Protocol","text":"<p>The Interface Protocol defines how external clients (CLIs, Dashboards, other Agents) interact with an AI Partner Up Flow node. This specification is mandatory for all implementations.</p>"},{"location":"protocol/06-interfaces/#protocol-standards","title":"Protocol Standards","text":"<p>The protocol implements two complementary standards:</p> <ol> <li>JSON-RPC 2.0: Standard RPC protocol for request-response communication</li> <li>A2A Protocol: Agent-to-Agent communication protocol for AI agent systems</li> </ol> <p>MUST: All implementations MUST support JSON-RPC 2.0 over HTTP. SHOULD: Implementations SHOULD support A2A Protocol for enhanced agent-to-agent communication. MAY: Implementations MAY support additional transport layers (SSE, WebSocket) for streaming.</p>"},{"location":"protocol/06-interfaces/#transport-layer","title":"Transport Layer","text":""},{"location":"protocol/06-interfaces/#http-transport-json-rpc-20","title":"HTTP Transport (JSON-RPC 2.0)","text":"<p>MUST: Implementations MUST support HTTP/1.1 or HTTP/2.</p> <p>Request: - Method: <code>POST</code> - Endpoint: <code>/</code> (root endpoint) or <code>/tasks</code> (legacy) - Content-Type: <code>application/json</code> - Body: JSON-RPC 2.0 request object</p> <p>Response: - Content-Type: <code>application/json</code> - Body: JSON-RPC 2.0 response object</p>"},{"location":"protocol/06-interfaces/#a2a-protocol-transport","title":"A2A Protocol Transport","text":"<p>SHOULD: Implementations SHOULD support A2A Protocol over: - HTTP (request-response) - Server-Sent Events (SSE) for streaming - WebSocket for bidirectional communication</p> <p>Endpoints: - <code>GET /.well-known/agent-card</code>: Agent capability discovery - <code>POST /</code>: A2A Protocol RPC endpoint</p>"},{"location":"protocol/06-interfaces/#json-rpc-20-compliance","title":"JSON-RPC 2.0 Compliance","text":"<p>The protocol uses JSON-RPC 2.0 for all RPC operations. Implementations MUST comply with the JSON-RPC 2.0 Specification.</p>"},{"location":"protocol/06-interfaces/#request-format","title":"Request Format","text":"<pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"method_name\",\n  \"params\": {},\n  \"id\": \"request-id\"\n}\n</code></pre> <p>Fields: - <code>jsonrpc</code> (string, required): MUST be <code>\"2.0\"</code> - <code>method</code> (string, required): Method name (see Standard Methods) - <code>params</code> (object, required): Method parameters (varies by method) - <code>id</code> (string/number, required): Request identifier for matching responses</p>"},{"location":"protocol/06-interfaces/#response-format-success","title":"Response Format (Success)","text":"<pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {},\n  \"id\": \"request-id\"\n}\n</code></pre> <p>Fields: - <code>jsonrpc</code> (string, required): MUST be <code>\"2.0\"</code> - <code>result</code> (any, required): Method result (varies by method) - <code>id</code> (string/number, required): Request identifier (matches request)</p>"},{"location":"protocol/06-interfaces/#response-format-error","title":"Response Format (Error)","text":"<pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32600,\n    \"message\": \"Invalid Request\",\n    \"data\": \"Error details\"\n  },\n  \"id\": \"request-id\"\n}\n</code></pre> <p>Fields: - <code>jsonrpc</code> (string, required): MUST be <code>\"2.0\"</code> - <code>error</code> (object, required): Error object   - <code>code</code> (integer, required): Error code (see Error Codes)   - <code>message</code> (string, required): Error message   - <code>data</code> (any, optional): Additional error data - <code>id</code> (string/number, required): Request identifier (matches request, or <code>null</code> for parse errors)</p>"},{"location":"protocol/06-interfaces/#json-rpc-20-error-codes","title":"JSON-RPC 2.0 Error Codes","text":"Code Name Description -32700 Parse error Invalid JSON was received -32600 Invalid Request The JSON sent is not a valid Request object -32601 Method not found The method does not exist / is not available -32602 Invalid params Invalid method parameter(s) -32603 Internal error Internal JSON-RPC error -32000 to -32099 Server error Reserved for implementation-defined server errors <p>MUST: Implementations MUST use standard JSON-RPC 2.0 error codes for protocol-level errors.</p> <p>SHOULD: Implementations SHOULD use custom error codes (outside -32000 to -32099) for application-specific errors.</p>"},{"location":"protocol/06-interfaces/#standard-methods","title":"Standard Methods","text":"<p>A compliant node MUST support the following methods. All methods follow JSON-RPC 2.0 format.</p>"},{"location":"protocol/06-interfaces/#task-management-methods","title":"Task Management Methods","text":""},{"location":"protocol/06-interfaces/#taskscreate","title":"<code>tasks.create</code>","text":"<p>Create a new task or task tree.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.create\",\n  \"params\": {\n    \"name\": \"Task Name\",\n    \"user_id\": \"user123\",\n    \"parent_id\": null,\n    \"priority\": 2,\n    \"inputs\": {},\n    \"schemas\": {\n      \"method\": \"executor_id\"\n    },\n    \"dependencies\": []\n  },\n  \"id\": \"req-001\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"id\": \"task-uuid\",\n    \"status\": \"pending\"\n  },\n  \"id\": \"req-001\"\n}\n</code></pre></p> <p>Parameters: - <code>name</code> (string, required): Task name - <code>user_id</code> (string, optional): User identifier - <code>parent_id</code> (string, optional): Parent task ID (UUID) - <code>priority</code> (integer, optional): Priority (0-3, default: 2) - <code>inputs</code> (object, optional): Runtime inputs - <code>schemas</code> (object, optional): Executor configuration - <code>dependencies</code> (array, optional): Dependency list</p> <p>MUST: Validate task schema before creation. MUST: Generate unique UUID for task <code>id</code>. MUST: Set initial <code>status</code> to <code>\"pending\"</code>.</p>"},{"location":"protocol/06-interfaces/#tasksget","title":"<code>tasks.get</code>","text":"<p>Retrieve a task by ID.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.get\",\n  \"params\": {\n    \"task_id\": \"task-uuid\"\n  },\n  \"id\": \"req-002\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"id\": \"task-uuid\",\n    \"name\": \"Task Name\",\n    \"status\": \"completed\",\n    // ... complete task object\n  },\n  \"id\": \"req-002\"\n}\n</code></pre></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID (UUID)</p> <p>MUST: Return complete task object if task exists. MUST: Return error if task not found.</p>"},{"location":"protocol/06-interfaces/#tasksupdate","title":"<code>tasks.update</code>","text":"<p>Update task fields.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.update\",\n  \"params\": {\n    \"task_id\": \"task-uuid\",\n    \"updates\": {\n      \"name\": \"Updated Name\",\n      \"priority\": 1\n    }\n  },\n  \"id\": \"req-003\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"id\": \"task-uuid\",\n    \"status\": \"pending\"\n  },\n  \"id\": \"req-003\"\n}\n</code></pre></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID (UUID) - <code>updates</code> (object, required): Fields to update</p> <p>MUST: Validate updates against task schema. MUST NOT: Allow updates to critical fields (<code>id</code>, <code>parent_id</code>, <code>user_id</code>) when task is executing. SHOULD: Validate <code>dependencies</code> updates (no circular dependencies).</p>"},{"location":"protocol/06-interfaces/#tasksdelete","title":"<code>tasks.delete</code>","text":"<p>Delete a task.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.delete\",\n  \"params\": {\n    \"task_id\": \"task-uuid\"\n  },\n  \"id\": \"req-004\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"success\": true\n  },\n  \"id\": \"req-004\"\n}\n</code></pre></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID (UUID)</p> <p>MUST: Only delete tasks with status <code>pending</code>. MUST: Reject deletion if task has children or dependents. SHOULD: Cascade delete children if explicitly requested (implementation-specific).</p>"},{"location":"protocol/06-interfaces/#taskslist","title":"<code>tasks.list</code>","text":"<p>List tasks with filters.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.list\",\n  \"params\": {\n    \"limit\": 100,\n    \"offset\": 0,\n    \"status\": \"pending\",\n    \"user_id\": \"user123\"\n  },\n  \"id\": \"req-005\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"tasks\": [\n      {\n        \"id\": \"task-uuid\",\n        \"name\": \"Task Name\",\n        \"status\": \"pending\"\n      }\n    ],\n    \"total\": 50,\n    \"limit\": 100,\n    \"offset\": 0\n  },\n  \"id\": \"req-005\"\n}\n</code></pre></p> <p>Parameters: - <code>limit</code> (integer, optional): Maximum results (default: 100, max: 1000) - <code>offset</code> (integer, optional): Pagination offset (default: 0) - <code>status</code> (string, optional): Filter by status - <code>user_id</code> (string, optional): Filter by user ID</p> <p>MUST: Support pagination via <code>limit</code> and <code>offset</code>. SHOULD: Return total count for pagination.</p>"},{"location":"protocol/06-interfaces/#tasksexecute","title":"<code>tasks.execute</code>","text":"<p>Execute a task or task tree.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.execute\",\n  \"params\": {\n    \"tasks\": [\n      {\n        \"id\": \"task1\",\n        \"name\": \"Task 1\",\n        \"user_id\": \"user123\",\n        \"schemas\": {\n          \"method\": \"executor_id\"\n        },\n        \"inputs\": {}\n      }\n    ]\n  },\n  \"id\": \"req-006\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"root_task_id\": \"task-uuid\",\n    \"status\": \"started\"\n  },\n  \"id\": \"req-006\"\n}\n</code></pre></p> <p>Parameters: - <code>tasks</code> (array, required): Array of task objects to execute</p> <p>MUST: Validate task tree structure before execution. MUST: Execute tasks according to dependencies and priorities. SHOULD: Support streaming mode (see Streaming).</p>"},{"location":"protocol/06-interfaces/#taskscancel","title":"<code>tasks.cancel</code>","text":"<p>Cancel a running task.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.cancel\",\n  \"params\": {\n    \"task_id\": \"task-uuid\"\n  },\n  \"id\": \"req-007\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"task_id\": \"task-uuid\",\n    \"status\": \"cancelled\"\n  },\n  \"id\": \"req-007\"\n}\n</code></pre></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID (UUID)</p> <p>MUST: Cancel task if status is <code>pending</code> or <code>in_progress</code>. MUST: Transition task to <code>cancelled</code> status.</p>"},{"location":"protocol/06-interfaces/#task-query-methods","title":"Task Query Methods","text":""},{"location":"protocol/06-interfaces/#taskstree","title":"<code>tasks.tree</code>","text":"<p>Get the full task hierarchy.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.tree\",\n  \"params\": {\n    \"task_id\": \"root-task-uuid\"\n  },\n  \"id\": \"req-008\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"task\": {\n      \"id\": \"root-task-uuid\",\n      \"name\": \"Root Task\"\n    },\n    \"children\": [\n      {\n        \"task\": {\n          \"id\": \"child-task-uuid\",\n          \"name\": \"Child Task\"\n        },\n        \"children\": []\n      }\n    ]\n  },\n  \"id\": \"req-008\"\n}\n</code></pre></p> <p>Parameters: - <code>task_id</code> (string, required): Root task ID (UUID)</p> <p>MUST: Return complete task tree structure. MUST: Include all descendants recursively.</p>"},{"location":"protocol/06-interfaces/#taskschildren","title":"<code>tasks.children</code>","text":"<p>Get direct children of a task.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.children\",\n  \"params\": {\n    \"parent_id\": \"parent-task-uuid\"\n  },\n  \"id\": \"req-009\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"children\": [\n      {\n        \"id\": \"child-task-uuid\",\n        \"name\": \"Child Task\"\n      }\n    ]\n  },\n  \"id\": \"req-009\"\n}\n</code></pre></p> <p>Parameters: - <code>parent_id</code> (string, required): Parent task ID (UUID)</p> <p>MUST: Return only direct children (not grandchildren).</p>"},{"location":"protocol/06-interfaces/#additional-methods","title":"Additional Methods","text":""},{"location":"protocol/06-interfaces/#taskscopy","title":"<code>tasks.copy</code>","text":"<p>Copy a task tree for re-execution.</p> <p>Request: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.copy\",\n  \"params\": {\n    \"task_id\": \"task-uuid\",\n    \"copy_children\": true\n  },\n  \"id\": \"req-010\"\n}\n</code></pre></p> <p>Response (Success): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"original_task_id\": \"task-uuid\",\n    \"copied_task_id\": \"new-task-uuid\",\n    \"status\": \"pending\"\n  },\n  \"id\": \"req-010\"\n}\n</code></pre></p> <p>Parameters: - <code>task_id</code> (string, required): Task ID to copy (UUID) - <code>copy_children</code> (boolean, optional): Also copy children (default: <code>false</code>)</p> <p>MUST: Create new task with new UUID. MUST: Reset execution state (status, result, error, progress). SHOULD: Preserve task definition (name, inputs, schemas, dependencies).</p>"},{"location":"protocol/06-interfaces/#a2a-protocol-integration","title":"A2A Protocol Integration","text":"<p>The protocol implements the A2A (Agent-to-Agent) Protocol standard for enhanced agent-to-agent communication.</p>"},{"location":"protocol/06-interfaces/#a2a-protocol-overview","title":"A2A Protocol Overview","text":"<p>A2A Protocol provides: - Standardized agent communication - Streaming execution support - Push notifications - Agent capability discovery</p> <p>Reference: A2A Protocol Documentation</p>"},{"location":"protocol/06-interfaces/#agent-card-discovery","title":"Agent Card Discovery","text":"<p>Endpoint: <code>GET /.well-known/agent-card</code></p> <p>Response: <pre><code>{\n  \"name\": \"apflow\",\n  \"description\": \"Agent workflow orchestration and execution platform\",\n  \"url\": \"http://localhost:8000\",\n  \"version\": \"0.2.0\",\n  \"capabilities\": {\n    \"streaming\": true,\n    \"push_notifications\": true\n  },\n  \"skills\": [\n    {\n      \"id\": \"tasks.execute\",\n      \"name\": \"Execute Task Tree\",\n      \"description\": \"Execute a complete task tree with multiple tasks\",\n      \"tags\": [\"task\", \"orchestration\", \"workflow\", \"execution\"]\n    }\n  ]\n}\n</code></pre></p> <p>MUST: Implementations SHOULD support agent card discovery for A2A Protocol compatibility.</p>"},{"location":"protocol/06-interfaces/#a2a-protocol-task-mapping","title":"A2A Protocol Task Mapping","text":"<p>A2A Protocol uses a <code>Task</code> object that differs from apflow's Task. The mapping is as follows:</p> apflow Task A2A Protocol Task Notes <code>id</code> <code>context_id</code> Task definition ID <code>status</code> <code>status.state</code> Status mapping <code>result</code> <code>artifacts</code> Execution results <code>error</code> <code>status.message</code> Error messages <code>user_id</code> <code>metadata.user_id</code> User identifier - <code>id</code> A2A execution instance ID (auto-generated) - <code>history</code> LLM conversation history (execution-level) <p>MUST: Implementations MUST map between apflow Task and A2A Protocol Task when using A2A Protocol.</p>"},{"location":"protocol/06-interfaces/#a2a-protocol-methods","title":"A2A Protocol Methods","text":"<p>A2A Protocol supports the same methods as JSON-RPC 2.0, but with A2A-specific request/response formats.</p> <p>Request Format (A2A Protocol): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.execute\",\n  \"params\": {\n    \"tasks\": [...]\n  },\n  \"id\": \"req-001\",\n  \"metadata\": {\n    \"stream\": true\n  },\n  \"configuration\": {\n    \"push_notification_config\": {\n      \"url\": \"https://callback.url\",\n      \"headers\": {\n        \"Authorization\": \"Bearer token\"\n      }\n    }\n  }\n}\n</code></pre></p> <p>Response Format (A2A Protocol): <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"result\": {\n    \"id\": \"execution-instance-id\",\n    \"context_id\": \"task-definition-id\",\n    \"kind\": \"task\",\n    \"status\": {\n      \"state\": \"completed\",\n      \"message\": {\n        \"kind\": \"message\",\n        \"parts\": [\n          {\n            \"kind\": \"data\",\n            \"data\": {\n              \"protocol\": \"a2a\",\n              \"status\": \"completed\",\n              \"progress\": 1.0,\n              \"root_task_id\": \"task-uuid\"\n            }\n          }\n        ]\n      }\n    },\n    \"artifacts\": [...],\n    \"metadata\": {\n      \"protocol\": \"a2a\",\n      \"root_task_id\": \"task-uuid\"\n    }\n  },\n  \"id\": \"req-001\"\n}\n</code></pre></p> <p>MUST: A2A Protocol responses MUST include <code>protocol: \"a2a\"</code> in metadata and event data.</p>"},{"location":"protocol/06-interfaces/#streaming","title":"Streaming","text":"<p>The protocol supports real-time streaming of task execution updates.</p>"},{"location":"protocol/06-interfaces/#streaming-modes","title":"Streaming Modes","text":"<ol> <li>Server-Sent Events (SSE): One-way streaming from server to client</li> <li>WebSocket: Bidirectional streaming</li> <li>Push Notifications: HTTP callbacks to external URLs</li> </ol>"},{"location":"protocol/06-interfaces/#streaming-request","title":"Streaming Request","text":"<p>Enable streaming via <code>metadata.stream</code> or <code>use_streaming</code> parameter:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"method\": \"tasks.execute\",\n  \"params\": {\n    \"tasks\": [...],\n    \"use_streaming\": true\n  },\n  \"metadata\": {\n    \"stream\": true\n  },\n  \"id\": \"req-001\"\n}\n</code></pre>"},{"location":"protocol/06-interfaces/#streaming-events","title":"Streaming Events","text":"<p>Streaming events are sent via EventQueue (A2A Protocol) or SSE/WebSocket.</p> <p>Event Format: <pre><code>{\n  \"event\": \"task_status_update\",\n  \"data\": {\n    \"protocol\": \"a2a\",\n    \"task_id\": \"task-uuid\",\n    \"status\": \"in_progress\",\n    \"progress\": 0.5,\n    \"root_task_id\": \"root-task-uuid\"\n  }\n}\n</code></pre></p> <p>Event Types: - <code>task_status_update</code>: Task status changed - <code>task_progress_update</code>: Task progress updated - <code>task_completed</code>: Task completed - <code>task_failed</code>: Task failed - <code>task_cancelled</code>: Task cancelled</p> <p>MUST: Implementations MUST support at least one streaming mode. SHOULD: Implementations SHOULD support SSE for simple streaming scenarios.</p>"},{"location":"protocol/06-interfaces/#push-notifications","title":"Push Notifications","text":"<p>Push notifications send task updates to external callback URLs.</p> <p>Configuration: <pre><code>{\n  \"configuration\": {\n    \"push_notification_config\": {\n      \"url\": \"https://callback.url\",\n      \"headers\": {\n        \"Authorization\": \"Bearer token\"\n      },\n      \"method\": \"POST\"\n    }\n  }\n}\n</code></pre></p> <p>Callback Request: <pre><code>{\n  \"task_id\": \"task-uuid\",\n  \"status\": \"completed\",\n  \"progress\": 1.0,\n  \"result\": {...}\n}\n</code></pre></p> <p>MUST: Implementations MUST send HTTP POST requests to callback URL. SHOULD: Implementations SHOULD include authentication headers if provided. MAY: Implementations MAY retry failed callbacks (implementation-specific).</p>"},{"location":"protocol/06-interfaces/#authentication","title":"Authentication","text":"<p>MAY: Implementations MAY support authentication (JWT, API keys, etc.).</p> <p>SHOULD: If authentication is supported, implementations SHOULD: - Validate authentication tokens - Enforce access control based on <code>user_id</code> - Return appropriate errors for unauthorized requests</p> <p>Authentication Header (if supported): <pre><code>Authorization: Bearer &lt;token&gt;\n</code></pre></p>"},{"location":"protocol/06-interfaces/#error-handling","title":"Error Handling","text":""},{"location":"protocol/06-interfaces/#error-response-format","title":"Error Response Format","text":"<p>All errors follow JSON-RPC 2.0 error format:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Invalid params\",\n    \"data\": {\n      \"field\": \"task_id\",\n      \"reason\": \"Invalid UUID format\"\n    }\n  },\n  \"id\": \"req-001\"\n}\n</code></pre>"},{"location":"protocol/06-interfaces/#common-error-scenarios","title":"Common Error Scenarios","text":"<ol> <li>Task Not Found: Return error code -32001 with message \"Task not found\"</li> <li>Invalid Task Schema: Return error code -32602 with validation details</li> <li>Circular Dependency: Return error code -32002 with message \"Circular dependency detected\"</li> <li>Executor Not Found: Return error code -32003 with message \"Executor not found\"</li> <li>Unauthorized: Return error code -32004 with message \"Unauthorized\" (if auth enabled)</li> </ol>"},{"location":"protocol/06-interfaces/#implementation-requirements","title":"Implementation Requirements","text":""},{"location":"protocol/06-interfaces/#method-support","title":"Method Support","text":"<p>MUST: All implementations MUST support all methods listed in Standard Methods.</p> <p>SHOULD: Implementations SHOULD support A2A Protocol for enhanced features.</p> <p>MAY: Implementations MAY support additional custom methods.</p>"},{"location":"protocol/06-interfaces/#validation","title":"Validation","text":"<p>MUST: Implementations MUST validate all requests against the schemas defined in this document.</p> <p>MUST: Implementations MUST return appropriate errors for invalid requests.</p>"},{"location":"protocol/06-interfaces/#concurrency","title":"Concurrency","text":"<p>MUST: Implementations MUST handle concurrent requests correctly.</p> <p>SHOULD: Implementations SHOULD use appropriate concurrency primitives (locks, transactions, etc.).</p>"},{"location":"protocol/06-interfaces/#see-also","title":"See Also","text":"<ul> <li>Data Model - Task schema definitions</li> <li>Execution Lifecycle - State machine and execution rules</li> <li>A2A Protocol Documentation - A2A Protocol specification</li> <li>JSON-RPC 2.0 Specification - JSON-RPC 2.0 standard</li> </ul>"},{"location":"protocol/07-conformance/","title":"Conformance Requirements","text":"<p>This document defines the conformance requirements for implementations of the AI Partner Up Flow Protocol. Implementations MUST meet the requirements specified in this document to be considered protocol-compliant.</p>"},{"location":"protocol/07-conformance/#conformance-levels","title":"Conformance Levels","text":"<p>Requirements are categorized using RFC 2119 keywords:</p> <ul> <li>MUST: Mandatory requirement. All implementations MUST comply.</li> <li>SHOULD: Recommended requirement. Implementations SHOULD comply unless there is a specific reason not to.</li> <li>MAY: Optional requirement. Implementations MAY choose to support.</li> </ul>"},{"location":"protocol/07-conformance/#core-requirements-must","title":"Core Requirements (MUST)","text":""},{"location":"protocol/07-conformance/#data-model-compliance","title":"Data Model Compliance","text":"<p>MUST: Implementations MUST support the complete Task schema as defined in Data Model.</p> <p>MUST: Implementations MUST validate all task data against the JSON Schema definitions.</p> <p>MUST: Implementations MUST support all required fields: - <code>id</code> (UUID v4) - <code>name</code> (string) - <code>status</code> (enum: <code>pending</code>, <code>in_progress</code>, <code>completed</code>, <code>failed</code>, <code>cancelled</code>)</p> <p>MUST: Implementations MUST support all optional fields with correct types and constraints.</p>"},{"location":"protocol/07-conformance/#state-machine-compliance","title":"State Machine Compliance","text":"<p>MUST: Implementations MUST implement the complete state machine as defined in Execution Lifecycle.</p> <p>MUST: Implementations MUST enforce valid state transitions only.</p> <p>MUST: Implementations MUST NOT allow invalid state transitions.</p> <p>MUST: Implementations MUST update timestamps (<code>started_at</code>, <code>completed_at</code>) according to state transitions.</p>"},{"location":"protocol/07-conformance/#dependency-resolution","title":"Dependency Resolution","text":"<p>MUST: Implementations MUST resolve dependencies before allowing task execution.</p> <p>MUST: Implementations MUST respect the <code>required</code> flag for each dependency.</p> <p>MUST: Implementations MUST detect and reject circular dependencies.</p> <p>MUST: Implementations MUST validate that all dependency IDs reference existing tasks.</p>"},{"location":"protocol/07-conformance/#priority-scheduling","title":"Priority Scheduling","text":"<p>MUST: Implementations MUST schedule tasks by priority (lower values = higher priority).</p> <p>MUST: Implementations MUST execute tasks with priority 0 before priority 1, priority 1 before priority 2, etc.</p> <p>MUST: Implementations MUST respect dependencies over priorities (tasks with higher priority but unsatisfied dependencies MUST wait).</p>"},{"location":"protocol/07-conformance/#executor-interface","title":"Executor Interface","text":"<p>MUST: Implementations MUST provide an <code>ExecutorRegistry</code> mechanism.</p> <p>MUST: Implementations MUST support executor registration with unique identifiers.</p> <p>MUST: Implementations MUST look up executors by <code>schemas.method</code> when executing tasks.</p> <p>MUST: Implementations MUST handle executor errors and update task status to <code>failed</code>.</p>"},{"location":"protocol/07-conformance/#storage-interface","title":"Storage Interface","text":"<p>MUST: Implementations MUST provide persistent storage for task state.</p> <p>MUST: Implementations MUST support the following storage operations: - <code>createTask</code> - <code>getTask</code> - <code>updateTask</code> - <code>deleteTask</code> - <code>listTasks</code> - <code>getTaskTree</code></p> <p>MUST: Implementations MUST ensure data consistency (no orphaned tasks, valid references).</p>"},{"location":"protocol/07-conformance/#api-interface","title":"API Interface","text":"<p>MUST: Implementations MUST support JSON-RPC 2.0 over HTTP.</p> <p>MUST: Implementations MUST support all standard methods defined in Interface Protocol: - <code>tasks.create</code> - <code>tasks.get</code> - <code>tasks.update</code> - <code>tasks.delete</code> - <code>tasks.list</code> - <code>tasks.execute</code> - <code>tasks.cancel</code> - <code>tasks.tree</code> - <code>tasks.children</code></p> <p>MUST: Implementations MUST return JSON-RPC 2.0 compliant responses.</p> <p>MUST: Implementations MUST use standard JSON-RPC 2.0 error codes for protocol errors.</p>"},{"location":"protocol/07-conformance/#recommended-requirements-should","title":"Recommended Requirements (SHOULD)","text":""},{"location":"protocol/07-conformance/#a2a-protocol-support","title":"A2A Protocol Support","text":"<p>SHOULD: Implementations SHOULD support A2A Protocol for enhanced agent-to-agent communication.</p> <p>SHOULD: Implementations SHOULD support agent card discovery (<code>GET /.well-known/agent-card</code>).</p> <p>SHOULD: Implementations SHOULD support A2A Protocol streaming (SSE, WebSocket).</p>"},{"location":"protocol/07-conformance/#streaming-support","title":"Streaming Support","text":"<p>SHOULD: Implementations SHOULD support at least one streaming mode (SSE, WebSocket, or push notifications).</p> <p>SHOULD: Implementations SHOULD provide real-time progress updates during task execution.</p>"},{"location":"protocol/07-conformance/#input-validation","title":"Input Validation","text":"<p>SHOULD: Implementations SHOULD validate <code>inputs</code> against <code>schemas.input_schema</code> before execution.</p> <p>SHOULD: Implementations SHOULD provide clear error messages for validation failures.</p>"},{"location":"protocol/07-conformance/#error-handling","title":"Error Handling","text":"<p>SHOULD: Implementations SHOULD provide descriptive error messages.</p> <p>SHOULD: Implementations SHOULD include error context (field names, validation details) in error responses.</p>"},{"location":"protocol/07-conformance/#concurrency","title":"Concurrency","text":"<p>SHOULD: Implementations SHOULD support concurrent execution of independent tasks.</p> <p>SHOULD: Implementations SHOULD provide configuration for concurrency limits.</p>"},{"location":"protocol/07-conformance/#re-execution","title":"Re-execution","text":"<p>SHOULD: Implementations SHOULD support re-execution of failed tasks.</p> <p>SHOULD: Implementations SHOULD support cascading re-execution (re-execute dependents when a task is re-executed).</p>"},{"location":"protocol/07-conformance/#copy-execution","title":"Copy Execution","text":"<p>SHOULD: Implementations SHOULD support copying tasks before execution.</p> <p>SHOULD: Implementations SHOULD support copying task trees (task + children).</p>"},{"location":"protocol/07-conformance/#optional-requirements-may","title":"Optional Requirements (MAY)","text":""},{"location":"protocol/07-conformance/#authentication","title":"Authentication","text":"<p>MAY: Implementations MAY support authentication (JWT, API keys, etc.).</p> <p>MAY: Implementations MAY enforce access control based on <code>user_id</code>.</p>"},{"location":"protocol/07-conformance/#retry-mechanisms","title":"Retry Mechanisms","text":"<p>MAY: Implementations MAY support automatic retry of failed tasks.</p> <p>MAY: Implementations MAY provide configuration for retry behavior (max attempts, backoff strategy).</p>"},{"location":"protocol/07-conformance/#advanced-features","title":"Advanced Features","text":"<p>MAY: Implementations MAY support: - Task checkpoints and resume - Task scheduling (cron-like) - Task versioning - Task templates</p>"},{"location":"protocol/07-conformance/#implementation-checklist","title":"Implementation Checklist","text":"<p>Use this checklist to verify your implementation's conformance:</p>"},{"location":"protocol/07-conformance/#data-model","title":"Data Model","text":"<ul> <li> Complete Task schema support</li> <li> JSON Schema validation</li> <li> All required fields supported</li> <li> All optional fields with correct types</li> <li> Field constraints enforced (priority range, progress range, etc.)</li> </ul>"},{"location":"protocol/07-conformance/#state-machine","title":"State Machine","text":"<ul> <li> All states supported (<code>pending</code>, <code>in_progress</code>, <code>completed</code>, <code>failed</code>, <code>cancelled</code>)</li> <li> Valid transitions enforced</li> <li> Invalid transitions rejected</li> <li> Timestamps updated correctly</li> </ul>"},{"location":"protocol/07-conformance/#dependency-resolution_1","title":"Dependency Resolution","text":"<ul> <li> Dependencies checked before execution</li> <li> <code>required</code> flag respected</li> <li> Circular dependency detection</li> <li> Dependency reference validation</li> </ul>"},{"location":"protocol/07-conformance/#priority-scheduling_1","title":"Priority Scheduling","text":"<ul> <li> Priority ordering enforced</li> <li> Dependencies take precedence over priority</li> <li> Same-priority tasks handled fairly</li> </ul>"},{"location":"protocol/07-conformance/#executor-interface_1","title":"Executor Interface","text":"<ul> <li> ExecutorRegistry implemented</li> <li> Executor registration supported</li> <li> Executor lookup by method</li> <li> Executor errors handled</li> </ul>"},{"location":"protocol/07-conformance/#storage-interface_1","title":"Storage Interface","text":"<ul> <li> All required operations implemented</li> <li> Data consistency ensured</li> <li> Query operations supported</li> <li> Transaction support (if applicable)</li> </ul>"},{"location":"protocol/07-conformance/#api-interface_1","title":"API Interface","text":"<ul> <li> JSON-RPC 2.0 compliance</li> <li> All standard methods implemented</li> <li> Error responses compliant</li> <li> Request validation</li> </ul>"},{"location":"protocol/07-conformance/#optional-features","title":"Optional Features","text":"<ul> <li> A2A Protocol support (if applicable)</li> <li> Streaming support (if applicable)</li> <li> Authentication (if applicable)</li> <li> Retry mechanisms (if applicable)</li> </ul>"},{"location":"protocol/07-conformance/#compatibility-matrix","title":"Compatibility Matrix","text":""},{"location":"protocol/07-conformance/#version-compatibility","title":"Version Compatibility","text":"Protocol Version Compatible Implementations 1.0 All implementations following this specification <p>MUST: Implementations MUST specify the protocol version they support.</p> <p>SHOULD: Implementations SHOULD be backward compatible with previous protocol versions when possible.</p>"},{"location":"protocol/07-conformance/#feature-compatibility","title":"Feature Compatibility","text":"Feature Required Optional Core Task Schema \u2705 MUST State Machine \u2705 MUST Dependency Resolution \u2705 MUST Priority Scheduling \u2705 MUST Executor Interface \u2705 MUST Storage Interface \u2705 MUST JSON-RPC 2.0 API \u2705 MUST A2A Protocol \u2705 SHOULD Streaming \u2705 SHOULD Authentication \u2705 MAY Retry Mechanisms \u2705 MAY"},{"location":"protocol/07-conformance/#breaking-changes-policy","title":"Breaking Changes Policy","text":"<p>MUST: Breaking changes to the protocol MUST result in a new major version.</p> <p>MUST: Breaking changes MUST be documented in protocol version history.</p> <p>SHOULD: Implementations SHOULD support multiple protocol versions when possible.</p>"},{"location":"protocol/07-conformance/#validation-requirements","title":"Validation Requirements","text":""},{"location":"protocol/07-conformance/#schema-validation","title":"Schema Validation","text":"<p>MUST: Implementations MUST validate all incoming task data against the JSON Schema definitions.</p> <p>MUST: Implementations MUST reject invalid task data with appropriate error codes.</p> <p>SHOULD: Implementations SHOULD provide detailed validation error messages.</p>"},{"location":"protocol/07-conformance/#state-validation","title":"State Validation","text":"<p>MUST: Implementations MUST validate state transitions against the state machine.</p> <p>MUST: Implementations MUST reject invalid state transitions.</p> <p>MUST: Implementations MUST ensure field values are consistent with status (e.g., <code>result</code> is null when status is not <code>completed</code>).</p>"},{"location":"protocol/07-conformance/#dependency-validation","title":"Dependency Validation","text":"<p>MUST: Implementations MUST validate dependencies when tasks are created or updated.</p> <p>MUST: Implementations MUST detect and reject circular dependencies.</p> <p>MUST: Implementations MUST validate that all dependency IDs reference existing tasks.</p>"},{"location":"protocol/07-conformance/#error-handling-validation","title":"Error Handling Validation","text":"<p>MUST: Implementations MUST handle all error cases defined in the protocol.</p> <p>MUST: Implementations MUST return appropriate error codes and messages.</p> <p>SHOULD: Implementations SHOULD provide error context (field names, validation details).</p>"},{"location":"protocol/07-conformance/#testing-requirements","title":"Testing Requirements","text":""},{"location":"protocol/07-conformance/#unit-testing","title":"Unit Testing","text":"<p>SHOULD: Implementations SHOULD include unit tests for: - Task schema validation - State machine transitions - Dependency resolution - Priority scheduling - Executor interface</p>"},{"location":"protocol/07-conformance/#integration-testing","title":"Integration Testing","text":"<p>SHOULD: Implementations SHOULD include integration tests for: - Complete task execution flows - Dependency chains - Error handling - API endpoints</p>"},{"location":"protocol/07-conformance/#compliance-testing","title":"Compliance Testing","text":"<p>SHOULD: Implementations SHOULD include compliance tests that verify: - All MUST requirements are met - All SHOULD requirements are met (if applicable) - Protocol compatibility</p>"},{"location":"protocol/07-conformance/#certification","title":"Certification","text":"<p>MAY: Implementations MAY seek protocol compliance certification.</p> <p>SHOULD: Certification SHOULD verify: - All MUST requirements - Recommended SHOULD requirements - Protocol compatibility</p>"},{"location":"protocol/07-conformance/#see-also","title":"See Also","text":"<ul> <li>Data Model - Task schema definitions</li> <li>Execution Lifecycle - State machine specification</li> <li>Interface Protocol - API specifications</li> <li>Validation - Validation algorithms</li> <li>Error Handling - Error codes and handling</li> </ul>"},{"location":"protocol/08-errors/","title":"Error Handling Specification","text":"<p>This document defines the error handling requirements and error codes for the AI Partner Up Flow Protocol.</p>"},{"location":"protocol/08-errors/#error-principles","title":"Error Principles","text":"<p>MUST: All errors MUST follow JSON-RPC 2.0 error format.</p> <p>MUST: Error messages MUST be descriptive and include context.</p> <p>SHOULD: Error messages SHOULD be human-readable.</p> <p>SHOULD: Error responses SHOULD include error codes for programmatic handling.</p>"},{"location":"protocol/08-errors/#error-response-format","title":"Error Response Format","text":"<p>All errors follow this format:</p> <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Invalid params\",\n    \"data\": {\n      \"field\": \"task_id\",\n      \"reason\": \"Invalid UUID format\",\n      \"details\": \"Expected UUID v4 format\"\n    }\n  },\n  \"id\": \"request-id\"\n}\n</code></pre> <p>Fields: - <code>code</code> (integer, required): Error code (see Error Codes section below) - <code>message</code> (string, required): Human-readable error message - <code>data</code> (any, optional): Additional error context</p>"},{"location":"protocol/08-errors/#error-codes","title":"Error Codes","text":"<p>This section defines all error codes used by the protocol.</p>"},{"location":"protocol/08-errors/#standard-json-rpc-20-error-codes","title":"Standard JSON-RPC 2.0 Error Codes","text":"<p>These codes are defined by the JSON-RPC 2.0 specification:</p> Code Name Description When to Use -32700 Parse error Invalid JSON was received Request body is not valid JSON -32600 Invalid Request The JSON sent is not a valid Request object Missing required fields (<code>jsonrpc</code>, <code>method</code>, <code>params</code>, <code>id</code>) -32601 Method not found The method does not exist / is not available Method name is not recognized -32602 Invalid params Invalid method parameter(s) Parameters don't match method signature or fail validation -32603 Internal error Internal JSON-RPC error Server-side error (unexpected exception, etc.) <p>MUST: Implementations MUST use these codes for protocol-level errors.</p>"},{"location":"protocol/08-errors/#protocol-specific-error-codes","title":"Protocol-Specific Error Codes","text":"<p>These codes are specific to the AI Partner Up Flow Protocol:</p> Code Name Description When to Use -32001 Task not found The specified task does not exist Task ID references non-existent task -32002 Circular dependency Circular dependency detected in task tree Task dependencies form a cycle -32003 Executor not found The specified executor is not registered <code>schemas.method</code> doesn't match any registered executor -32004 Unauthorized Request is not authorized Authentication failed or insufficient permissions -32005 Invalid task schema Task schema validation failed Task data doesn't conform to schema (including invalid or missing <code>origin_type</code>, <code>original_task_id</code>, <code>has_references</code>, or if <code>origin_type</code> is <code>link</code>/<code>archive</code> and the referenced task is not <code>completed</code>) -32006 Invalid state transition Invalid state transition attempted Attempted transition violates state machine rules -32007 Dependency not satisfied Task dependencies are not satisfied Task cannot execute because dependencies are not ready -32008 Task already executing Task is already being executed Attempt to execute a task that's already <code>in_progress</code> -32009 Cannot delete task Task cannot be deleted Task has children, dependents, or is not in <code>pending</code> status -32010 Invalid parent reference Parent task reference is invalid <code>parent_id</code> references non-existent or invalid task -32011 Invalid dependency reference Dependency task reference is invalid Dependency ID references non-existent task -32012 Task tree validation failed Task tree structure is invalid Tree has multiple roots, circular parent-child, etc. <p>MUST: Implementations MUST use these codes for protocol-specific errors.</p> <p>SHOULD: Implementations SHOULD provide detailed error data in the <code>data</code> field.</p>"},{"location":"protocol/08-errors/#error-categories","title":"Error Categories","text":""},{"location":"protocol/08-errors/#validation-errors","title":"Validation Errors","text":"<p>Errors that occur when input data fails validation.</p> <p>Common Scenarios: - Invalid UUID format - Missing required fields - Invalid field types - Field value out of range - Schema validation failure</p> <p>Error Code: <code>-32602</code> (Invalid params) or <code>-32005</code> (Invalid task schema)</p> <p>Example: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Invalid params\",\n    \"data\": {\n      \"field\": \"priority\",\n      \"reason\": \"Value out of range\",\n      \"expected\": \"0-3\",\n      \"actual\": 5\n    }\n  },\n  \"id\": \"req-001\"\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#not-found-errors","title":"Not Found Errors","text":"<p>Errors that occur when a referenced resource doesn't exist.</p> <p>Common Scenarios: - Task not found - Executor not found - Parent task not found - Dependency task not found</p> <p>Error Code: <code>-32001</code> (Task not found) or <code>-32003</code> (Executor not found)</p> <p>Example: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32001,\n    \"message\": \"Task not found\",\n    \"data\": {\n      \"task_id\": \"550e8400-e29b-41d4-a716-446655440000\"\n    }\n  },\n  \"id\": \"req-002\"\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#state-machine-errors","title":"State Machine Errors","text":"<p>Errors that occur when state transitions are invalid.</p> <p>Common Scenarios: - Invalid state transition - Task already executing - Task in terminal state</p> <p>Error Code: <code>-32006</code> (Invalid state transition) or <code>-32008</code> (Task already executing)</p> <p>Example: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32006,\n    \"message\": \"Invalid state transition\",\n    \"data\": {\n      \"task_id\": \"task-uuid\",\n      \"current_status\": \"completed\",\n      \"attempted_transition\": \"pending -&gt; in_progress\",\n      \"reason\": \"Cannot transition from terminal state\"\n    }\n  },\n  \"id\": \"req-003\"\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#dependency-errors","title":"Dependency Errors","text":"<p>Errors that occur when dependencies are invalid or not satisfied.</p> <p>Common Scenarios: - Circular dependency - Invalid dependency reference - Dependency not satisfied - Self-reference</p> <p>Error Code: <code>-32002</code> (Circular dependency), <code>-32011</code> (Invalid dependency reference), or <code>-32007</code> (Dependency not satisfied)</p> <p>Example: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32002,\n    \"message\": \"Circular dependency detected\",\n    \"data\": {\n      \"cycle\": [\n        \"task-a\",\n        \"task-b\",\n        \"task-c\",\n        \"task-a\"\n      ]\n    }\n  },\n  \"id\": \"req-004\"\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#authorization-errors","title":"Authorization Errors","text":"<p>Errors that occur when authentication or authorization fails.</p> <p>Common Scenarios: - Missing authentication token - Invalid authentication token - Insufficient permissions - User ID mismatch</p> <p>Error Code: <code>-32004</code> (Unauthorized)</p> <p>Example: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32004,\n    \"message\": \"Unauthorized\",\n    \"data\": {\n      \"reason\": \"Invalid authentication token\"\n    }\n  },\n  \"id\": \"req-005\"\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#execution-errors","title":"Execution Errors","text":"<p>Errors that occur during task execution.</p> <p>Common Scenarios: - Executor execution failed - Executor timeout - Executor not found - Invalid executor inputs</p> <p>Error Code: <code>-32003</code> (Executor not found) or <code>-32603</code> (Internal error)</p> <p>Example: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32603,\n    \"message\": \"Internal error\",\n    \"data\": {\n      \"task_id\": \"task-uuid\",\n      \"executor\": \"web_crawler\",\n      \"error\": \"Connection timeout after 30 seconds\"\n    }\n  },\n  \"id\": \"req-006\"\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#task-error-field","title":"Task Error Field","text":"<p>When a task fails, the error is stored in the <code>task.error</code> field.</p> <p>Format: String containing the error message.</p> <p>MUST: <code>error</code> MUST be <code>null</code> when status is not <code>failed</code> or <code>cancelled</code>.</p> <p>MUST: <code>error</code> MUST be a non-empty string when status is <code>failed</code> or <code>cancelled</code>.</p> <p>SHOULD: Error messages SHOULD be descriptive and include context.</p> <p>Example: <pre><code>{\n  \"id\": \"task-uuid\",\n  \"status\": \"failed\",\n  \"error\": \"Executor 'web_crawler' raised exception: Connection timeout after 30 seconds\"\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#error-propagation","title":"Error Propagation","text":""},{"location":"protocol/08-errors/#dependency-failure-propagation","title":"Dependency Failure Propagation","text":"<p>When a required dependency fails:</p> <ol> <li>Block Execution: Dependent task MUST NOT execute.</li> <li>Status: Dependent task remains in <code>pending</code> status.</li> <li>Error Information: Dependent task MAY inherit error information (implementation-specific).</li> </ol> <p>Example: <pre><code>{\n  \"id\": \"dependent-task\",\n  \"status\": \"pending\",\n  \"dependencies\": [\n    {\n      \"id\": \"failed-task\",\n      \"required\": true\n    }\n  ],\n  \"error\": null  // Task hasn't failed, but cannot execute due to dependency\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#cascading-failures","title":"Cascading Failures","text":"<p>When a task fails, dependent tasks may also be affected:</p> <ol> <li>Required Dependencies: Tasks with <code>required: true</code> dependencies on failed tasks MUST NOT execute.</li> <li>Optional Dependencies: Tasks with <code>required: false</code> dependencies on failed tasks CAN execute.</li> </ol>"},{"location":"protocol/08-errors/#error-recovery","title":"Error Recovery","text":""},{"location":"protocol/08-errors/#retry-mechanisms","title":"Retry Mechanisms","text":"<p>MAY: Implementations MAY support automatic retry of failed tasks.</p> <p>SHOULD: If retry is supported, implementations SHOULD: - Limit the number of retries - Use exponential backoff - Provide configuration for retry behavior</p> <p>Example Retry Configuration: <pre><code>{\n  \"retry\": {\n    \"max_attempts\": 3,\n    \"backoff\": \"exponential\",\n    \"initial_delay\": 1.0,\n    \"max_delay\": 60.0\n  }\n}\n</code></pre></p>"},{"location":"protocol/08-errors/#manual-recovery","title":"Manual Recovery","text":"<p>SHOULD: Implementations SHOULD support manual re-execution of failed tasks.</p> <p>MUST: Re-execution MUST reset task state to <code>pending</code> and clear error.</p>"},{"location":"protocol/08-errors/#error-handling-best-practices","title":"Error Handling Best Practices","text":""},{"location":"protocol/08-errors/#for-implementations","title":"For Implementations","text":"<ol> <li>Validate Early: Validate inputs as early as possible.</li> <li>Provide Context: Include field names, expected values, and actual values in error messages.</li> <li>Use Appropriate Codes: Use standard JSON-RPC codes for protocol errors, custom codes for application errors.</li> <li>Log Errors: Log errors with full context for debugging.</li> <li>Handle Gracefully: Don't expose internal implementation details in error messages.</li> </ol>"},{"location":"protocol/08-errors/#for-clients","title":"For Clients","text":"<ol> <li>Check Error Codes: Use error codes for programmatic error handling.</li> <li>Display Messages: Show error messages to users.</li> <li>Retry Appropriately: Retry only for transient errors (not validation errors).</li> <li>Handle Gracefully: Provide fallback behavior for expected errors.</li> </ol>"},{"location":"protocol/08-errors/#implementation-requirements","title":"Implementation Requirements","text":"<p>MUST: Implementations MUST handle all error cases defined in this document.</p> <p>MUST: Implementations MUST return appropriate error codes.</p> <p>SHOULD: Implementations SHOULD provide detailed error data in the <code>data</code> field.</p> <p>SHOULD: Implementations SHOULD log errors with full context.</p>"},{"location":"protocol/08-errors/#see-also","title":"See Also","text":"<ul> <li>Interface Protocol - API error handling</li> <li>Execution Lifecycle - State machine error handling</li> <li>Conformance - Error handling requirements</li> </ul>"},{"location":"protocol/09-validation/","title":"Validation Rules and Algorithms","text":"<p>This document defines the validation rules and algorithms that implementations MUST follow to ensure protocol compliance.</p>"},{"location":"protocol/09-validation/#validation-principles","title":"Validation Principles","text":"<p>MUST: All implementations MUST validate data according to the rules defined in this document.</p> <p>MUST: Implementations MUST reject invalid data with appropriate error codes.</p> <p>SHOULD: Implementations SHOULD provide detailed validation error messages.</p>"},{"location":"protocol/09-validation/#task-validation","title":"Task Validation","text":""},{"location":"protocol/09-validation/#schema-validation","title":"Schema Validation","text":"<p>Tasks MUST be validated against the JSON Schema defined in Data Model, including provenance/reference fields: - <code>origin_type</code> MUST be one of: <code>create</code>, <code>link</code>, <code>copy</code>, <code>archive</code>, or null. - If <code>origin_type</code> is <code>link</code> or <code>archive</code>, <code>original_task_id</code> MUST reference a task in <code>completed</code> status (in principle). - <code>original_task_id</code> MUST be a valid UUID v4 if present. - <code>has_references</code> MUST be a boolean.</p> <p>Algorithm: <pre><code>function validateTaskSchema(task):\n    // 1. Check required fields\n    if not task.id:\n        return error(\"id is required\")\n    if not task.name:\n        return error(\"name is required\")\n    if not task.status:\n        return error(\"status is required\")\n\n    // 2. Validate field types\n    if not isValidUUID(task.id):\n        return error(\"id must be valid UUID v4\")\n    if not isinstance(task.name, str) or len(task.name) == 0:\n        return error(\"name must be non-empty string\")\n    if task.status not in [\"pending\", \"in_progress\", \"completed\", \"failed\", \"cancelled\"]:\n        return error(\"status must be one of: pending, in_progress, completed, failed, cancelled\")\n\n    // 3. Validate field constraints\n    if task.priority is not None:\n        if not isinstance(task.priority, int) or task.priority &lt; 0 or task.priority &gt; 3:\n            return error(\"priority must be integer in range 0-3\")\n\n    if task.progress is not None:\n        if not isinstance(task.progress, (int, float)) or task.progress &lt; 0.0 or task.progress &gt; 1.0:\n            return error(\"progress must be number in range 0.0-1.0\")\n\n    // 4. Validate field relationships\n    if task.status == \"completed\" and task.result is None:\n        return warning(\"result should be populated when status is completed\")\n\n    if task.status in [\"failed\", \"cancelled\"] and (task.error is None or len(task.error) == 0):\n        return warning(\"error should be populated when status is failed or cancelled\")\n\n    if task.status == \"pending\" and task.started_at is not None:\n        return error(\"started_at must be null when status is pending\")\n\n    if task.status == \"in_progress\" and task.started_at is None:\n        return error(\"started_at must be set when status is in_progress\")\n\n    if task.status in [\"completed\", \"failed\", \"cancelled\"] and task.completed_at is None:\n        return error(\"completed_at must be set when status is terminal\")\n\n    // 5. Validate UUIDs\n    if task.parent_id is not None and not isValidUUID(task.parent_id):\n        return error(\"parent_id must be valid UUID v4\")\n\n    // 6. Validate inputs against schema\n    if task.schemas and task.schemas.input_schema:\n        if not validateJSONSchema(task.inputs, task.schemas.input_schema):\n            return error(\"inputs do not conform to input_schema\")\n\n    return success\n</code></pre></p> <p>MUST: Implementations MUST perform all validation checks listed above.</p>"},{"location":"protocol/09-validation/#field-consistency-validation","title":"Field Consistency Validation","text":"<p>Tasks MUST have consistent field values based on their status.</p> <p>Rules: 1. If <code>status</code> is <code>completed</code>, <code>result</code> SHOULD NOT be <code>null</code>. 2. If <code>status</code> is <code>failed</code> or <code>cancelled</code>, <code>error</code> SHOULD NOT be <code>null</code>. 3. If <code>status</code> is <code>pending</code>, <code>started_at</code> MUST be <code>null</code>. 4. If <code>status</code> is <code>in_progress</code>, <code>started_at</code> MUST NOT be <code>null</code>. 5. If <code>status</code> is terminal (<code>completed</code>, <code>failed</code>, <code>cancelled</code>), <code>completed_at</code> MUST NOT be <code>null</code>. 6. <code>progress</code> MUST be in range [0.0, 1.0].</p> <p>Algorithm: <pre><code>function validateFieldConsistency(task):\n    errors = []\n\n    if task.status == \"completed\" and task.result is None:\n        errors.append(\"result should be populated when status is completed\")\n\n    if task.status in [\"failed\", \"cancelled\"]:\n        if task.error is None or len(task.error) == 0:\n            errors.append(\"error must be populated when status is failed or cancelled\")\n\n    if task.status == \"pending\":\n        if task.started_at is not None:\n            errors.append(\"started_at must be null when status is pending\")\n\n    if task.status == \"in_progress\":\n        if task.started_at is None:\n            errors.append(\"started_at must be set when status is in_progress\")\n\n    if task.status in [\"completed\", \"failed\", \"cancelled\"]:\n        if task.completed_at is None:\n            errors.append(\"completed_at must be set when status is terminal\")\n\n    if task.progress is not None:\n        if task.progress &lt; 0.0 or task.progress &gt; 1.0:\n            errors.append(\"progress must be in range 0.0-1.0\")\n\n    return errors\n</code></pre></p>"},{"location":"protocol/09-validation/#dependency-validation","title":"Dependency Validation","text":""},{"location":"protocol/09-validation/#reference-validation","title":"Reference Validation","text":"<p>All dependency IDs MUST reference existing tasks.</p> <p>Algorithm: <pre><code>function validateDependencyReferences(task, allTasks):\n    errors = []\n\n    for dependency in task.dependencies:\n        dep_id = dependency.id\n\n        // Check if dependency task exists\n        if dep_id not in [t.id for t in allTasks]:\n            errors.append(f\"Dependency task '{dep_id}' not found\")\n\n        // Check self-reference\n        if dep_id == task.id:\n            errors.append(\"Task cannot depend on itself\")\n\n    return errors\n</code></pre></p> <p>MUST: Implementations MUST validate that all dependency IDs reference existing tasks.</p> <p>MUST: Implementations MUST reject self-references (task depending on itself).</p>"},{"location":"protocol/09-validation/#circular-dependency-detection","title":"Circular Dependency Detection","text":"<p>Dependencies MUST NOT form cycles.</p> <p>Algorithm (Depth-First Search): <pre><code>function hasCircularDependency(task, allTasks, visited=None, recStack=None):\n    if visited is None:\n        visited = set()\n    if recStack is None:\n        recStack = set()\n\n    // Mark current task as visited and add to recursion stack\n    visited.add(task.id)\n    recStack.add(task.id)\n\n    // Check all dependencies\n    for dependency in task.dependencies:\n        dep_id = dependency.id\n        dep_task = findTask(dep_id, allTasks)\n\n        if dep_task is None:\n            continue  // Skip invalid references (handled separately)\n\n        // If dependency not visited, recurse\n        if dep_id not in visited:\n            if hasCircularDependency(dep_task, allTasks, visited, recStack):\n                return true\n\n        // If dependency is in recursion stack, cycle detected\n        elif dep_id in recStack:\n            return true\n\n    // Remove from recursion stack (backtrack)\n    recStack.remove(task.id)\n    return false\n</code></pre></p> <p>MUST: Implementations MUST detect circular dependencies.</p> <p>MUST: Implementations MUST reject task definitions with circular dependencies.</p> <p>Example: <pre><code>Task A depends on Task B\nTask B depends on Task C\nTask C depends on Task A  // Cycle detected!\n</code></pre></p>"},{"location":"protocol/09-validation/#dependency-satisfaction-validation","title":"Dependency Satisfaction Validation","text":"<p>When validating if a task can execute, dependencies MUST be checked.</p> <p>Algorithm: <pre><code>function validateDependencySatisfaction(task, allTasks):\n    for dependency in task.dependencies:\n        dep_id = dependency.id\n        dep_task = findTask(dep_id, allTasks)\n\n        if dep_task is None:\n            return false  // Invalid reference\n\n        // Check if dependency is ready\n        if dep_task.status == \"pending\" or dep_task.status == \"in_progress\":\n            return false  // Dependency not ready\n\n        // Check required dependencies\n        if dependency.required == true:\n            if dep_task.status != \"completed\":\n                return false  // Required dependency failed\n\n    return true  // All dependencies satisfied\n</code></pre></p>"},{"location":"protocol/09-validation/#task-tree-validation","title":"Task Tree Validation","text":""},{"location":"protocol/09-validation/#root-task-validation","title":"Root Task Validation","text":"<p>A task tree MUST have exactly one root task.</p> <p>Algorithm: <pre><code>function validateRootTask(allTasks):\n    roots = [task for task in allTasks if task.parent_id is None]\n\n    if len(roots) == 0:\n        return error(\"No root task found\")\n\n    if len(roots) &gt; 1:\n        return error(f\"Multiple root tasks found: {[r.id for r in roots]}\")\n\n    return success\n</code></pre></p> <p>MUST: Implementations MUST validate that there is exactly one root task.</p>"},{"location":"protocol/09-validation/#parent-child-consistency-validation","title":"Parent-Child Consistency Validation","text":"<p>All <code>parent_id</code> values MUST reference valid parent tasks.</p> <p>Algorithm: <pre><code>function validateParentChildConsistency(allTasks):\n    errors = []\n    taskIds = {task.id for task in allTasks}\n\n    for task in allTasks:\n        if task.parent_id is not None:\n            if task.parent_id not in taskIds:\n                errors.append(f\"Task '{task.id}' has invalid parent_id: '{task.parent_id}'\")\n\n    return errors\n</code></pre></p> <p>MUST: Implementations MUST validate that all <code>parent_id</code> values reference existing tasks.</p>"},{"location":"protocol/09-validation/#tree-acyclicity-validation","title":"Tree Acyclicity Validation","text":"<p>Parent-child relationships MUST NOT form cycles.</p> <p>Algorithm: <pre><code>function hasCircularParentChild(task, allTasks, visited=None):\n    if visited is None:\n        visited = set()\n\n    if task.id in visited:\n        return true  // Cycle detected\n\n    visited.add(task.id)\n\n    if task.parent_id is not None:\n        parent = findTask(task.parent_id, allTasks)\n        if parent and hasCircularParentChild(parent, allTasks, visited):\n            return true\n\n    visited.remove(task.id)\n    return false\n</code></pre></p> <p>MUST: Implementations MUST detect circular parent-child relationships.</p> <p>MUST: Implementations MUST reject task trees with circular parent-child relationships.</p>"},{"location":"protocol/09-validation/#tree-completeness-validation","title":"Tree Completeness Validation","text":"<p>All referenced tasks MUST be present in the tree.</p> <p>Algorithm: <pre><code>function validateTreeCompleteness(rootTask, allTasks):\n    errors = []\n    taskIds = {task.id for task in allTasks}\n\n    // Check all parent references\n    for task in allTasks:\n        if task.parent_id is not None and task.parent_id not in taskIds:\n            errors.append(f\"Task '{task.id}' references non-existent parent: '{task.parent_id}'\")\n\n    // Check all dependency references\n    for task in allTasks:\n        for dependency in task.dependencies:\n            if dependency.id not in taskIds:\n                errors.append(f\"Task '{task.id}' references non-existent dependency: '{dependency.id}'\")\n\n    return errors\n</code></pre></p> <p>MUST: Implementations MUST validate that all referenced tasks exist in the tree.</p>"},{"location":"protocol/09-validation/#input-validation","title":"Input Validation","text":""},{"location":"protocol/09-validation/#schema-based-validation","title":"Schema-Based Validation","text":"<p>If <code>schemas.input_schema</code> is present, <code>inputs</code> MUST conform to the schema.</p> <p>Algorithm: <pre><code>function validateInputsAgainstSchema(inputs, inputSchema):\n    // Use JSON Schema validator\n    validator = JSONSchemaValidator(inputSchema)\n\n    try:\n        validator.validate(inputs)\n        return success\n    except ValidationError as e:\n        return error(f\"Input validation failed: {e.message}\")\n</code></pre></p> <p>MUST: Implementations MUST validate <code>inputs</code> against <code>schemas.input_schema</code> if present.</p> <p>SHOULD: Implementations SHOULD use a JSON Schema validator (draft-07).</p>"},{"location":"protocol/09-validation/#executor-specific-validation","title":"Executor-Specific Validation","text":"<p>Executors MAY provide additional input validation.</p> <p>Algorithm: <pre><code>function validateExecutorInputs(task, executor):\n    // 1. Check if executor exists\n    if executor is None:\n        return error(\"Executor not found\")\n\n    // 2. Validate against executor's input schema (if provided)\n    if hasattr(executor, 'get_input_schema'):\n        schema = executor.get_input_schema()\n        if not validateJSONSchema(task.inputs, schema):\n            return error(\"Inputs do not conform to executor's input schema\")\n\n    // 3. Executor-specific validation (if provided)\n    if hasattr(executor, 'validate_inputs'):\n        try:\n            executor.validate_inputs(task.inputs)\n        except ValidationError as e:\n            return error(f\"Executor validation failed: {e.message}\")\n\n    return success\n</code></pre></p> <p>SHOULD: Implementations SHOULD validate inputs using executor's validation if available.</p>"},{"location":"protocol/09-validation/#state-transition-validation","title":"State Transition Validation","text":"<p>State transitions MUST follow the state machine rules.</p> <p>Algorithm: <pre><code>function validateStateTransition(currentStatus, newStatus):\n    validTransitions = {\n        \"pending\": [\"in_progress\", \"cancelled\"],\n        \"in_progress\": [\"completed\", \"failed\", \"cancelled\"],\n        \"failed\": [\"pending\"],  // Re-execution\n        \"completed\": [],  // Terminal state\n        \"cancelled\": []  // Terminal state\n    }\n\n    if newStatus not in validTransitions.get(currentStatus, []):\n        return error(f\"Invalid state transition: {currentStatus} -&gt; {newStatus}\")\n\n    return success\n</code></pre></p> <p>MUST: Implementations MUST validate state transitions.</p> <p>MUST: Implementations MUST reject invalid state transitions.</p>"},{"location":"protocol/09-validation/#uuid-validation","title":"UUID Validation","text":"<p>All UUID fields MUST be valid UUID v4 format.</p> <p>Algorithm: <pre><code>function isValidUUID(value):\n    if not isinstance(value, str):\n        return false\n\n    // UUID v4 format: xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx\n    pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$'\n    return re.match(pattern, value.lower()) is not None\n</code></pre></p> <p>MUST: Implementations MUST validate UUID format for <code>id</code>, <code>parent_id</code>, and dependency IDs.</p>"},{"location":"protocol/09-validation/#timestamp-validation","title":"Timestamp Validation","text":"<p>All timestamp fields MUST be valid ISO 8601 format.</p> <p>Algorithm: <pre><code>function isValidTimestamp(value):\n    if value is None:\n        return true  // Null is valid\n\n    if not isinstance(value, str):\n        return false\n\n    try:\n        datetime.fromisoformat(value.replace('Z', '+00:00'))\n        return true\n    except ValueError:\n        return false\n</code></pre></p> <p>MUST: Implementations MUST validate timestamp format.</p> <p>SHOULD: Implementations SHOULD use UTC timezone for timestamps.</p>"},{"location":"protocol/09-validation/#validation-error-reporting","title":"Validation Error Reporting","text":""},{"location":"protocol/09-validation/#error-format","title":"Error Format","text":"<p>Validation errors MUST be reported in a structured format.</p> <p>Format: <pre><code>{\n  \"field\": \"field_name\",\n  \"reason\": \"Validation reason\",\n  \"expected\": \"Expected value or format\",\n  \"actual\": \"Actual value\",\n  \"path\": [\"nested\", \"field\", \"path\"]\n}\n</code></pre></p>"},{"location":"protocol/09-validation/#error-aggregation","title":"Error Aggregation","text":"<p>Multiple validation errors SHOULD be aggregated and returned together.</p> <p>Example: <pre><code>{\n  \"jsonrpc\": \"2.0\",\n  \"error\": {\n    \"code\": -32602,\n    \"message\": \"Invalid params\",\n    \"data\": {\n      \"errors\": [\n        {\n          \"field\": \"priority\",\n          \"reason\": \"Value out of range\",\n          \"expected\": \"0-3\",\n          \"actual\": 5\n        },\n        {\n          \"field\": \"progress\",\n          \"reason\": \"Value out of range\",\n          \"expected\": \"0.0-1.0\",\n          \"actual\": 1.5\n        }\n      ]\n    }\n  },\n  \"id\": \"req-001\"\n}\n</code></pre></p>"},{"location":"protocol/09-validation/#implementation-requirements","title":"Implementation Requirements","text":"<p>MUST: Implementations MUST perform all validation checks defined in this document.</p> <p>MUST: Implementations MUST reject invalid data with appropriate error codes.</p> <p>SHOULD: Implementations SHOULD provide detailed validation error messages.</p> <p>SHOULD: Implementations SHOULD aggregate multiple validation errors when possible.</p>"},{"location":"protocol/09-validation/#see-also","title":"See Also","text":"<ul> <li>Data Model - Task schema definitions</li> <li>Execution Lifecycle - State machine rules</li> <li>Error Handling - Error codes and handling</li> <li>Conformance - Validation requirements</li> </ul>"}]}